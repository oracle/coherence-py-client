Index: etc/proto/messages.proto
===================================================================
diff --git a/etc/proto/messages.proto b/etc/proto/messages.proto
deleted file mode 100644
--- a/etc/proto/messages.proto	(revision 7ed18d6c9ea66aec63a655a1b556e3c67cdb16b6)
+++ /dev/null	(revision 7ed18d6c9ea66aec63a655a1b556e3c67cdb16b6)
@@ -1,508 +0,0 @@
-/*
- * Copyright (c) 2020, 2023 Oracle and/or its affiliates.
- *
- * Licensed under the Universal Permissive License v 1.0 as shown at
- * https://oss.oracle.com/licenses/upl.
- */
-
-// Authors:
-//  Mahesh Kannan
-//  Jonathan Knight
-
-// NamedCacheService message types
-//
-
-syntax = "proto3";
-
-package coherence;
-
-option java_multiple_files = true;
-option java_package = "com.oracle.coherence.grpc";
-
-// A request to clear all the entries in the cache.
-message ClearRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-}
-
-// A request to determine whether an entry exists in a cache
-// with a specific key and value.
-message ContainsEntryRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The serialization format.
-    bytes key = 4;
-    // The value of the entry to verify.
-    bytes value = 5;
-}
-
-// A request to determine whether an entry exists in a cache
-// for the specified key.
-message ContainsKeyRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The key of the entry to verify.
-    bytes key = 4;
-}
-
-// A request to determine whether an entry exists in a cache
-// with the specified value.
-message ContainsValueRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The value of the entry to verify.
-    bytes value = 4;
-}
-
-// A request to destroy a cache.
-message DestroyRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-}
-
-// A request to determine whether a cache is empty or not.
-message IsEmptyRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-}
-
-// A request to determine the number of entries in a cache.
-message SizeRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-}
-
-// A request to obtain the value to which a cache maps the
-// specified key.
-message GetRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The key of the entry to retrieve.
-    bytes key = 4;
-}
-
-// A request to obtain the values that map to the specified keys
-message GetAllRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The key of the entry to retrieve.
-    repeated bytes key = 4;
-}
-
-// A request to associate the specified value with the
-// specified key in a cache.
-message PutRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The cache entry key.
-    bytes key = 4;
-    // The value of the entry.
-    bytes value = 5;
-    // The time to live in millis.
-    int64 ttl = 6;
-}
-
-// A request to associate the specified value with the
-// specified key in a cache.
-message PutAllRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The cache entries to put.
-    repeated Entry entry = 4;
-}
-
-// A request to associate the specified value with the
-// specified key in a cache only if the specified key
-// is not associated with any value (including null).
-message PutIfAbsentRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The cache entry key.
-    bytes key = 4;
-    // The value to be put.
-    bytes value = 5;
-    // The time to live in millis.
-    int64 ttl = 6;
-}
-
-// A request to remove the mapping for a key from a cache
-// if it is present.
-message RemoveRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The key of the entry to be removed.
-    bytes key = 4;
-}
-
-// A request to remove the mapping for a key from a cache
-// only if the specified key is associated with the specified
-// value in that cache.
-message RemoveMappingRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The key of the entry to be removed.
-    bytes key = 4;
-    // The value of the entry to verify.
-    bytes value = 5;
-}
-
-// A request to replace the mapping for the specified key
-// with the specified value in a cache only if the specified
-// key is associated with some value in that cache.
-message ReplaceRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The key of the entry to be replaced.
-    bytes key = 4;
-    // The value of the entry to be replaced.
-    bytes value = 5;
-}
-
-// A request to replace the mapping for the specified key
-// with the specified newValue in a cache only if the specified
-// key is associated with the specified previousValue in
-// that cache.
-message ReplaceMappingRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The key of the entry to be replaced.
-    bytes key = 4;
-    // The previous value that should exist in the cache.
-    bytes previousValue = 5;
-    // The new value to put.
-    bytes newValue = 6;
-}
-
-// A request for a page of data from a cache.
-// This request is used for implementing methods such as NamedCache.keySet(),
-// NamedCache.entrySet() and NamedCache.values() where it would be impractical
-// to return the whole data set in one response.
-message PageRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format
-    string format = 3;
-    // An opaque cookie to track the requested page.
-    bytes cookie = 4;
-}
-
-// A cache entry key/value pair.
-message EntryResult {
-    // The cache entry key.
-    bytes key = 1;
-    // The cache entry value.
-    bytes value = 2;
-    // An opaque cookie to track the requested page.
-    bytes cookie = 3;
-}
-
-// A key value pair.
-message Entry {
-    // The cache entry key.
-    bytes key = 1;
-    // The value of the entry.
-    bytes value = 2;
-}
-
-// A request to truncate a cache.
-message TruncateRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-}
-
-// A request to add an index to a cache
-message AddIndexRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The serialized ValueExtractor to use to create the index.
-    bytes extractor = 4;
-    // A flag indicating whether to sort the index.
-    bool sorted = 5;
-    // The optional comparator to use to sort the index.
-    bytes comparator = 6;
-}
-
-// A request to remove an index from a cache
-message RemoveIndexRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The serialized ValueExtractor to use to create the index.
-    bytes extractor = 4;
-}
-
-// A request to aggreagte entries in a cache.
-message AggregateRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The serialized EntryAggregator to aggregate.
-    bytes aggregator = 4;
-    // The optional set of serialized keys of the entries to aggregate.
-    repeated bytes keys = 5;
-    // The optional serialized Filter to identify the entries to aggregate.
-    bytes filter = 6;
-}
-
-// A request to invoke an EntryProcessor against a single entry.
-message InvokeRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The serialized EntryProcessor to invoke.
-    bytes processor = 4;
-    // The serialized key of the entry to process.
-    bytes key = 5;
-}
-
-// A request to invoke an entry processor against a number of entries.
-message InvokeAllRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The serialized EntryProcessor to invoke.
-    bytes processor = 4;
-    // The optional set of serialized keys of the entries to process.
-    repeated bytes keys = 5;
-    // The optional serialized Filter to identify the entries to process.
-    bytes filter = 6;
-}
-
-// A request to get a set of entries from a cache.
-message EntrySetRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The serialized Filter to identify the entries to return.
-    bytes filter = 4;
-    // The optional comparator to use to sort the returned entries.
-    bytes comparator = 5;
-}
-
-// A request to get a set of keys from a cache.
-message KeySetRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The serialized Filter to identify the keys to return.
-    bytes filter = 4;
-}
-
-// A request to get a collection of values from a cache.
-message ValuesRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // The serialized Filter to identify the values to return.
-    bytes filter = 4;
-    // The optional comparator to use to sort the returned values.
-    bytes comparator = 5;
-}
-
-// An optional value.
-message OptionalValue {
-    // A flag indicating whether the value is present.
-    bool present = 1;
-    // The serialized value.
-    bytes value = 2;
-}
-
-// A message to subscribe to or unsubscribe from MapEvents for a cache.
-message MapListenerRequest {
-    // The scope name to use to obtain the cache.
-    string scope = 1;
-    // The name of the cache.
-    string cache = 2;
-    // The serialization format.
-    string format = 3;
-    // A unique identifier for the request so that the client
-    // can match a request to a response
-    string uid = 4;
-    // An enum representing the request type
-    enum RequestType {
-        // The request to initialise the channel.
-        INIT = 0;
-        // The request is for a key listener.
-        KEY = 1;
-        // The request is for a Filter listener.
-        FILTER = 2;
-    }
-    // The type of the request.
-    RequestType type = 5;
-    // The serialized Filter to identify the entries to subscribe to.
-    bytes filter = 6;
-    // The serialized key to identify the entry to subscribe to.
-    bytes key = 7;
-    // A flag set to true to indicate that the MapEvent objects do
-    // not have to include the OldValue and NewValue property values
-    // in order to allow optimizations
-    bool lite = 8;
-    // A flag indicating whether to subscribe to (true) or unsubscribe from (false) events.
-    bool subscribe = 9;
-    // A flag set to true to indicate that the listener is a priming listener.
-    bool priming = 10;
-    // An optional serialized MapTrigger.
-    bytes trigger = 11;
-    // A unique filter identifier.
-    int64 filterId = 12;
-}
-
-// A response to indicate that a MapListener was subscribed to a cache.
-message MapListenerResponse {
-    // A response can be one of either a subscribed response or an event response.
-    oneof response_type {
-        MapListenerSubscribedResponse subscribed = 1;
-        MapListenerUnsubscribedResponse unsubscribed = 2;
-        MapEventResponse event = 3;
-        MapListenerErrorResponse error = 4;
-        CacheDestroyedResponse destroyed = 5;
-        CacheTruncatedResponse truncated = 6;
-    }
-}
-
-// A response to indicate that a MapListener was subscribed to a cache.
-message MapListenerSubscribedResponse {
-    string uid = 1;
-}
-
-// A response to indicate that a MapListener was unsubscribed from a cache.
-message MapListenerUnsubscribedResponse {
-    string uid = 1;
-}
-
-// A response to indicate that a cache was destroyed.
-message CacheDestroyedResponse {
-    string cache = 1;
-}
-
-// A response to indicate that a cache was truncated.
-message CacheTruncatedResponse {
-    string cache = 1;
-}
-
-// A response to indicate that an error occurred processing a MapListener request.
-message MapListenerErrorResponse {
-    string uid = 1;
-    string message = 2;
-    int32 code = 3;
-    repeated string stack = 4;
-}
-
-// A response containing a MapEvent for a MapListener
-message MapEventResponse {
-    // The type of the event
-    int32 id = 1;
-    // The key of the entry
-    bytes key = 2;
-    // The new value of the entry
-    bytes newValue = 3;
-    // The old value of the entry
-    bytes oldValue = 4;
-    // An enum of TransformationState values to describes how a CacheEvent has been or should be transformed.
-    enum TransformationState {
-        // Value used to indicate that an event is non-transformable and should
-        // not be passed to any transformer-based listeners.
-        NON_TRANSFORMABLE = 0;
-        // Value used to indicate that an event is transformable and could be
-        // passed to transformer-based listeners.
-        TRANSFORMABLE = 1;
-        // Value used to indicate that an event has been transformed, and should
-        // only be passed to transformer-based listeners.
-        TRANSFORMED = 2;
-    }
-    // TransformationState describes how a CacheEvent has been or should be transformed.
-    TransformationState transformationState = 5;
-    // The Filter identifiers applicable to the event.
-    repeated int64 filterIds = 6;
-    // A flag indicating whether the event is a synthetic event.
-    bool synthetic = 7;
-    // A flag indicating whether the event is a priming event.
-    bool priming = 8;
-}
-
-
Index: etc/proto/services.proto
===================================================================
diff --git a/etc/proto/services.proto b/etc/proto/services.proto
deleted file mode 100644
--- a/etc/proto/services.proto	(revision 7ed18d6c9ea66aec63a655a1b556e3c67cdb16b6)
+++ /dev/null	(revision 7ed18d6c9ea66aec63a655a1b556e3c67cdb16b6)
@@ -1,155 +0,0 @@
-/*
- * Copyright (c) 2020, Oracle and/or its affiliates.
- *
- * Licensed under the Universal Permissive License v 1.0 as shown at
- * http://oss.oracle.com/licenses/upl.
- */
-
-// Authors:
-//  Mahesh Kannan
-//  Jonathan Knight
-
-// NamedCacheService service definition.
-
-syntax = "proto3";
-
-package coherence;
-
-import "messages.proto";
-import "google/protobuf/empty.proto";
-import "google/protobuf/wrappers.proto";
-
-option java_multiple_files = true;
-option java_package = "com.oracle.coherence.grpc";
-
-// A gRPC NamedCache service.
-//
-service NamedCacheService {
-
-    // Add an index to a cache.
-    rpc addIndex (AddIndexRequest) returns (google.protobuf.Empty) {
-    }
-
-    // Obtain the results of running an entry aggregator against the cache.
-    // The aggregator may run against entries specified by key or entries
-    // matching a given filter.
-    rpc aggregate (AggregateRequest) returns (google.protobuf.BytesValue) {
-    }
-
-    // Clear a cache.
-    rpc clear (ClearRequest) returns (google.protobuf.Empty) {
-    }
-
-    // Check if this map contains a mapping for the specified key to the specified value.
-    rpc containsEntry (ContainsEntryRequest) returns (google.protobuf.BoolValue) {
-    }
-
-    // Check if this map contains a mapping for the specified key.
-    rpc containsKey (ContainsKeyRequest) returns (google.protobuf.BoolValue) {
-    }
-
-    // Check if this map contains a mapping for the specified value.
-    rpc containsValue (ContainsValueRequest) returns (google.protobuf.BoolValue) {
-    }
-
-    // Destroy a cache.
-    rpc destroy (DestroyRequest) returns (google.protobuf.Empty) {
-    }
-
-    // Obtain all of the entries in the cache where the cache entries
-    // match a given filter.
-    rpc entrySet (EntrySetRequest) returns (stream Entry) {
-    }
-
-    // Sets up a bidirectional channel for cache events.
-    rpc events (stream MapListenerRequest) returns (stream MapListenerResponse) {
-    }
-
-    // Get a value for a given key from a cache.
-    rpc get (GetRequest) returns (OptionalValue) {
-    }
-
-    // Get all of the values from a cache for a given collection of keys.
-    rpc getAll (GetAllRequest) returns (stream Entry) {
-    }
-
-    // Invoke an entry processor against an entry in a cache.
-    rpc invoke (InvokeRequest) returns (google.protobuf.BytesValue) {
-    }
-
-    // Invoke an entry processor against a number of entries in a cache.
-    rpc invokeAll (InvokeAllRequest) returns (stream Entry) {
-    }
-
-    // Determine whether a cache is empty.
-    rpc isEmpty (IsEmptyRequest) returns (google.protobuf.BoolValue) {
-    }
-
-    // Obtain all of the keys in the cache where the cache entries
-    // match a given filter.
-    rpc keySet (KeySetRequest) returns (stream google.protobuf.BytesValue) {
-    }
-
-    // Get the next page of a paged entry set request.
-    rpc nextEntrySetPage (PageRequest) returns (stream EntryResult) {
-    }
-
-    // Get the next page of a paged key set request.
-    rpc nextKeySetPage (PageRequest) returns (stream google.protobuf.BytesValue) {
-    }
-
-    // Associate the specified value with the specified key in this cache.
-    // If the cache previously contained a mapping for the key, the old value
-    // is replaced by the specified value.
-    // An optional expiry (TTL) value may be set for the entry to expire the
-    // entry from the cache after that time.
-    rpc put (PutRequest) returns (google.protobuf.BytesValue) {
-    }
-
-    // Copies all of the mappings from the request into the cache.
-    rpc putAll (PutAllRequest) returns (google.protobuf.Empty) {
-    }
-
-    // If the specified key is not already associated with a value (or is mapped
-    // to null associate it with the given value and returns null, else return
-    // the current value.
-    rpc putIfAbsent (PutIfAbsentRequest) returns (google.protobuf.BytesValue) {
-    }
-
-    // Remove the mapping that is associated with the specified key.
-    rpc remove (RemoveRequest) returns (google.protobuf.BytesValue) {
-    }
-
-    // Remove an index from the cache.
-    rpc removeIndex (RemoveIndexRequest) returns (google.protobuf.Empty) {
-    }
-
-    // Remove the mapping that is associated with the specified key only
-    // if the mapping exists in the cache.
-    rpc removeMapping (RemoveMappingRequest) returns (google.protobuf.BoolValue) {
-    }
-
-    // Replace the entry for the specified key only if it is currently
-    // mapped to some value.
-    rpc replace (ReplaceRequest) returns (google.protobuf.BytesValue) {
-    }
-
-    // Replace the mapping for the specified key only if currently mapped
-    // to the specified value.
-    rpc replaceMapping (ReplaceMappingRequest) returns (google.protobuf.BoolValue) {
-    }
-
-    // Determine the number of entries in a cache.
-    rpc size (SizeRequest) returns (google.protobuf.Int32Value) {
-    }
-
-    // Truncate a cache. This is the same as clearing a cache but no
-    // cache entry events will be generated.
-    rpc truncate (TruncateRequest) returns (google.protobuf.Empty) {
-    }
-
-    // Obtain all of the values in the cache where the cache entries
-    // match a given filter.
-    rpc values (ValuesRequest) returns (stream google.protobuf.BytesValue) {
-    }
-}
\ No newline at end of file
Index: src/coherence/util_v1.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Copyright (c) 2022, 2023 Oracle and/or its affiliates.\n# Licensed under the Universal Permissive License v 1.0 as shown at\n# https://oss.oracle.com/licenses/upl.\n\nfrom __future__ import annotations\n\nimport asyncio\nimport logging\nimport sys\nimport threading\nimport time\nfrom asyncio import Event\nfrom typing import Optional, TypeVar\n\nimport grpc\nfrom google.protobuf.json_format import MessageToJson\nfrom requests import Response\n\nfrom . import cache_service_messages_v1_pb2, proxy_service_messages_v1_pb2,common_messages_v1_pb2\nfrom .aggregator import EntryAggregator\nfrom .comparator import Comparator\nfrom .extractor import ValueExtractor\nfrom .filter import Filter, Filters, MapEventFilter\nfrom .messages_pb2 import (  # type: ignore\n    AddIndexRequest,\n    AggregateRequest,\n    ClearRequest,\n    ContainsKeyRequest,\n    ContainsValueRequest,\n    DestroyRequest,\n    Entry,\n    EntrySetRequest,\n    GetAllRequest,\n    GetRequest,\n    InvokeAllRequest,\n    InvokeRequest,\n    IsEmptyRequest,\n    KeySetRequest,\n    MapListenerRequest,\n    PageRequest,\n    PutAllRequest,\n    PutIfAbsentRequest,\n    PutRequest,\n    RemoveIndexRequest,\n    RemoveMappingRequest,\n    RemoveRequest,\n    ReplaceMappingRequest,\n    ReplaceRequest,\n    SizeRequest,\n    TruncateRequest,\n    ValuesRequest,\n)\nfrom .processor import EntryProcessor\nfrom .serialization import Serializer\nfrom google.protobuf.any_pb2 import Any\nfrom google.protobuf.wrappers_pb2 import BytesValue\n\nE = TypeVar(\"E\")\nK = TypeVar(\"K\")\nR = TypeVar(\"R\")\nT = TypeVar(\"T\")\nV = TypeVar(\"V\")\n\nCOH_LOG = logging.getLogger(\"coherence\")\n\n\nclass Request_ID_Generator:\n    _generator = None\n\n    def __init__(self):\n        self._lock = threading.Lock()\n        self._counter = 0\n\n    @classmethod\n    def generator(cls):\n        if Request_ID_Generator._generator is None:\n            Request_ID_Generator._generator = Request_ID_Generator()\n        return Request_ID_Generator._generator\n\n    @classmethod\n    def get_next_id(cls):\n        generator = cls.generator()\n        with generator._lock:\n            if generator._counter == sys.maxsize:\n                generator._counter = 0\n            else:\n                generator._counter += 1\n            return generator._counter\n\n\nclass RequestFactory_v1:\n\n    def __init__(self, cache_name: str, cache_id: int, session: Session, serializer: Serializer) -> None:\n        self._cache_name = cache_name\n        self._cache_id = cache_id\n        self._session = session\n        self._scope: str = session.scope\n        self._serializer: Serializer = serializer\n        # self.__uidPrefix: str = \"-\" + cache_name + \"-\" + str(time.time_ns())\n        # self.__next_request_id: int = 0\n        # self.__next_filter_id: int = 0\n\n    @property\n    def cache_id(self):\n        return self._cache_id\n\n    @cache_id.setter\n    def cache_id(self, value):\n        self._cache_id = value\n\n    def get_serializer(self) -> Serializer:\n        return self._serializer\n\n    def create_proxy_request(self, named_cache_request:\n    cache_service_messages_v1_pb2.NamedCacheRequest) -> proxy_service_messages_v1_pb2.ProxyRequest:\n        any_named_cache_request = Any()\n        any_named_cache_request.Pack(named_cache_request)\n        req_id = Request_ID_Generator.get_next_id()\n        proxy_request = proxy_service_messages_v1_pb2.ProxyRequest(\n            id=req_id,\n            message=any_named_cache_request,\n        )\n        return proxy_request\n\n    def ensure_request(self, cache_name: str) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        cache_request = cache_service_messages_v1_pb2.EnsureCacheRequest(\n            cache=cache_name\n        )\n\n        any_cache_request = Any()\n        any_cache_request.Pack(cache_request)\n\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.EnsureCache,\n            message=any_cache_request,\n        )\n        return named_cache_request\n\n    def put_request(self, key: K, value: V, ttl: int = -1) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        cache_request = cache_service_messages_v1_pb2.PutRequest(\n            key=self._serializer.serialize(key),     # Serialized key\n            value=self._serializer.serialize(value), # Serialized value\n            ttl=ttl\n        )\n\n        any_cache_request = Any()\n        any_cache_request.Pack(cache_request)\n\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Put,\n            cacheId=self.cache_id,\n            message=any_cache_request,\n        )\n\n        return named_cache_request\n\n    def get_request(self, key: K) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        cache_request = BytesValue(value=self._serializer.serialize(key))\n\n        any_cache_request = Any()\n        any_cache_request.Pack(cache_request)\n\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Get,\n            cacheId=self.cache_id,\n            message=any_cache_request,\n        )\n\n        return named_cache_request\n\n    def get_all_request(self, keys: set[K]) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        if keys is None:\n            raise ValueError(\"Must specify a set of keys\")\n\n        l = list()\n        for k in keys:\n            l.append(self._serializer.serialize(k))\n        cache_request = common_messages_v1_pb2.CollectionOfBytesValues(\n            values=l,\n        )\n\n        any_cache_request = Any()\n        any_cache_request.Pack(cache_request)\n\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.GetAll,\n            cacheId=self.cache_id,\n            message=any_cache_request,\n        )\n\n        return named_cache_request\n\n    def put_if_absent_request(self, key: K, value: V, ttl: int = -1) -> PutIfAbsentRequest:\n        cache_request = cache_service_messages_v1_pb2.PutRequest(\n            key=self._serializer.serialize(key),     # Serialized key\n            value=self._serializer.serialize(value), # Serialized value\n            ttl=ttl\n        )\n\n        any_cache_request = Any()\n        any_cache_request.Pack(cache_request)\n\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.PutIfAbsent,\n            cacheId=self.cache_id,\n            message=any_cache_request,\n        )\n\n        return named_cache_request\n\n    def put_all_request(self, map: dict[K, V], ttl: Optional[\n        int] = 0) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        entry_list = list()\n        for key, value in map.items():\n            k = self._serializer.serialize(key)\n            v = self._serializer.serialize(value)\n            e = common_messages_v1_pb2.BinaryKeyAndValue(key=k, value=v)\n            entry_list.append(e)\n        cache_request = cache_service_messages_v1_pb2.PutAllRequest(\n            entries=entry_list,\n            ttl=ttl,\n        )\n\n        any_cache_request = Any()\n        any_cache_request.Pack(cache_request)\n\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.PutAll,\n            cacheId=self.cache_id,\n            message=any_cache_request,\n        )\n\n        return named_cache_request\n\n    def clear_request(self) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Clear,\n            cacheId=self.cache_id,\n        )\n        return named_cache_request\n\n    def destroy_request(\n            self) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Destroy,\n            cacheId=self.cache_id,\n        )\n        return named_cache_request\n\n    def truncate_request(\n            self) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Truncate,\n            cacheId=self.cache_id,\n        )\n        return named_cache_request\n\n    def remove_request(self,\n                       key: K) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        cache_request = BytesValue(value=self._serializer.serialize(key))\n\n        any_cache_request = Any()\n        any_cache_request.Pack(cache_request)\n\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Remove,\n            cacheId=self.cache_id,\n            message=any_cache_request,\n        )\n\n        return named_cache_request\n\n\n    def remove_mapping_request(self, key: K, value: V) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        cache_request = common_messages_v1_pb2.BinaryKeyAndValue(\n            key=self._serializer.serialize(key),\n            value=self._serializer.serialize(value))\n\n        any_cache_request = Any()\n        any_cache_request.Pack(cache_request)\n\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.RemoveMapping,\n            cacheId=self.cache_id,\n            message=any_cache_request,\n        )\n\n        return named_cache_request\n\n    def replace_request(self, key: K, value: V) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        cache_request = common_messages_v1_pb2.BinaryKeyAndValue(\n                            key=self._serializer.serialize(key),\n                            value=self._serializer.serialize(value))\n\n        any_cache_request = Any()\n        any_cache_request.Pack(cache_request)\n\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Replace,\n            cacheId=self.cache_id,\n            message=any_cache_request,\n        )\n\n        return named_cache_request\n\n    def replace_mapping_request(self, key: K, old_value: V,\n                                new_value: V) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        cache_request = cache_service_messages_v1_pb2.ReplaceMappingRequest(\n            key=self._serializer.serialize(key),\n            previousValue=self._serializer.serialize(old_value),\n            newValue=self._serializer.serialize(new_value),\n        )\n\n        any_cache_request = Any()\n        any_cache_request.Pack(cache_request)\n\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.ReplaceMapping,\n            cacheId=self.cache_id,\n            message=any_cache_request,\n        )\n\n        return named_cache_request\n\n    def contains_key_request(self, key: K) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        cache_request = BytesValue(value=self._serializer.serialize(key))\n\n        any_cache_request = Any()\n        any_cache_request.Pack(cache_request)\n\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.ContainsKey,\n            cacheId=self.cache_id,\n            message=any_cache_request,\n        )\n\n        return named_cache_request\n\n    def contains_value_request(self,\n                               value: V) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        cache_request = BytesValue(value=self._serializer.serialize(value))\n\n        any_cache_request = Any()\n        any_cache_request.Pack(cache_request)\n\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.ContainsValue,\n            cacheId=self.cache_id,\n            message=any_cache_request,\n        )\n\n        return named_cache_request\n\n    def is_empty_request(\n            self) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.IsEmpty,\n            cacheId=self.cache_id,\n        )\n        return named_cache_request\n\n    def size_request(self) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Size,\n            cacheId=self.cache_id,\n        )\n        return named_cache_request\n\n    def invoke_request(self, key: K, processor: EntryProcessor[\n        R]) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        cache_request = cache_service_messages_v1_pb2.ExecuteRequest(\n            agent=self._serializer.serialize(processor),\n            keys=cache_service_messages_v1_pb2.KeysOrFilter(\n                key=self._serializer.serialize(key),\n            )\n        )\n\n        any_cache_request = Any()\n        any_cache_request.Pack(cache_request)\n\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Invoke,\n            cacheId=self.cache_id,\n            message=any_cache_request,\n        )\n\n        return named_cache_request\n\n    def invoke_all_request(\n            self, processor: EntryProcessor[R], keys: Optional[set[K]] = None,\n            filter: Optional[Filter] = None\n    ) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        if keys is not None and filter is not None:\n            raise ValueError(\"keys and filter are mutually exclusive\")\n\n        if keys is not None:\n            list_of_keys = list()\n            for key in keys:\n                list_of_keys.append(self._serializer.serialize(key))\n            cache_request = cache_service_messages_v1_pb2.ExecuteRequest(\n                agent=self._serializer.serialize(processor),\n                keys=cache_service_messages_v1_pb2.KeysOrFilter(\n                    keys=common_messages_v1_pb2.CollectionOfBytesValues(\n                        values=list_of_keys,\n                    ),\n                )\n            )\n        elif filter is not None:\n            cache_request = cache_service_messages_v1_pb2.ExecuteRequest(\n                agent=self._serializer.serialize(processor),\n                keys=cache_service_messages_v1_pb2.KeysOrFilter(\n                    filter=self._serializer.serialize(filter),\n                )\n            )\n        else:\n            cache_request = cache_service_messages_v1_pb2.ExecuteRequest(\n                agent=self._serializer.serialize(processor),\n            )\n\n        any_cache_request = Any()\n        any_cache_request.Pack(cache_request)\n\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Invoke,\n            cacheId=self.cache_id,\n            message=any_cache_request,\n        )\n\n        return named_cache_request\n\n    def aggregate_request(\n        self, aggregator: EntryAggregator[R], keys: Optional[set[K]] = None, filter: Optional[Filter] = None\n    ) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        if keys is not None and filter is not None:\n            raise ValueError(\"keys and filter are mutually exclusive\")\n\n        if keys is not None:\n            list_of_keys = list()\n            for key in keys:\n                list_of_keys.append(self._serializer.serialize(key))\n            cache_request = cache_service_messages_v1_pb2.ExecuteRequest(\n                agent=self._serializer.serialize(aggregator),\n                keys=cache_service_messages_v1_pb2.KeysOrFilter(\n                    keys=common_messages_v1_pb2.CollectionOfBytesValues(\n                        values=list_of_keys,\n                    ),\n                )\n            )\n        elif filter is not None:\n            cache_request = cache_service_messages_v1_pb2.ExecuteRequest(\n                agent=self._serializer.serialize(aggregator),\n                keys=cache_service_messages_v1_pb2.KeysOrFilter(\n                    filter=self._serializer.serialize(filter),\n                )\n            )\n        else:\n            cache_request = cache_service_messages_v1_pb2.ExecuteRequest(\n                agent=self._serializer.serialize(aggregator),\n            )\n\n        any_cache_request = Any()\n        any_cache_request.Pack(cache_request)\n\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Aggregate,\n            cacheId=self.cache_id,\n            message=any_cache_request,\n        )\n\n        return named_cache_request\n\n    def values_request(self, filter: Optional[Filter] = None, comparator: Optional[Comparator] = None) -> ValuesRequest:\n        if filter is None and comparator is not None:\n            raise ValueError(\"Filter cannot be None\")\n\n        r: ValuesRequest = ValuesRequest(\n            scope=self._scope,\n            cache=self._cache_name,\n            format=self._serializer.format,\n        )\n\n        if filter is not None:\n            r.filter = self._serializer.serialize(filter)\n\n        if comparator is not None:\n            r.comparator = self._serializer.serialize(comparator)\n\n        return r\n\n    def keys_request(self, filter: Optional[Filter] = None) -> KeySetRequest:\n        r: KeySetRequest = KeySetRequest(\n            scope=self._scope,\n            cache=self._cache_name,\n            format=self._serializer.format,\n        )\n\n        if filter is not None:\n            r.filter = self._serializer.serialize(filter)\n\n        return r\n\n    def entries_request(\n        self, filter: Optional[Filter] = None, comparator: Optional[Comparator] = None\n    ) -> EntrySetRequest:\n        if filter is None and comparator is not None:\n            raise ValueError(\"Filter cannot be None\")\n\n        r: EntrySetRequest = EntrySetRequest(\n            scope=self._scope,\n            cache=self._cache_name,\n            format=self._serializer.format,\n        )\n\n        if filter is not None:\n            r.filter = self._serializer.serialize(filter)\n\n        if comparator is not None:\n            r.comparator = self._serializer.serialize(comparator)\n\n        return r\n\n    def page_request(self, cookie: bytes) -> PageRequest:\n        \"\"\"\n        Creates a gRPC PageRequest.\n\n        :param cookie: the cookie used for paging\n        :return: a new PageRequest\n        \"\"\"\n\n        r: PageRequest = PageRequest(\n            scope=self._scope, cache=self._cache_name, format=self._serializer.format, cookie=cookie\n        )\n\n        return r\n\n    def map_listener_request(\n        self, subscribe: bool, lite: bool = False, *, key: Optional[K] = None, filter: Optional[Filter] = None\n    ) -> MapListenerRequest:\n        \"\"\"Creates a gRPC generated MapListenerRequest\"\"\"\n\n        if key is None and filter is None:\n            raise AssertionError(\"Must specify a key or a filter\")\n\n        request: MapListenerRequest = MapListenerRequest(\n            cache=self._cache_name, scope=self._scope, format=self._serializer.format\n        )\n\n        request.lite = lite\n        request.subscribe = subscribe\n        request.uid = self.__generate_next_request_id(\"key\" if key is not None else \"filter\")\n        request.trigger = bytes()\n        request.priming = False\n\n        if key is not None:  # registering a key listener\n            request.type = MapListenerRequest.RequestType.KEY\n            request.key = self._serializer.serialize(key)\n        else:  # registering a Filter listener\n            request.type = MapListenerRequest.RequestType.FILTER\n            self.__next_filter_id += 1\n            request.filterId = self.__next_filter_id\n            filter_local: Filter = filter if filter is not None else Filters.always()\n            if not isinstance(filter_local, MapEventFilter):\n                filter_local = MapEventFilter.from_filter(filter_local)\n\n            request.filter = self._serializer.serialize(filter_local)\n\n        return request\n\n    def map_event_subscribe(self) -> MapListenerRequest:\n        request: MapListenerRequest = MapListenerRequest(\n            cache=self._cache_name, scope=self._scope, format=self._serializer.format\n        )\n        request.uid = self.__generate_next_request_id(\"init\")\n        request.subscribe = True\n        request.type = MapListenerRequest.RequestType.INIT\n\n        return request\n\n    def __generate_next_request_id(self, prefix: str) -> str:\n        \"\"\"Generates a prefix map-specific prefix when starting a MapEvent gRPC stream.\"\"\"\n        self.__next_request_id += 1\n        return prefix + self.__uidPrefix + str(self.__next_request_id)\n\n    def add_index_request(\n            self, extractor: ValueExtractor[T, E], ordered: bool = False,\n            comparator: Optional[Comparator] = None\n    ) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        cache_request = cache_service_messages_v1_pb2.IndexRequest(\n            add=True,\n            extractor=self._serializer.serialize(extractor),\n            sorted=ordered,\n        )\n        if comparator is not None:\n            cache_request.comparator = self._serializer.serialize(comparator)\n\n        any_cache_request = Any()\n        any_cache_request.Pack(cache_request)\n\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Index,\n            cacheId=self.cache_id,\n            message=any_cache_request,\n        )\n\n        return named_cache_request\n\n    def remove_index_request(self, extractor: ValueExtractor[\n        T, E]) -> cache_service_messages_v1_pb2.NamedCacheRequest:\n        cache_request = cache_service_messages_v1_pb2.IndexRequest(\n            add=False,\n            extractor=self._serializer.serialize(extractor),\n        )\n\n        any_cache_request = Any()\n        any_cache_request.Pack(cache_request)\n\n        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(\n            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Index,\n            cacheId=self.cache_id,\n            message=any_cache_request,\n        )\n\n        return named_cache_request\n\n\nclass StreamHandler:\n    theStreamHandler = None\n\n    def __init__(self, session: Session, stream: grpc.aio._call.StreamStreamCall):\n        self._session: Session = session\n        self._stream: grpc.aio._call.StreamStreamCall = stream\n        self._request_id_to_event_map = dict()\n        self._request_id_request_map = dict()\n        self.result_available = Event()\n        self.result_available.clear()\n        self.response_result = None\n        self.response_result_collection = list()\n        self._background_tasks = set()\n        task = asyncio.create_task(self.handle_response())\n        self._background_tasks.add(task)\n        task.add_done_callback(self._background_tasks.discard)\n\n    @property\n    def session(self) -> Session:\n        return self._session\n\n    @property\n    def stream(self) -> grpc.aio._call.StreamStreamCall:\n        return self._stream\n\n    @classmethod\n    def getStreamHandler(cls, session: Session, stream: grpc.aio._call.StreamStreamCall):\n        if cls.theStreamHandler is None:\n            cls.theStreamHandler = StreamHandler(session, stream)\n            return cls.theStreamHandler\n        else:\n            return cls.theStreamHandler\n\n    async def handle_response(self):\n        COH_LOG.setLevel(logging.DEBUG)\n        while not self.session.closed:\n            await asyncio.sleep(0)\n            response = await self.stream.read()\n            response_id = response.id\n            COH_LOG.debug(f\"response_id: {response_id}\")\n            if response_id == 0 :\n                self.handle_zero_id_response(response)\n            else:\n                if response.HasField(\"message\"):\n                    req_type = self._request_id_request_map[response_id].type\n                    if req_type == cache_service_messages_v1_pb2.NamedCacheRequestType.EnsureCache:\n                        named_cache_response = cache_service_messages_v1_pb2.NamedCacheResponse()\n                        response.message.Unpack(named_cache_response)\n                        # COH_LOG.info(f\"cache_id: {named_cache_response.cacheId}\")\n                        self.response_result = named_cache_response\n                    elif req_type == cache_service_messages_v1_pb2.NamedCacheRequestType.Put:\n                        named_cache_response = cache_service_messages_v1_pb2.NamedCacheResponse()\n                        response.message.Unpack(named_cache_response)\n                        self.response_result = named_cache_response\n                    elif req_type == cache_service_messages_v1_pb2.NamedCacheRequestType.PutIfAbsent:\n                        named_cache_response = cache_service_messages_v1_pb2.NamedCacheResponse()\n                        response.message.Unpack(named_cache_response)\n                        self.response_result = named_cache_response\n                    elif req_type == cache_service_messages_v1_pb2.NamedCacheRequestType.Get:\n                        named_cache_response = cache_service_messages_v1_pb2.NamedCacheResponse()\n                        response.message.Unpack(named_cache_response)\n                        self.response_result = named_cache_response\n                    elif req_type == cache_service_messages_v1_pb2.NamedCacheRequestType.GetAll:\n                        named_cache_response = cache_service_messages_v1_pb2.NamedCacheResponse()\n                        response.message.Unpack(named_cache_response)\n                        self.response_result_collection.append(named_cache_response)\n                    elif req_type == cache_service_messages_v1_pb2.NamedCacheRequestType.Remove:\n                        named_cache_response = cache_service_messages_v1_pb2.NamedCacheResponse()\n                        response.message.Unpack(named_cache_response)\n                        self.response_result = named_cache_response\n                    elif req_type == cache_service_messages_v1_pb2.NamedCacheRequestType.Replace:\n                        named_cache_response = cache_service_messages_v1_pb2.NamedCacheResponse()\n                        response.message.Unpack(named_cache_response)\n                        self.response_result = named_cache_response\n                    elif req_type == cache_service_messages_v1_pb2.NamedCacheRequestType.RemoveMapping:\n                        named_cache_response = cache_service_messages_v1_pb2.NamedCacheResponse()\n                        response.message.Unpack(named_cache_response)\n                        self.response_result = named_cache_response\n                    elif req_type == cache_service_messages_v1_pb2.NamedCacheRequestType.ReplaceMapping:\n                        named_cache_response = cache_service_messages_v1_pb2.NamedCacheResponse()\n                        response.message.Unpack(named_cache_response)\n                        self.response_result = named_cache_response\n                    elif req_type == cache_service_messages_v1_pb2.NamedCacheRequestType.ContainsKey:\n                        named_cache_response = cache_service_messages_v1_pb2.NamedCacheResponse()\n                        response.message.Unpack(named_cache_response)\n                        self.response_result = named_cache_response\n                    elif req_type == cache_service_messages_v1_pb2.NamedCacheRequestType.ContainsValue:\n                        named_cache_response = cache_service_messages_v1_pb2.NamedCacheResponse()\n                        response.message.Unpack(named_cache_response)\n                        self.response_result = named_cache_response\n                    elif req_type == cache_service_messages_v1_pb2.NamedCacheRequestType.IsEmpty:\n                        named_cache_response = cache_service_messages_v1_pb2.NamedCacheResponse()\n                        response.message.Unpack(named_cache_response)\n                        self.response_result = named_cache_response\n                    elif req_type == cache_service_messages_v1_pb2.NamedCacheRequestType.Size:\n                        named_cache_response = cache_service_messages_v1_pb2.NamedCacheResponse()\n                        response.message.Unpack(named_cache_response)\n                        self.response_result = named_cache_response\n                    elif req_type == cache_service_messages_v1_pb2.NamedCacheRequestType.Invoke:\n                        named_cache_response = cache_service_messages_v1_pb2.NamedCacheResponse()\n                        response.message.Unpack(named_cache_response)\n                        self.response_result_collection.append(named_cache_response)\n                    elif req_type == cache_service_messages_v1_pb2.NamedCacheRequestType.Aggregate:\n                        named_cache_response = cache_service_messages_v1_pb2.NamedCacheResponse()\n                        response.message.Unpack(named_cache_response)\n                        self.response_result = named_cache_response\n                    else:\n                        pass\n                elif response.HasField(\"init\"):\n                    self._request_id_to_event_map.pop(response_id)\n                    print(\"InitRequest request completed.\")\n                    self.result_available.set()\n                elif response.HasField(\"error\"):\n                    error_message = response.error\n                    print(f\"EnsureCache request failed with error: {error_message}\")\n                    return\n                elif response.HasField(\"complete\"):\n                    # self.session.request_id_map.pop(response_id)\n                    # COH_LOG.info(\"Complete response received successfully.\")\n                    self._request_id_to_event_map[response_id].set()\n\n    async def get_response(self, response_id: int):\n        await self._request_id_to_event_map[response_id].wait()\n        result = self.response_result\n        self.response_result = None\n        self._request_id_to_event_map[response_id].clear()\n        self._request_id_to_event_map.pop(response_id)\n        self._request_id_request_map.pop(response_id)\n        return result\n\n    async def get_response_collection(self, response_id: int):\n        await self._request_id_to_event_map[response_id].wait()\n        result = self.response_result_collection\n        self.response_result_collection = list()\n        self._request_id_to_event_map[response_id].clear()\n        self._request_id_to_event_map.pop(response_id)\n        self._request_id_request_map.pop(response_id)\n        return result\n\n    async def write_request(self, proxy_request: proxy_service_messages_v1_pb2.ProxyRequest,\n                            request_id: int,\n                            request: cache_service_messages_v1_pb2.NamedCacheRequest):\n        self._request_id_to_event_map[request_id] = Event()\n        self._request_id_to_event_map[request_id].clear()\n        self._request_id_request_map[request_id] = request\n        await self._stream.write(proxy_request)\n\n    def handle_zero_id_response(self, response):\n        if response.HasField(\"message\"):\n            named_cache_response = cache_service_messages_v1_pb2.NamedCacheResponse()\n            response.message.Unpack(named_cache_response)\n            type = named_cache_response.type\n            cache_id = named_cache_response.cacheId\n            if type == cache_service_messages_v1_pb2.ResponseType.Message:\n                pass\n            elif type == cache_service_messages_v1_pb2.ResponseType.MapEvent:\n                # Handle MapEvent Response\n                COH_LOG.debug(\"MapEvent Response type received\")\n                response_json = MessageToJson(named_cache_response)\n                COH_LOG.debug(response_json)\n                pass\n            elif type == cache_service_messages_v1_pb2.ResponseType.Destroyed:\n                # Handle Destroyed Response\n                COH_LOG.debug(\"Destroyed Response type received\")\n                response_json = MessageToJson(named_cache_response)\n                COH_LOG.debug(response_json)\n                pass\n            elif type == cache_service_messages_v1_pb2.ResponseType.Truncated:\n                # Handle Truncated Response\n                COH_LOG.debug(\"Truncated Response type received\")\n                response_json = MessageToJson(named_cache_response)\n                COH_LOG.debug(response_json)\n            else:\n                pass\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/coherence/util_v1.py b/src/coherence/util_v1.py
--- a/src/coherence/util_v1.py	(revision 7ed18d6c9ea66aec63a655a1b556e3c67cdb16b6)
+++ b/src/coherence/util_v1.py	(date 1725566513086)
@@ -10,7 +10,8 @@
 import threading
 import time
 from asyncio import Event
-from typing import Optional, TypeVar
+from functools import wraps
+from typing import Optional, TypeVar, Callable
 
 import grpc
 from google.protobuf.json_format import MessageToJson
@@ -63,6 +64,29 @@
 
 COH_LOG = logging.getLogger("coherence")
 
+def proxy_request(request_type):
+    def wrapper(func):
+        @wraps(func)
+        def inner(self, *args, **kwargs):
+            req_to_pack = func(self, *args, **kwargs)
+            packed_req = None
+
+            if req_to_pack is not None:
+                packed_req = Any()
+                packed_req.Pack(self.create_proxy_request(func()))
+
+            msg = cache_service_messages_v1_pb2.NamedCacheRequest(
+                type = request_type,
+                cacheId = self.cache_id)
+
+            if packed_req is not None:
+                msg.message = packed_req
+
+            return msg
+
+        return inner
+    return wrapper
+
 
 class Request_ID_Generator:
     _generator = None
@@ -136,256 +160,115 @@
         )
         return named_cache_request
 
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.Put)
     def put_request(self, key: K, value: V, ttl: int = -1) -> cache_service_messages_v1_pb2.NamedCacheRequest:
-        cache_request = cache_service_messages_v1_pb2.PutRequest(
+        return cache_service_messages_v1_pb2.PutRequest(
             key=self._serializer.serialize(key),     # Serialized key
             value=self._serializer.serialize(value), # Serialized value
             ttl=ttl
         )
 
-        any_cache_request = Any()
-        any_cache_request.Pack(cache_request)
-
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Put,
-            cacheId=self.cache_id,
-            message=any_cache_request,
-        )
-
-        return named_cache_request
-
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.Get)
     def get_request(self, key: K) -> cache_service_messages_v1_pb2.NamedCacheRequest:
-        cache_request = BytesValue(value=self._serializer.serialize(key))
-
-        any_cache_request = Any()
-        any_cache_request.Pack(cache_request)
+        return BytesValue(value=self._serializer.serialize(key))
 
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Get,
-            cacheId=self.cache_id,
-            message=any_cache_request,
-        )
-
-        return named_cache_request
-
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.GetAll)
     def get_all_request(self, keys: set[K]) -> cache_service_messages_v1_pb2.NamedCacheRequest:
         if keys is None:
             raise ValueError("Must specify a set of keys")
 
-        l = list()
-        for k in keys:
-            l.append(self._serializer.serialize(k))
-        cache_request = common_messages_v1_pb2.CollectionOfBytesValues(
-            values=l,
-        )
-
-        any_cache_request = Any()
-        any_cache_request.Pack(cache_request)
-
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.GetAll,
-            cacheId=self.cache_id,
-            message=any_cache_request,
+        return common_messages_v1_pb2.CollectionOfBytesValues(
+            values = list(self._serializer.serialize(k) for k in keys),
         )
 
-        return named_cache_request
-
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.PutIfAbsent)
     def put_if_absent_request(self, key: K, value: V, ttl: int = -1) -> PutIfAbsentRequest:
-        cache_request = cache_service_messages_v1_pb2.PutRequest(
+        return cache_service_messages_v1_pb2.PutRequest(
             key=self._serializer.serialize(key),     # Serialized key
             value=self._serializer.serialize(value), # Serialized value
             ttl=ttl
         )
 
-        any_cache_request = Any()
-        any_cache_request.Pack(cache_request)
-
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.PutIfAbsent,
-            cacheId=self.cache_id,
-            message=any_cache_request,
-        )
-
-        return named_cache_request
-
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.PutAll)
     def put_all_request(self, map: dict[K, V], ttl: Optional[
         int] = 0) -> cache_service_messages_v1_pb2.NamedCacheRequest:
-        entry_list = list()
-        for key, value in map.items():
-            k = self._serializer.serialize(key)
-            v = self._serializer.serialize(value)
-            e = common_messages_v1_pb2.BinaryKeyAndValue(key=k, value=v)
-            entry_list.append(e)
-        cache_request = cache_service_messages_v1_pb2.PutAllRequest(
-            entries=entry_list,
+        return cache_service_messages_v1_pb2.PutAllRequest(
+            entries=list(common_messages_v1_pb2.BinaryKeyAndValue(
+                self._serializer.serialize(key),
+                self._serializer.serialize(value)) for key, value in
+                         map.items()),
             ttl=ttl,
         )
 
-        any_cache_request = Any()
-        any_cache_request.Pack(cache_request)
-
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.PutAll,
-            cacheId=self.cache_id,
-            message=any_cache_request,
-        )
-
-        return named_cache_request
-
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.Clear)
     def clear_request(self) -> cache_service_messages_v1_pb2.NamedCacheRequest:
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Clear,
-            cacheId=self.cache_id,
-        )
-        return named_cache_request
+        return None
 
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.Destroy)
     def destroy_request(
             self) -> cache_service_messages_v1_pb2.NamedCacheRequest:
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Destroy,
-            cacheId=self.cache_id,
-        )
-        return named_cache_request
+        return None
 
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.Truncate)
     def truncate_request(
             self) -> cache_service_messages_v1_pb2.NamedCacheRequest:
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Truncate,
-            cacheId=self.cache_id,
-        )
-        return named_cache_request
+        return None
 
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.Remove)
     def remove_request(self,
                        key: K) -> cache_service_messages_v1_pb2.NamedCacheRequest:
-        cache_request = BytesValue(value=self._serializer.serialize(key))
-
-        any_cache_request = Any()
-        any_cache_request.Pack(cache_request)
-
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Remove,
-            cacheId=self.cache_id,
-            message=any_cache_request,
-        )
-
-        return named_cache_request
+        return BytesValue(value=self._serializer.serialize(key))
 
-
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.RemoveMapping)
     def remove_mapping_request(self, key: K, value: V) -> cache_service_messages_v1_pb2.NamedCacheRequest:
-        cache_request = common_messages_v1_pb2.BinaryKeyAndValue(
+        return common_messages_v1_pb2.BinaryKeyAndValue(
             key=self._serializer.serialize(key),
             value=self._serializer.serialize(value))
 
-        any_cache_request = Any()
-        any_cache_request.Pack(cache_request)
-
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.RemoveMapping,
-            cacheId=self.cache_id,
-            message=any_cache_request,
-        )
-
-        return named_cache_request
-
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.Replace)
     def replace_request(self, key: K, value: V) -> cache_service_messages_v1_pb2.NamedCacheRequest:
-        cache_request = common_messages_v1_pb2.BinaryKeyAndValue(
-                            key=self._serializer.serialize(key),
-                            value=self._serializer.serialize(value))
+        return common_messages_v1_pb2.BinaryKeyAndValue(
+            key=self._serializer.serialize(key),
+            value=self._serializer.serialize(value))
 
-        any_cache_request = Any()
-        any_cache_request.Pack(cache_request)
-
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Replace,
-            cacheId=self.cache_id,
-            message=any_cache_request,
-        )
-
-        return named_cache_request
-
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.ReplaceMapping)
     def replace_mapping_request(self, key: K, old_value: V,
                                 new_value: V) -> cache_service_messages_v1_pb2.NamedCacheRequest:
-        cache_request = cache_service_messages_v1_pb2.ReplaceMappingRequest(
+        return cache_service_messages_v1_pb2.ReplaceMappingRequest(
             key=self._serializer.serialize(key),
             previousValue=self._serializer.serialize(old_value),
             newValue=self._serializer.serialize(new_value),
         )
 
-        any_cache_request = Any()
-        any_cache_request.Pack(cache_request)
-
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.ReplaceMapping,
-            cacheId=self.cache_id,
-            message=any_cache_request,
-        )
-
-        return named_cache_request
-
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.ContainsKey)
     def contains_key_request(self, key: K) -> cache_service_messages_v1_pb2.NamedCacheRequest:
-        cache_request = BytesValue(value=self._serializer.serialize(key))
-
-        any_cache_request = Any()
-        any_cache_request.Pack(cache_request)
+        return BytesValue(value=self._serializer.serialize(key))
 
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.ContainsKey,
-            cacheId=self.cache_id,
-            message=any_cache_request,
-        )
-
-        return named_cache_request
-
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.ContainsValue)
     def contains_value_request(self,
                                value: V) -> cache_service_messages_v1_pb2.NamedCacheRequest:
-        cache_request = BytesValue(value=self._serializer.serialize(value))
-
-        any_cache_request = Any()
-        any_cache_request.Pack(cache_request)
+        return BytesValue(value=self._serializer.serialize(value))
 
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.ContainsValue,
-            cacheId=self.cache_id,
-            message=any_cache_request,
-        )
-
-        return named_cache_request
-
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.IsEmpty)
     def is_empty_request(
             self) -> cache_service_messages_v1_pb2.NamedCacheRequest:
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.IsEmpty,
-            cacheId=self.cache_id,
-        )
-        return named_cache_request
+        return None
 
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.Size)
     def size_request(self) -> cache_service_messages_v1_pb2.NamedCacheRequest:
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Size,
-            cacheId=self.cache_id,
-        )
-        return named_cache_request
+        return None
 
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.Invoke)
     def invoke_request(self, key: K, processor: EntryProcessor[
         R]) -> cache_service_messages_v1_pb2.NamedCacheRequest:
-        cache_request = cache_service_messages_v1_pb2.ExecuteRequest(
+        return cache_service_messages_v1_pb2.ExecuteRequest(
             agent=self._serializer.serialize(processor),
             keys=cache_service_messages_v1_pb2.KeysOrFilter(
                 key=self._serializer.serialize(key),
             )
         )
 
-        any_cache_request = Any()
-        any_cache_request.Pack(cache_request)
-
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Invoke,
-            cacheId=self.cache_id,
-            message=any_cache_request,
-        )
-
-        return named_cache_request
-
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.Invoke)
     def invoke_all_request(
             self, processor: EntryProcessor[R], keys: Optional[set[K]] = None,
             filter: Optional[Filter] = None
@@ -417,17 +300,9 @@
                 agent=self._serializer.serialize(processor),
             )
 
-        any_cache_request = Any()
-        any_cache_request.Pack(cache_request)
-
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Invoke,
-            cacheId=self.cache_id,
-            message=any_cache_request,
-        )
+        return cache_request
 
-        return named_cache_request
-
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.Aggregate)
     def aggregate_request(
         self, aggregator: EntryAggregator[R], keys: Optional[set[K]] = None, filter: Optional[Filter] = None
     ) -> cache_service_messages_v1_pb2.NamedCacheRequest:
@@ -458,66 +333,48 @@
                 agent=self._serializer.serialize(aggregator),
             )
 
-        any_cache_request = Any()
-        any_cache_request.Pack(cache_request)
-
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Aggregate,
-            cacheId=self.cache_id,
-            message=any_cache_request,
-        )
+        return cache_request
 
-        return named_cache_request
-
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.QueryValues)
     def values_request(self, filter: Optional[Filter] = None, comparator: Optional[Comparator] = None) -> ValuesRequest:
         if filter is None and comparator is not None:
             raise ValueError("Filter cannot be None")
 
-        r: ValuesRequest = ValuesRequest(
-            scope=self._scope,
-            cache=self._cache_name,
-            format=self._serializer.format,
-        )
+        req = cache_service_messages_v1_pb2.QueryRequest()
 
         if filter is not None:
-            r.filter = self._serializer.serialize(filter)
+            req.filter = self._serializer.serialize(filter)
 
         if comparator is not None:
-            r.comparator = self._serializer.serialize(comparator)
+            req.comparator = self._serializer.serialize(comparator)
 
-        return r
+        return req
 
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.QueryKeys)
     def keys_request(self, filter: Optional[Filter] = None) -> KeySetRequest:
-        r: KeySetRequest = KeySetRequest(
-            scope=self._scope,
-            cache=self._cache_name,
-            format=self._serializer.format,
-        )
+        req = cache_service_messages_v1_pb2.QueryRequest()
 
         if filter is not None:
-            r.filter = self._serializer.serialize(filter)
+            req.filter = self._serializer.serialize(filter)
 
-        return r
+        return req
 
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.QueryEntries)
     def entries_request(
         self, filter: Optional[Filter] = None, comparator: Optional[Comparator] = None
     ) -> EntrySetRequest:
         if filter is None and comparator is not None:
             raise ValueError("Filter cannot be None")
 
-        r: EntrySetRequest = EntrySetRequest(
-            scope=self._scope,
-            cache=self._cache_name,
-            format=self._serializer.format,
-        )
+        req = cache_service_messages_v1_pb2.QueryRequest()
 
         if filter is not None:
-            r.filter = self._serializer.serialize(filter)
+            req.filter = self._serializer.serialize(filter)
 
         if comparator is not None:
-            r.comparator = self._serializer.serialize(comparator)
+            req.comparator = self._serializer.serialize(comparator)
 
-        return r
+        return req
 
     def page_request(self, cookie: bytes) -> PageRequest:
         """
@@ -581,6 +438,7 @@
         self.__next_request_id += 1
         return prefix + self.__uidPrefix + str(self.__next_request_id)
 
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.Index)
     def add_index_request(
             self, extractor: ValueExtractor[T, E], ordered: bool = False,
             comparator: Optional[Comparator] = None
@@ -593,36 +451,16 @@
         if comparator is not None:
             cache_request.comparator = self._serializer.serialize(comparator)
 
-        any_cache_request = Any()
-        any_cache_request.Pack(cache_request)
-
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Index,
-            cacheId=self.cache_id,
-            message=any_cache_request,
-        )
+        return cache_request
 
-        return named_cache_request
-
+    @proxy_request(cache_service_messages_v1_pb2.NamedCacheRequestType.Clear)
     def remove_index_request(self, extractor: ValueExtractor[
         T, E]) -> cache_service_messages_v1_pb2.NamedCacheRequest:
-        cache_request = cache_service_messages_v1_pb2.IndexRequest(
+        return cache_service_messages_v1_pb2.IndexRequest(
             add=False,
             extractor=self._serializer.serialize(extractor),
         )
 
-        any_cache_request = Any()
-        any_cache_request.Pack(cache_request)
-
-        named_cache_request = cache_service_messages_v1_pb2.NamedCacheRequest(
-            type=cache_service_messages_v1_pb2.NamedCacheRequestType.Index,
-            cacheId=self.cache_id,
-            message=any_cache_request,
-        )
-
-        return named_cache_request
-
-
 class StreamHandler:
     theStreamHandler = None
 
@@ -763,11 +601,10 @@
         return result
 
     async def write_request(self, proxy_request: proxy_service_messages_v1_pb2.ProxyRequest,
-                            request_id: int,
-                            request: cache_service_messages_v1_pb2.NamedCacheRequest):
+                            request_id: int):
         self._request_id_to_event_map[request_id] = Event()
         self._request_id_to_event_map[request_id].clear()
-        self._request_id_request_map[request_id] = request
+        self._request_id_request_map[request_id] = proxy_request.message
         await self._stream.write(proxy_request)
 
     def handle_zero_id_response(self, response):
Index: src/coherence/processor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Copyright (c) 2022, 2024, Oracle and/or its affiliates.\n# Licensed under the Universal Permissive License v 1.0 as shown at\n# https://oss.oracle.com/licenses/upl.\n\nfrom __future__ import annotations\n\nfrom abc import ABC\nfrom decimal import Decimal\nfrom typing import Any, Generic, List, Optional, TypeVar, Union, cast\n\nfrom typing_extensions import TypeAlias\n\nfrom .extractor import (\n    CompositeUpdater,\n    ExtractorExpression,\n    Extractors,\n    ManipulatorExpression,\n    UniversalExtractor,\n    UniversalUpdater,\n    UpdaterExpression,\n    ValueExtractor,\n    ValueManipulator,\n    ValueUpdater,\n)\nfrom .filter import Filter\nfrom .serialization import mappings, proxy\n\nE = TypeVar(\"E\")\nK = TypeVar(\"K\")\nR = TypeVar(\"R\")\nT = TypeVar(\"T\")\nV = TypeVar(\"V\")\n\nNumeric: TypeAlias = Union[int, float, Decimal]\n\n\nclass EntryProcessor(ABC, Generic[R]):\n    \"\"\"\n    An invocable agent that operates against the entries within a NamedMap\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Constructs a new `EntryProcessor`\n        \"\"\"\n        super().__init__()\n\n    def and_then(self, processor: EntryProcessor[Any]) -> EntryProcessor[Any]:\n        \"\"\"\n        Returns a :class:`coherence.processor.CompositeProcessor` comprised of this and the provided processor.\n\n        :param processor: the next processor\n        :return: a :class:`coherence.processor.CompositeProcessor` comprised of this and the provided processor\n        \"\"\"\n        return CompositeProcessor(self, processor)\n\n    def when(self, filter: Filter) -> EntryProcessor[R]:\n        \"\"\"\n        Returns a :class:`coherence.processor.ConditionalProcessor` comprised of this processor and the provided filter.\n\n        The specified entry processor gets invoked if and only if the filter\n        applied to the entry evaluates to `true`; otherwise the\n        result of the invocation will return `None`.\n\n        :param filter: the filter :return: Returns a :class:`coherence.processor.ConditionalProcessor` comprised of\n         this processor and the provided filter.\n        \"\"\"\n        return ConditionalProcessor(filter, self)\n\n\n@proxy(\"processor.ExtractorProcessor\")\nclass ExtractorProcessor(EntryProcessor[R]):\n    \"\"\"\n    `ExtractorProcessor` is an :class:`coherence.processor.EntryProcessor` implementation that extracts a value from\n    an object cached within a NamedMap.\n\n    :Example:\n        A common usage pattern is:\n\n        >>> cache.invoke(aPerson,ExtractorProcessor(\"age\"))\n\n    For clustered caches using the ExtractorProcessor could significantly reduce the amount of network traffic.\n    \"\"\"\n\n    def __init__(self, value_extractor: ExtractorExpression[V, R]):\n        \"\"\"\n        Construct an `ExtractorProcessor` using the given extractor or method name.\n\n        :param value_extractor: the :class:`coherence.extractor.ValueExtractor` or string expression\n                                to use by this filter or the name of the method to\n                                invoke via java reflection\n        \"\"\"\n        super().__init__()\n        if value_extractor is None:\n            self.extractor: ValueExtractor[V, R] = Extractors.identity()\n        else:\n            if isinstance(value_extractor, ValueExtractor):\n                self.extractor = value_extractor\n            elif isinstance(value_extractor, str):\n                self.extractor = Extractors.extract(value_extractor)\n            else:\n                raise ValueError(\"value_extractor cannot be any other type\")\n\n\n@proxy(\"processor.CompositeProcessor\")\nclass CompositeProcessor(EntryProcessor[R]):\n    \"\"\"\n    CompositeProcessor represents a collection of entry processors that are invoked sequentially against the same\n    MapEntry.\n    \"\"\"\n\n    def __init__(self, *processors: Any):\n        \"\"\"\n        Construct a `CompositeProcessor` for the specified array of individual entry processors.\n\n        The result of the `CompositeProcessor` execution is an array of results returned by the individual\n        EntryProcessor invocations.\n\n        :param processors: the entry processor array\n        \"\"\"\n        super().__init__()\n        self.processors: list[EntryProcessor[Any]] = list()\n        for p in processors:\n            self.processors.append(p)\n\n    def and_then(self, processor: EntryProcessor[Any]) -> CompositeProcessor[R]:\n        self.processors.append(processor)\n        return self\n\n\n@proxy(\"processor.ConditionalProcessor\")\nclass ConditionalProcessor(EntryProcessor[V]):\n    \"\"\"\n    ConditionalProcessor represents a processor that is invoked conditionally based on the result of an entry\n    evaluation.  A `ConditionalProcessor` is returned from the `when()` function, which takes a filter as its argument.\n    \"\"\"\n\n    def __init__(self, filter: Filter, processor: EntryProcessor[V]):\n        \"\"\"\n        Construct a ConditionalProcessor for the specified filter and the processor.\n\n        The specified entry processor gets invoked if and only if the filter applied to the cache entry evaluates to\n        `true`; otherwise the result of the invocation will return `null`.\n\n        :param filter: the filter\n        :param processor: the entry processor\n        \"\"\"\n        super().__init__()\n        self.filter = filter\n        self.processor = processor\n\n\n@proxy(\"util.NullEntryProcessor\")\nclass NullProcessor(EntryProcessor[bool]):\n    \"\"\"\n    Put entry processor.\n\n    An implementation of an EntryProcessor that does nothing and returns `true` as a result of execution.\n    \"\"\"\n\n    __instance: Any = None\n\n    def __init__(self) -> None:\n        \"\"\"\n        Construct a Null EntryProcessor.\n        \"\"\"\n        super().__init__()\n\n\nclass PropertyProcessor(EntryProcessor[R]):\n    \"\"\"\n    `PropertyProcessor` is a base class for EntryProcessor implementations that depend on a ValueManipulator.\n    \"\"\"\n\n    def __init__(self, manipulator: ManipulatorExpression[T, E], use_is: bool = False):\n        \"\"\"\n        Construct a PropertyProcessor for the specified property name.\n\n        This constructor assumes that the corresponding property getter will have a name of (\"get\" + sName) and the\n        corresponding property setter's name will be (\"set\" + sName).\n\n        :param manipulator: the manipulator or property name\n        :param use_is: prefix with `is`\n        \"\"\"\n        super().__init__()\n        if type(manipulator) is str:\n            self.manipulator: ManipulatorExpression[T, E] = PropertyManipulator(manipulator, use_is)\n        else:\n            self.manipulator = manipulator\n\n\n@proxy(\"processor.PropertyManipulator\")\nclass PropertyManipulator(ValueManipulator[V, R]):\n    \"\"\"\n    `PropertyManipulator` is a reflection based ValueManipulator implementation based on the JavaBean property name\n    conventions.\n    \"\"\"\n\n    def __init__(self, property_name: str, use_is: bool = False):\n        \"\"\"\n        Construct a PropertyManipulator for the specified property name.\n\n        This constructor assumes that the corresponding property getter will have a name of either (\"get\" + sName) or\n        (\"is\" + sName) and the corresponding property setter's name will be (\"set\" + sName).\n\n        :param property_name: a property name\n        :param use_is: if true, the getter method will be prefixed with \"is\" rather than \"get\"\n        \"\"\"\n        super().__init__()\n        self.property_name = property_name\n        self.useIsPrefix = use_is\n\n    def get_extractor(self) -> ValueExtractor[V, R]:\n        raise NotImplementedError(\"Method not implemented\")\n\n    def get_updator(self) -> ValueUpdater[V, R]:\n        raise NotImplementedError(\"Method not implemented\")\n\n\n@proxy(\"processor.NumberMultiplier\")\nclass NumberMultiplier(PropertyProcessor[Numeric]):\n    \"\"\"\n    NumberMultiplier entry processor.\n    \"\"\"\n\n    def __init__(\n        self, name_or_manipulator: ManipulatorExpression[T, E], multiplier: Numeric, post_multiplication: bool = False\n    ):\n        \"\"\"\n        Construct an NumberMultiplier processor that will multiply a property value by a specified factor,\n        returning either the old or the new value as specified.\n\n        :param name_or_manipulator: the ValueManipulator or the property name\n        :param multiplier: the Number representing the magnitude and sign of the multiplier\n        :param post_multiplication: pass true to return the value as it was before it was multiplied, or pass false\n         to return the value as it is after it is multiplied\n        \"\"\"\n        if isinstance(name_or_manipulator, str):\n            manipulator: ValueManipulator[Any, Numeric] = self.create_custom_manipulator(name_or_manipulator)\n            super().__init__(manipulator)\n        else:\n            super().__init__(name_or_manipulator)\n        self.multiplier = multiplier\n        self.postMultiplication = post_multiplication\n\n    @staticmethod\n    def create_custom_manipulator(name_or_manipulator: str) -> ValueManipulator[V, Numeric]:\n        cu: CompositeUpdater[V, Numeric] = CompositeUpdater(\n            UniversalExtractor(name_or_manipulator), UniversalUpdater(name_or_manipulator)\n        )\n        return cu\n\n\n@proxy(\"processor.NumberIncrementor\")\nclass NumberIncrementor(PropertyProcessor[Numeric]):\n    \"\"\"\n    The :class:`coherence.processor.NumberIncrementor` :class:`coherence.processor.EntryProcessor` is used to increment\n    a property value of a numeric type.\n    \"\"\"\n\n    def __init__(\n        self, name_or_manipulator: ManipulatorExpression[T, E], increment: Numeric, post_increment: bool = False\n    ):\n        \"\"\"\n        Construct an :class:`coherence.processor.NumberIncrementor` processor that will increment a property\n        value by a specified amount, returning either the old or the new value as specified.\n\n        :param name_or_manipulator: the :class:`coherence.extractor.ValueManipulator` or string expression\n        :param increment: the numeric value representing the magnitude and sign of the increment\n        :param post_increment: pass `True` to return the value as it was before it was incremented, or pass `False`\n         to return the value as it is after it is incremented\n        \"\"\"\n        if isinstance(name_or_manipulator, str):\n            manipulator: ValueManipulator[Any, Numeric] = self.create_custom_manipulator(name_or_manipulator)\n            super().__init__(manipulator)\n        else:\n            super().__init__(name_or_manipulator)\n        self.increment = increment\n        self.postInc = post_increment\n\n    @staticmethod\n    def create_custom_manipulator(name_or_manipulator: str) -> ValueManipulator[Any, Numeric]:\n        cu: CompositeUpdater[Any, Numeric] = CompositeUpdater(\n            UniversalExtractor(name_or_manipulator), UniversalUpdater(name_or_manipulator)\n        )\n        return cu\n\n\n@proxy(\"processor.ConditionalPut\")\n@mappings({\"return_\": \"return\"})\nclass ConditionalPut(EntryProcessor[V]):\n    \"\"\"\n    :class:`coherence.processor.ConditionalPut` is an :class:`coherence.processor.EntryProcessor` that performs\n    an update operation for an entry that satisfies the specified condition.\n\n    While the :class:`coherence.processor.ConditionalPut` processing could be implemented via direct key-based\n    :class:`coherence.client.NamedMap` operations, it is more efficient and enforces concurrency control without\n    explicit locking.\n\n    Obviously, using more specific, fine-tuned filters (rather than ones based on the\n    :class:`coherence.extractor.IdentityExtractor`) may provide additional flexibility and efficiency allowing\n    the put operation to be performed conditionally on values of specific attributes (or even calculations)\n    instead of the entire object.\n    \"\"\"\n\n    def __init__(self, filter: Filter, value: V, return_value: bool = True):\n        \"\"\"\n        Construct a :class:`coherence.processor.ConditionalPut` that updates an entry with a new value if and only\n        if the filter applied to the entry evaluates to `True`.\n\n        :param filter: the :class:`coherence.filter.Filter` to evaluate an entry\n        :param value: a value to update an entry with\n        :param return_value: specifies whether the processor should return the current value in case it has\n         not been updated\n        \"\"\"\n        super().__init__()\n        self.filter = filter\n        self.value = value\n        self.return_ = return_value\n\n\n@proxy(\"processor.ConditionalPutAll\")\nclass ConditionalPutAll(EntryProcessor[V]):\n    \"\"\"\n    `ConditionalPutAll` is an `EntryProcessor` that performs an update operation for multiple entries that satisfy the\n    specified condition.\n\n    This allows for concurrent insertion/update of values within the cache.\n\n    :Example:\n\n        For example a concurrent `replaceAll(map)` could be implemented as:\n\n            >>> filter = PresentFilter.INSTANCE\n            >>> cache.invokeAll(map.keys(), ConditionalPutAll(filter, map))\n\n        or `putAllIfAbsent` could be done by inverting the filter:\n\n            >>> filter = NotFilter(PresentFilter())\n\n\n    Obviously, using more specific, fine-tuned filters may provide additional flexibility and efficiency allowing the\n    multi-put operations to be performed conditionally on values of specific attributes (or even calculations)\n    instead of a simple existence check.\n    \"\"\"\n\n    def __init__(self, filter: Filter, the_map: dict[K, V]):\n        \"\"\"\n        Construct a `ConditionalPutAll` processor that updates an entry with a new value if and only if the filter\n        applied to the entry evaluates to `True`. The new value is extracted from the specified map based on the\n        entry's key.\n\n        :param filter: the filter to evaluate all supplied entries\n        :param the_map: a dict of values to update entries with\n        \"\"\"\n        super().__init__()\n        self.filter = filter\n        self.entries = the_map\n\n\n@proxy(\"processor.ConditionalRemove\")\n@mappings({\"return_value\": \"return\"})\nclass ConditionalRemove(EntryProcessor[V]):\n    \"\"\"\n    `ConditionalRemove` is an `EntryProcessor` that performs n remove operation if the specified condition is satisfied.\n\n    While the `ConditionalRemove` processing could be implemented via direct key-based `NamedMap` operations, it is more\n    efficient and enforces concurrency control without explicit locking.\n    \"\"\"\n\n    def __init__(self, filter: Filter, return_value: bool = False):\n        \"\"\"\n        Construct a :class:`coherence.processor.ConditionalRemove` processor that removes an\n        :class:`coherence.client.NamedMap` entry if and only if the filter applied to the entry evaluates to `True`.\n\n        This processor may optionally return the current value as a result of\n        the invocation if it has not been removed (the :class:`coherence.filter.Filter` evaluated to\n        `False`).\n\n        :param filter: the filter to evaluate an entry\n        :param return_value: specifies whether the processor should return the current value if it has not\n         been removed\n        \"\"\"\n        super().__init__()\n        self.filter = filter\n        self.return_value = return_value\n\n\n@proxy(\"processor.MethodInvocationProcessor\")\nclass MethodInvocationProcessor(EntryProcessor[R]):\n    \"\"\"\n    An :class:`coherence.processor.EntryProcessor` that invokes the specified method on a value of a cache entry\n    and optionally updates the entry with a modified value.\n    \"\"\"\n\n    def __init__(self, method_name: str, mutator: bool, *args: Any):\n        \"\"\"\n        Construct :class:`coherence.processor.MethodInvoctionProcessor` instance.\n\n        :param method_name: the name of the method to invoke\n        :param mutator: the flag specifying whether the method mutates the state of a target object, which implies\n         that the entry value should be updated after method invocation\n        :param args: the method arguments\n        \"\"\"\n        super().__init__()\n        self.methodName = method_name\n        self.mutator = mutator\n        self.args: List[Any] = list(*args)\n\n\n@proxy(\"processor.TouchProcessor\")\nclass TouchProcessor(EntryProcessor[None]):\n    \"\"\"\n    Touches an entry (if present) in order to trigger interceptor re-evaluation and possibly increment expiry time.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Construct a `TouchProcessor`\n        \"\"\"\n        super().__init__()\n\n\n@proxy(\"processor.ScriptProcessor\")\nclass ScriptProcessor(EntryProcessor[Any]):\n    \"\"\"\n    ScriptProcessor wraps a script written in one of the languages supported by Graal VM.\n    \"\"\"\n\n    def __init__(self, name: str, language: str, *args: Any):\n        \"\"\"\n        Create a :class:`coherence.processor.ScriptProcessor` that wraps a script written in the specified language\n        and identified by the specified name. The specified args will be passed during execution of the script.\n\n        :param name: the name of the :class:`coherence.processor.EntryProcessor` that needs to be executed\n        :param language: the language the script is written. Currently, only `js` (for JavaScript) is supported\n        :param args: the arguments to be passed to the :class:`coherence.processor.EntryProcessor`\n        \"\"\"\n        super().__init__()\n        self.name = name\n        self.language = language\n        self.args: List[Any] = list(*args)\n\n\n@proxy(\"processor.PreloadRequest\")\nclass PreloadRequest(EntryProcessor[None]):\n    \"\"\"\n    `PreloadRequest` is a simple :class:`coherence.processor.EntryProcessor` that performs\n    a get call. No results are reported back to the caller.\n\n    The :class:`coherence.processor.PreloadRequest` process provides a means to \"preload\" an entry or a collection\n    of entries into the cache using the cache loader without incurring the cost of sending the value(s) over the\n    network. If the corresponding entry (or entries) already exists in the cache, or if the cache does not have a\n    loader, then invoking this :class:`coherence.processor.EntryProcessor` has no effect.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Construct a PreloadRequest EntryProcessor.\n        \"\"\"\n        super().__init__()\n\n\n@proxy(\"processor.UpdaterProcessor\")\nclass UpdaterProcessor(EntryProcessor[bool]):\n    \"\"\"\n    `UpdaterProcessor` is an :class:`coherence.processor.EntryProcessor` implementations that updates an attribute\n    of an object cached in an InvocableMap.\n\n    While it's possible to update a value via standard Map API, using the updater allows for clustered caches using\n    the UpdaterProcessor allows avoiding explicit concurrency control and could significantly reduce the amount of\n    network traffic.\n    \"\"\"\n\n    def __init__(self, updater_or_property_name: UpdaterExpression[V, bool], value: V):\n        \"\"\"\n        Construct an `UpdaterProcessor` based on the specified ValueUpdater.\n\n        :param updater_or_property_name: a ValueUpdater object or the method name; passing null will simpy replace\n         the entry's value with the specified one instead of updating it\n        :param value: the value to update the target entry with\n        \"\"\"\n        super().__init__()\n        if type(updater_or_property_name) == str:  # noqa: E721\n            self.updater: ValueUpdater[V, bool]\n            if updater_or_property_name.find(\".\") == -1:\n                self.updater = UniversalUpdater(updater_or_property_name)\n            else:\n                self.updater = CompositeUpdater(updater_or_property_name)\n        else:\n            self.updater = cast(ValueUpdater[V, bool], updater_or_property_name)\n        self.value = value\n\n\n@proxy(\"processor.VersionedPut\")\n@mappings({\"return_current\": \"return\"})\nclass VersionedPut(EntryProcessor[V]):\n    \"\"\"\n    `VersionedPut` is an :class:`coherence.processor.EntryProcessor` that assumes that entry values are versioned (see\n    Coherence Versionable interface for details) and performs an update/insert operation if and only if the version\n    of the specified value matches the version of the corresponding value. `VersionedPutAll` will increment the\n    version indicator before each value is updated.\n    \"\"\"\n\n    def __init__(self, value: V, allow_insert: bool = False, return_current: bool = False):\n        \"\"\"\n        Construct a `VersionedPut` that updates an entry with a new value if and only if the version of the new value\n        matches to the version of the current entry's value. This processor optionally returns the current value as a\n        result of the invocation if it has not been updated (the versions did not match).\n\n        :param value: a value to update an entry with\n        :param allow_insert: specifies whether an insert should be allowed (no currently existing value)\n        :param return_current: specifies whether the processor should return the current value in case it has\n         not been updated\n        \"\"\"\n        super().__init__()\n        self.value = value\n        self.allowInsert = allow_insert\n        self.return_current = return_current\n\n\n@proxy(\"processor.VersionedPutAll\")\n@mappings({\"return_current\": \"return\"})\nclass VersionedPutAll(EntryProcessor[V]):\n    \"\"\"\n    `VersionedPutAll` is an :class:`coherence.processor.EntryProcessor` that assumes that entry values are versioned (\n    see Coherence Versionable interface for details) and performs an update/insert operation only for entries whose\n    versions match to versions of the corresponding current values. In case of the match, the `VersionedPutAll` will\n    increment the version indicator before each value is updated.\n    \"\"\"\n\n    def __init__(self, values: dict[K, V], allow_insert: bool = False, return_current: bool = False):\n        \"\"\"\n        Construct a VersionedPutAll processor that updates an entry with a new value if and only if the version of\n        the new value matches to the version of the current entry's value (which must exist). This processor\n        optionally returns a map of entries that have not been updated (the versions did not match).\n\n        :param values: a `dict` of values to update entries with\n        :param allow_insert: specifies whether an insert should be allowed (no currently existing value)\n        :param return_current: specifies whether the processor should return the current value in case it has\n         not been updated\n        \"\"\"\n        super().__init__()\n        self.entries = values\n        self.allowInsert = allow_insert\n        self.return_current = return_current\n\n\nclass Processors:\n    \"\"\"\n    The `Processors` class provides a set of static methods for creating standard Coherence\n    :class:`coherence.processor.EntryProcessor`'s.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Raises `NotImplementedError` if called.\n        \"\"\"\n        raise NotImplementedError()\n\n    @staticmethod\n    def conditional_put(filter: Filter, value: V, return_value: bool = False) -> EntryProcessor[V]:\n        \"\"\"\n        Construct a :class:`coherence.processor.ConditionalPut` that updates an entry with a new value if and only\n        if the :class:`coherence.filter.Filter` applied to the entry evaluates to `True`.\n\n         :param filter: the :class:`coherence.filter.Filter` to evaluate an entry\n         :param value: a value to update an entry with\n         :param return_value: specifies whether the processor should return the current value in case it\n                has not been updated\n         :return: a processor that updates an entry with a new value if and only if the filter applied\n                  to the entry evaluates to `True`.\n        \"\"\"\n        return ConditionalPut(filter, value, return_value)\n\n    @staticmethod\n    def conditional_put_all(filter: Filter, values: dict[K, V]) -> EntryProcessor[V]:\n        \"\"\"\n        Construct a :class:`coherence.processor.ConditionalRemove` processor that updates an entry with a new value if\n        and only if the :class:`coherence.filter.Filter` applied to the entry evaluates to `True`. The new value is\n        extracted from the specified map based on the entry's key.\n\n        :param filter: the :class:`coherence.filter.Filter` to evaluate all supplied entries\n        :param values: a `dict` of values to update entries with\n        :return: a processor that updates one or more entries with the provided values if and only if the\n                 filter applied to the entry evaluates to `True`\n        \"\"\"\n        return ConditionalPutAll(filter, values)\n\n    @staticmethod\n    def conditional_remove(filter: Filter, return_value: bool = False) -> EntryProcessor[V]:\n        \"\"\"\n        Constructs a :class:`coherence.processor.ConditionalRemove` processor that removes an\n        :class:`coherence.client.NamedMap` entry if and only if the :class:`coherence.filter.Filter`\n        applied to the entry evaluates to `True`.\n\n        This processor may optionally return the current value as a result of\n        the invocation if it has not been removed (the :class:`coherence.filter.Filter` evaluated to\n        `False`).\n\n        :param filter: the :class:`coherence.filter.Filter` to evaluate an entry\n        :param return_value: specifies whether the processor should return the current value if it has not\n         been removed\n        \"\"\"\n        return ConditionalRemove(filter, return_value)\n\n    @staticmethod\n    def extract(extractor: Optional[ExtractorExpression[T, E]] = None) -> EntryProcessor[E]:\n        \"\"\"\n        Construct an :class:`coherence.processor.ExtractorProcessor` using the given\n        :class:`coherence.extractor.ValueExtractor` or string expression to extract a value from an object cached\n        within a :class:`coherence.client.NamedMap`.\n\n        For clustered caches using the :class:`coherence.processor.ExtractorProcessor` could significantly reduce\n        the amount of network traffic.\n\n        :param extractor: the :class:`coherence.extractor.ValueExtractor` or string expression to use by this\n                          processor or the name of the method to invoke via java reflection.  If `None`, an\n                          :class:`coherence.extractor.IdentityExtractor` will be used.\n        \"\"\"\n        ext: ExtractorExpression[T, E] = extractor if extractor is not None else Extractors.identity()\n        return ExtractorProcessor(ext)\n\n    @staticmethod\n    def increment(\n        name_or_manipulator: ManipulatorExpression[T, E], increment: Numeric, post_increment: bool = False\n    ) -> EntryProcessor[Numeric]:\n        \"\"\"\n        Construct an :class:`coherence.processor.NumberIncrementor` processor that will increment a property\n        value by a specified amount, returning either the old or the new value as specified.\n\n        :param name_or_manipulator: the :class:`coherence.extractor.ValueManipulator` or string expression\n        :param increment: the numeric value representing the magnitude and sign of the increment\n        :param post_increment: pass `True` to return the value as it was before it was incremented, or pass `False`\n         to return the value as it is after it is incremented\n        :return:\n        \"\"\"\n        return NumberIncrementor(name_or_manipulator, increment, post_increment)\n\n    @staticmethod\n    def invoke_accessor(method_name: str, *args: Any) -> EntryProcessor[R]:\n        \"\"\"\n        Constructs a :class:`coherence.processor.MethodInvocationProcessor` that invokes the specified method on\n         a value of a cache entry.\n\n        :param method_name: the name of the method to invoke\n        :param args: the method arguments\n        :return: a :class:`coherence.processor.MethodInvocationProcessor` that invokes the specified method on\n                 a value of a cache entry and optionally updates the entry with a modified value\n        \"\"\"\n        return MethodInvocationProcessor(method_name, False, args)\n\n    @staticmethod\n    def invoke_mutator(method_name: str, *args: Any) -> EntryProcessor[R]:\n        \"\"\"\n        Constructs a :class:`coherence.processor.MethodInvocationProcessor` that invokes the specified method on\n        a value of a cache entry updating the entry with a modified value.\n\n        :param method_name: the name of the method to invoke\n        :param args: the method arguments\n        :return: a :class:`coherence.processor.MethodInvocationProcessor` that invokes the specified method on\n                 a value of a cache entry and optionally updates the entry with a modified value\n        \"\"\"\n        return MethodInvocationProcessor(method_name, True, args)\n\n    @staticmethod\n    def multiply(\n        name_or_manipulator: ManipulatorExpression[T, E], multiplier: Numeric, post_multiplication: bool = False\n    ) -> EntryProcessor[Numeric]:\n        \"\"\"\n        Construct an NumberMultiplier processor that will multiply a property value by a specified factor,\n        returning either the old or the new value as specified.\n\n        :param name_or_manipulator: the ValueManipulator or the property name\n        :param multiplier: the Number representing the magnitude and sign of the multiplier\n        :param post_multiplication: pass `True` to return the value as it was before it was multiplied, or pass `False`\n         to return the value as it is after it is multiplied\n        \"\"\"\n        return NumberMultiplier(name_or_manipulator, multiplier, post_multiplication)\n\n    @staticmethod\n    def nop() -> EntryProcessor[bool]:\n        \"\"\"\n        Construct an :class:`coherence.processor.EntryProcessor` that does nothing and returns `True`\n        as a result of execution\n        :return: an :class:`coherence.processor.EntryProcessor` that does nothing and returns `True`\n        as a result of execution\n        \"\"\"\n        return NullProcessor()\n\n    @staticmethod\n    def preload() -> EntryProcessor[None]:\n        \"\"\"\n        :class:`coherence.processor.PreloadRequest` is a simple :class:`coherence.processor.EntryProcessor` that\n        performs a get call. No results are reported back to the caller.\n\n        The :class:`coherence.processor.PreloadRequest` process provides a means to \"preload\" an entry or a\n        collection of entries into the cache using the cache loader without incurring the cost of sending the\n        value(s) over the network. If the corresponding entry (or entries) already exists in the cache,\n        or if the cache does not have a loader, then invoking this :class:`coherence.processor.PreloadRequest`\n        has no effect.\n        :return:\n        \"\"\"\n        return PreloadRequest()\n\n    @staticmethod\n    def script(name: str, language: str, *args: Any) -> EntryProcessor[Any]:\n        \"\"\"\n        Create a :class:`coherence.processor.ScriptProcessor` that wraps a script written in the specified language\n        and identified by the specified name. The specified args will be passed during execution of the script.\n\n        :param name: the name of the :class:`coherence.processor.EntryProcessor` that needs to be executed\n        :param language: the language the script is written. Currently, only `js` (for JavaScript) is supported\n        :param args: the arguments to be passed to the :class:`coherence.processor.EntryProcessor`\n        \"\"\"\n        return ScriptProcessor(name, language, args)\n\n    @staticmethod\n    def touch() -> EntryProcessor[None]:\n        \"\"\"\n        Creates an :class:`coherence.processor.EntryProcessor` that touches an entry (if present) in order to\n        trigger interceptor re-evaluation and possibly increment expiry time.\n        :return:\n        \"\"\"\n        return TouchProcessor()\n\n    @staticmethod\n    def update(updater_or_property_name: UpdaterExpression[V, bool], value: V) -> EntryProcessor[bool]:\n        \"\"\"\n        Construct an :class:`coherence.processor.UpdaterProcessor` based on the specified `ValueUpdater`.\n\n        While it's possible to update a value via standard Map API, using the updater allows for clustered caches using\n        the `UpdaterProcessor` allows avoiding explicit concurrency control and could significantly reduce the amount of\n        network traffic.\n\n        :param updater_or_property_name: a ValueUpdater object or the method name; passing null will simpy replace\n         the entry's value with the specified one instead of updating it\n        :param value: the value to update the target entry with\n        \"\"\"\n        return UpdaterProcessor(updater_or_property_name, value)\n\n    @staticmethod\n    def versioned_put(value: V, allow_insert: bool = False, return_current: bool = False) -> EntryProcessor[V]:\n        \"\"\"\n        Construct a :class:`coherence.processor.VersionedPut` that updates an entry with a new value if and only\n        if the version of the new value matches to the version of the current entry's value. This processor optionally\n        returns the current value as a result of the invocation if it has not been updated (the versions did not match).\n\n        :param value: a value to update an entry with\n        :param allow_insert: specifies whether an insert should be allowed (no currently existing value)\n        :param return_current: specifies whether the processor should return the current value in case it has\n         not been updated\n        \"\"\"\n        return VersionedPut(value, allow_insert, return_current)\n\n    @staticmethod\n    def versioned_put_all(\n        values: dict[K, V], allow_insert: bool = False, return_current: bool = False\n    ) -> EntryProcessor[V]:\n        \"\"\"\n        Construct a :class:`coherence.processor.VersionedPut` processor that updates an entry with a new value if\n        and only if the version of the new value matches to the version of the current entry's value\n        (which must exist). This processor optionally returns a map of entries that have not been updated\n        (the versions did not match).\n\n        :param values: a `dict` of values to update entries with\n        :param allow_insert: specifies whether an insert should be allowed (no currently existing value)\n        :param return_current: specifies whether the processor should return the current value in case it has\n         not been updated\n        \"\"\"\n        return VersionedPutAll(values, allow_insert, return_current)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/coherence/processor.py b/src/coherence/processor.py
--- a/src/coherence/processor.py	(revision 7ed18d6c9ea66aec63a655a1b556e3c67cdb16b6)
+++ b/src/coherence/processor.py	(date 1725564602820)
@@ -6,9 +6,7 @@
 
 from abc import ABC
 from decimal import Decimal
-from typing import Any, Generic, List, Optional, TypeVar, Union, cast
-
-from typing_extensions import TypeAlias
+from typing import Any, Generic, List, Optional, TypeVar, Union, cast, TypeAlias
 
 from .extractor import (
     CompositeUpdater,
Index: src/coherence/serialization.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Copyright (c) 2022, 2023, Oracle and/or its affiliates.\n# Licensed under the Universal Permissive License v 1.0 as shown at\n# https://oss.oracle.com/licenses/upl.\n\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom decimal import Decimal\nfrom typing import Any, Callable, Dict, Final, Optional, Type, TypeVar, cast\n\nimport jsonpickle\n\nT = TypeVar(\"T\", covariant=True)\n\n_BIG_DEC_ALIAS: Final[str] = \"math.BigDec\"\n_BIG_INT_ALIAS: Final[str] = \"math.BigInt\"\n\n_META_CLASS: Final[str] = \"@class\"\n_META_VERSION: Final[str] = \"@version\"\n_META_ENUM: Final[str] = \"enum\"\n\n_JSON_KEY = \"key\"\n_JSON_VALUE = \"value\"\n_JSON_ENTRIES = \"entries\"\n\n_JSON_PICKLE_OBJ = \"py/object\"\n\nMAGIC_BYTE: Final[bytes] = b\"\\x15\"\n\n_type_to_alias: Final[Dict[Type[Any], str]] = dict()\n\"\"\"A mapping of proxied Python types to their alias.\"\"\"\n\n_alias_to_type: Final[Dict[str, Type[Any]]] = dict()\n\"\"\"A mapping of aliases to their proxied Python type.\"\"\"\n\n_attribute_mappings: Final[Dict[Type[Any], Dict[str, str]]] = dict()\n\"\"\"A mapping of object attributes that require a different name when serialized/deserialized.\"\"\"\n\n_attribute_mappings_rev: Final[Dict[Type[Any], Dict[str, str]]] = dict()\n\"\"\"The same mapping as _attribute_mappings, but in reverse for deserialization.\"\"\"\n\n\nclass Serializer(ABC):\n    @abstractmethod\n    def serialize(self, obj: object) -> bytes:\n        \"\"\"documentation\"\"\"\n\n    @abstractmethod\n    def deserialize(self, value: bytes) -> T:  # type: ignore\n        \"\"\"documentation\"\"\"\n\n    @property\n    def format(self) -> str:\n        return self._ser_format\n\n    def __init__(self, ser_format: str):\n        self._ser_format = ser_format\n\n    def __str__(self) -> str:\n        return f\"Serializer(format={self.format})\"\n\n\nclass JSONSerializer(Serializer):\n    SER_FORMAT = \"json\"\n\n    def __init__(self) -> None:\n        super().__init__(JSONSerializer.SER_FORMAT)\n        self._pickler = JavaProxyPickler()\n        self._unpickler = JavaProxyUnpickler()\n\n    def serialize(self, obj: object) -> bytes:\n        jsn: str = jsonpickle.encode(obj, context=self._pickler)\n        b: bytes = MAGIC_BYTE + jsn.encode()\n        return b\n\n    def deserialize(self, value: bytes) -> T:  # type: ignore\n        if isinstance(value, bytes):\n            s = value.decode()\n            if value.__len__() == 0:  # empty string\n                return cast(T, None)\n            else:\n                if ord(s[0]) == ord(MAGIC_BYTE):\n                    return jsonpickle.decode(s[1:], context=self._unpickler)\n                else:\n                    raise ValueError(\"Invalid JSON serialization format\")\n        else:\n            return cast(T, value)\n\n\nclass SerializerRegistry:\n    _singleton: SerializerRegistry\n\n    def __init__(self) -> None:\n        self.serializers: dict[str, Serializer] = {JSONSerializer.SER_FORMAT: JSONSerializer()}\n\n    def __new__(cls) -> SerializerRegistry:\n        if not hasattr(cls, \"_singleton\"):\n            cls._singleton = super(SerializerRegistry, cls).__new__(cls)\n        return cls._singleton\n\n    @staticmethod\n    def instance() -> SerializerRegistry:\n        return SerializerRegistry()._singleton\n\n    @staticmethod\n    def serializer(ser_format: str) -> Serializer:\n        s = SerializerRegistry.instance().serializers[ser_format]\n        if s is not None:\n            return s\n        else:\n            raise ValueError(\"No serializer registered for format: \" + ser_format)\n\n\nclass JavaProxyPickler(jsonpickle.Pickler):\n    MAX_NUMERIC: Final[int] = (2**63) - 1\n    MIN_NUMERIC: Final[int] = (-(2**63)) - 1\n\n    def __init__(self) -> None:\n        super().__init__(make_refs=False)\n\n    def _flatten(self, obj: Any) -> dict[str, str] | None | dict[str, Any] | Decimal:\n        if isinstance(obj, int):\n            if obj > self.MAX_NUMERIC or obj < self.MIN_NUMERIC:\n                return {_META_CLASS: _BIG_INT_ALIAS, \"value\": str(obj)}\n\n        if isinstance(obj, set):\n            return super()._flatten(list(obj))\n\n        return super()._flatten(obj)\n\n    def _getstate(self, obj: Any, data: Any) -> Any:\n        state = self._flatten(obj)\n\n        if state is not None:\n            data.update(state)\n\n        return data\n\n    def _flatten_obj(self, obj: Any) -> dict[str, str | dict[str, list[dict[str, Any]]]] | None:\n        result = super()._flatten_obj(obj)\n        object_type = type(obj)\n        alias: Optional[str] = _alias_for(object_type)\n        if alias is not None:\n            marker = result.get(_JSON_PICKLE_OBJ, None)\n            if marker is not None:\n                actual: dict[str, Any] = dict()\n                actual[_META_CLASS] = alias\n                for key, value in result.items():\n                    # ignore jsonpickle specific content as well as protected keys\n                    if key == _JSON_PICKLE_OBJ or str(key).startswith(\"_\"):\n                        continue\n\n                    # store the original key/value pair as logic below may change them\n                    key_ = key\n                    value_ = value\n\n                    # check for any attributes that need a different key name when serialized to JSON\n                    mappings_ = _attribute_mappings.get(object_type, None)\n                    if mappings_ is not None:\n                        mapping = mappings_.get(key, None)\n                        if mapping is not None:\n                            key_ = mapping\n\n                    # if the value being serialized is a dict, serialize it as a list of key/value pairs\n                    if (\n                        isinstance(value_, dict)\n                        and _META_CLASS not in value_\n                        and _META_VERSION not in value_\n                        and _META_ENUM not in value_\n                    ):\n                        entries = list()\n                        for key_inner, value_inner in value.items():\n                            entries.append({_JSON_KEY: key_inner, _JSON_VALUE: value_inner})\n\n                        padding: dict[str, Any] = dict()\n                        padding[\"entries\"] = entries\n                        value_ = padding\n\n                    actual[key_] = value_\n\n                result = actual\n\n        return result\n\n\n@jsonpickle.handlers.register(Decimal)\nclass DecimalHandler(jsonpickle.handlers.BaseHandler):\n    def flatten(self, obj: object, data: dict[str, Any]) -> dict[str, Any]:\n        return {_META_CLASS: _BIG_DEC_ALIAS, _JSON_VALUE: str(obj)}\n\n    def restore(self, obj: dict[str, Any]) -> Decimal:\n        return Decimal(obj[_JSON_VALUE])\n\n\n@jsonpickle.handlers.register(int)\nclass LargeIntHandler(jsonpickle.handlers.BaseHandler):\n    def flatten(self, obj: object, data: dict[str, Any]) -> dict[str, Any]:\n        return {_META_CLASS: _BIG_INT_ALIAS, _JSON_VALUE: str(obj)}\n\n    def restore(self, obj: dict[str, Any]) -> int:\n        return int(obj[_JSON_VALUE])\n\n\nclass JavaProxyUnpickler(jsonpickle.Unpickler):\n    # noinspection PyUnresolvedReferences\n    def _restore(self, obj: Any) -> Any:\n        if isinstance(obj, dict):\n            metadata: str = obj.get(_META_CLASS, None)\n            if metadata is not None:\n                type_: Optional[Type[Any]] = _type_for(metadata)\n                actual: dict[str, Any] = dict()\n                if type_ is None:\n                    if \"map\" in metadata.lower():\n                        for entry in obj[_JSON_ENTRIES]:\n                            actual[entry[_JSON_KEY]] = entry[_JSON_VALUE]\n                    else:\n                        return obj\n                else:\n                    type_name = jsonpickle.util.importable_name(type_)\n                    actual[_JSON_PICKLE_OBJ] = type_name\n                    rev_map = _attribute_mappings_rev.get(type_name, None)\n                    for key, value in obj.items():\n                        if key == _META_CLASS:\n                            continue\n\n                        key_ = rev_map.get(key, None) if rev_map is not None else None\n                        actual[key_ if key_ is not None else key] = value\n\n                return super().restore(actual, reset=False)\n\n        return super()._restore(obj)\n\n\ndef _alias_for(handled_type: Type[Any]) -> Optional[str]:\n    \"\"\"\n    Return the alias, if any, for the specified type.\n\n    :param handled_type: the type to check\n    :return: the alias for the type or `None`\n    \"\"\"\n    return _type_to_alias.get(handled_type, None)\n\n\ndef _type_for(alias: str) -> Optional[Type[Any]]:\n    \"\"\"\n    Return the type, if any, for the specified alias.\n\n    :param alias: the alias to check\n    :return: the type for the alias or `None`\n    \"\"\"\n    return _alias_to_type.get(alias, None)\n\n\ndef _register(handled_type: Type[Any], alias: str) -> None:\n    \"\"\"\n    Registers the specified type and alias.\n\n    :param handled_type: the type\n    :param alias: the alias for the type\n    \"\"\"\n    _type_to_alias[handled_type] = alias\n    _alias_to_type[alias] = handled_type\n\n\ndef _deregister(handled_type: Type[Any], alias: str) -> None:\n    \"\"\"\n    De-registers the specified type and alias.\n\n    :param handled_type: the type\n    :param alias: the alias for the type\n    \"\"\"\n    del _type_to_alias[handled_type]\n    del _alias_to_type[alias]\n\n\ndef mappings(attributes: Dict[str, str]) -> Callable[[Type[Any]], Type[Any]]:\n    # noinspection PyUnresolvedReferences\n    def _do_register(type_: Type[Any]) -> Type[Any]:\n        _attribute_mappings[type_] = attributes\n        _attribute_mappings_rev[jsonpickle.util.importable_name(type_)] = {y: x for x, y in attributes.items()}\n\n        return type_\n\n    return _do_register\n\n\ndef proxy(alias: str) -> Callable[[Type[Any]], Type[Any]]:\n    \"\"\"A class-level decorator to mark a particular type as having a Java equivalent type known\n    to Coherence.  This decorator accepts a string argument denoting the alias the known type.\n    \"\"\"\n\n    def _do_register(type_: Type[Any]) -> Type[Any]:\n        \"\"\"\n        Registers the type and its alias with the JSON serializer.  This information\n        will be used when serializing/deserializing these types.\n\n        :param type_: the Python type that has a corresponding Java type known to Coherence by the provided alias\n        :return: `proxy_cls` as it was passed\n        \"\"\"\n        _register(type_, alias)\n\n        return type_\n\n    return _do_register\n\n\n_register(Decimal, _BIG_DEC_ALIAS)\n_register(int, _BIG_INT_ALIAS)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/coherence/serialization.py b/src/coherence/serialization.py
--- a/src/coherence/serialization.py	(revision 7ed18d6c9ea66aec63a655a1b556e3c67cdb16b6)
+++ b/src/coherence/serialization.py	(date 1725565906321)
@@ -284,12 +284,12 @@
     return _do_register
 
 
-def proxy(alias: str) -> Callable[[Type[Any]], Type[Any]]:
+def proxy(alias: str) -> Callable[[Type[str]], Type[Any]]:
     """A class-level decorator to mark a particular type as having a Java equivalent type known
     to Coherence.  This decorator accepts a string argument denoting the alias the known type.
     """
 
-    def _do_register(type_: Type[Any]) -> Type[Any]:
+    def _do_register(type_: Type[str]) -> Type[Any]:
         """
         Registers the type and its alias with the JSON serializer.  This information
         will be used when serializing/deserializing these types.
Index: src/coherence/client.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Copyright (c) 2022, 2024, Oracle and/or its affiliates.\n# Licensed under the Universal Permissive License v 1.0 as shown at\n# https://oss.oracle.com/licenses/upl.\n\nfrom __future__ import annotations\n\nimport abc\nimport asyncio\nimport logging\nimport os\nimport time\nimport uuid\nfrom asyncio import Condition, Task\nfrom threading import Lock\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Awaitable,\n    Callable,\n    Final,\n    Generic,\n    Literal,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    TypeVar,\n    cast,\n    no_type_check,\n)\n\n# noinspection PyPackageRequirements\nimport grpc\nfrom pymitter import EventEmitter\n\nfrom . import proxy_service_messages_v1_pb2, proxy_service_v1_pb2_grpc, \\\n    common_messages_v1_pb2\nfrom .aggregator import AverageAggregator, EntryAggregator, PriorityAggregator, SumAggregator\nfrom .comparator import Comparator\nfrom .event import MapLifecycleEvent, MapListener, SessionLifecycleEvent\nfrom .extractor import ValueExtractor\nfrom .filter import Filter\nfrom .messages_pb2 import PageRequest  # type: ignore\nfrom .processor import EntryProcessor\nfrom .serialization import Serializer, SerializerRegistry\nfrom .services_pb2_grpc import NamedCacheServiceStub\nfrom .util import RequestFactory\nfrom .util_v1 import RequestFactory_v1, StreamHandler\nfrom google.protobuf.wrappers_pb2 import BytesValue, BoolValue, Int32Value\n\nE = TypeVar(\"E\")\nK = TypeVar(\"K\")\nV = TypeVar(\"V\")\nR = TypeVar(\"R\")\nT = TypeVar(\"T\")\n\nCOH_LOG = logging.getLogger(\"coherence\")\n\n\n@no_type_check\ndef _pre_call_cache(func):\n    def inner(self, *args, **kwargs):\n        if not self.active:\n            raise RuntimeError(\"Cache [] has been \" + \"released\" if self.released else \"destroyed\")\n\n        return func(self, *args, **kwargs)\n\n    async def inner_async(self, *args, **kwargs):\n        if not self.active:\n            raise RuntimeError(\n                \"Cache [{}] has been {}.\".format(self.name, \"released\" if self.released else \"destroyed\")\n            )\n\n        # noinspection PyProtectedMember\n        await self._session._wait_for_ready()\n\n        return await func(self, *args, **kwargs)\n\n    if asyncio.iscoroutinefunction(func):\n        return inner_async\n    return inner\n\n\n@no_type_check\ndef _pre_call_session(func):\n    def inner(self, *args, **kwargs):\n        if self._closed:\n            raise RuntimeError(\"Session has been closed.\")\n\n        return func(self, *args, **kwargs)\n\n    async def inner_async(self, *args, **kwargs):\n        if self._closed:\n            raise RuntimeError(\"Session has been closed.\")\n\n        return await func(self, *args, **kwargs)\n\n    if asyncio.iscoroutinefunction(func):\n        return inner_async\n    return inner\n\n\nclass MapEntry(Generic[K, V]):\n    \"\"\"\n    A map entry (key-value pair).\n    \"\"\"\n\n    def __init__(self, key: K, value: V):\n        self.key = key\n        self.value = value\n\n\nclass NamedMap(abc.ABC, Generic[K, V]):\n    \"\"\"\n    A Map-based data-structure that manages entries across one or more processes. Entries are typically managed in\n    memory, and are often comprised of data that is also stored persistently, on disk.\n\n    :param K:  the type of the map entry keys\n    :param V:  the type of the map entry values\n    \"\"\"\n\n    @property\n    @abc.abstractmethod\n    def name(self) -> str:\n        \"\"\"documentation\"\"\"\n\n    @abc.abstractmethod\n    def on(self, event: MapLifecycleEvent, callback: Callable[[str], None]) -> None:\n        \"\"\"\n        Add a callback that will be invoked when the specified MapLifecycleEvent is raised.\n        :param event:     the MapLifecycleEvent to listen for\n        :param callback:  the callback that will be invoked when the event occurs\n        \"\"\"\n\n    @property\n    @abc.abstractmethod\n    def destroyed(self) -> bool:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def released(self) -> bool:\n        pass\n\n    @property\n    def active(self) -> bool:\n        return not self.released and not self.destroyed\n\n    @abc.abstractmethod\n    async def add_map_listener(\n        self, listener: MapListener[K, V], listener_for: Optional[K | Filter] = None, lite: bool = False\n    ) -> None:\n        \"\"\"\n        Add a MapListener that will receive events (inserts, updates, deletes) that occur\n        against the map, with the key, old-value and new-value included.\n\n        :param listener:      the MapListener to register\n        :param listener_for:  the optional key that identifies the entry for which to raise events or a Filter\n         that will be passed MapEvent objects to select from; a MapEvent will be delivered to the listener only if the\n         filter evaluates to `True` for that MapEvent. `None` is equivalent to a Filter that always returns `True`\n        :param lite:          optionally pass `True` to indicate that the MapEvent objects do not have to include the\n         old or new values in order to allow optimizations\n        :raises ValueError: if `listener` is `None`\n        \"\"\"\n\n    @abc.abstractmethod\n    async def remove_map_listener(self, listener: MapListener[K, V], listener_for: Optional[K | Filter] = None) -> None:\n        \"\"\"\n        Remove a standard map listener that previously registered to receive events.\n        :param listener:      the MapListener to be removed\n        :param listener_for:  the key or filter, if any, passed to a previous addMapListener invocation\n        :raises ValueError: if `listener` is `None`\n        \"\"\"\n\n    @abc.abstractmethod\n    async def get(self, key: K) -> Optional[V]:\n        \"\"\"\n        Returns the value to which this cache maps the specified key.\n\n        :param key: the key whose associated value is to be returned\n\n        :Example:\n\n         >>> import asyncio\n         >>> from typing import Any, AsyncGenerator, Optional, TypeVar\n         >>> from coherence import NamedCache, Session\n         >>> K = TypeVar(\"K\")\n         >>> V = TypeVar(\"V\")\n         >>> R = TypeVar(\"R\")\n         >>> session: Session = Session(None)\n         >>> cache: NamedCache[Any, Any] = await session.get_cache(\"test\")\n         >>> k: str = \"one\"\n         >>> v: str = \"only-one\"\n         >>> await cache.put(k, v)\n         >>> r = await cache.get(k)\n         >>> print(r)\n         only-one\n\n        \"\"\"\n\n    @abc.abstractmethod\n    async def get_or_default(self, key: K, default_value: Optional[V] = None) -> Optional[V]:\n        \"\"\"\n        Returns the value to which the specified key is mapped, or the specified `defaultValue`\n        if this map contains no mapping for the key.\n\n        :param key: the key whose associated value is to be returned\n        :param default_value: defaultValue if this map contains no mapping for the key.\n        :return: value for the key in the map or the `defaultValue`\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_all(self, keys: set[K]) -> AsyncIterator[MapEntry[K, V]]:\n        \"\"\"\n        Get all the specified keys if they are in the map. For each key that is in the map,\n        that key and its corresponding value will be placed in the map that is returned by\n        this method. The absence of a key in the returned map indicates that it was not in the cache,\n        which may imply (for caches that can load behind the scenes) that the requested data\n        could not be loaded.\n\n        :param keys: an Iterable of keys that may be in this map\n        :return: an AsyncIterator of MapEntry instances for the specified keys passed in `keys`\n        \"\"\"\n\n    @abc.abstractmethod\n    async def put(self, key: K, value: V) -> V:\n        \"\"\"\n        Associates the specified value with the specified key in this map. If the\n        map previously contained a mapping for this key, the old value is replaced.\n\n        :param key: the key with which the specified value is to be associated\n        :param value: the value to be associated with the specified key\n        :return: the previous value associated with the specified key, or `None`\n         if there was no mapping for key. A `None` return can also indicate\n         that the map previously associated `None` with the specified key\n         if the implementation supports `None` values\n        \"\"\"\n\n    @abc.abstractmethod\n    async def put_if_absent(self, key: K, value: V) -> V:\n        \"\"\"\n        If the specified key is not already associated with a value (or is mapped to `None`) associates\n        it with the given value and returns `None`, else returns the current value.\n\n        :param key: the key with which the specified value is to be associated\n        :param value: the value to be associated with the specified key\n        :return: the previous value associated with the specified key, or `None` if there was no mapping for key. A\n         `None` return can also indicate that the map previously associated `None` with the specified key\n         if the implementation supports `None` values\n\n        \"\"\"\n\n    @abc.abstractmethod\n    async def put_all(self, map: dict[K, V], ttl: Optional[int] = 0) -> None:\n        \"\"\"\n        Copies all mappings from the specified map to this map\n\n        :param map: the map to copy from\n        :param ttl: the time to live for the map entries\n        \"\"\"\n\n    @abc.abstractmethod\n    async def clear(self) -> None:\n        \"\"\"\n        Clears all the mappings in the 'NamedMap'.\n\n        \"\"\"\n\n    @abc.abstractmethod\n    async def destroy(self) -> None:\n        \"\"\"\n        Release and destroy this cache.\n\n        Warning: This method is used to completely destroy the specified cache\n        across the cluster. All references in the entire cluster to this cache\n        will be invalidated, the cached data will be cleared, and all resources\n        will be released.\n        \"\"\"\n\n    @abc.abstractmethod\n    def release(self) -> None:\n        \"\"\"\n        Release local resources associated with instance.\n\n        \"\"\"\n\n    @abc.abstractmethod\n    async def truncate(self) -> None:\n        \"\"\"\n        Truncates the cache.  Unlike :func:`coherence.client.NamedMap.clear()`, this function does not generate an\n        event for each removed entry.\n\n        \"\"\"\n\n    @abc.abstractmethod\n    async def remove(self, key: K) -> V:\n        \"\"\"\n        Removes the mapping for a key from this map if it is present.\n\n        :param key: key whose mapping is to be removed from the map\n        :return: the previous value associated with key, or `None` if there was no mapping for key\n        \"\"\"\n\n    @abc.abstractmethod\n    async def remove_mapping(self, key: K, value: V) -> bool:\n        \"\"\"\n        Removes the entry for the specified key only if it is currently mapped to the specified value.\n\n        :param key: key with which the specified value is associated\n        :param value: expected to be associated with the specified key\n        :return: resolving to true if the value was removed\n        \"\"\"\n\n    @abc.abstractmethod\n    async def replace(self, key: K, value: V) -> V:\n        \"\"\"\n        Replaces the entry for the specified key only if currently mapped to the specified value.\n\n        :param key: key whose associated value is to be replaced\n        :param value: value expected to be associated with the specified key\n        :return: resolving to the previous value associated with the specified key, or `None` if there was no mapping\n         for the key. (A `None` return can also indicate that the map previously associated `None` with the key\n         if the implementation supports `None` values.)\n        \"\"\"\n\n    @abc.abstractmethod\n    async def replace_mapping(self, key: K, old_value: V, new_value: V) -> bool:\n        \"\"\"\n        Replaces the entry for the specified key only if currently mapped to the specified value.\n\n        :param key:         key whose associated value is to be removed\n        :param old_value:   value expected to be associated with the specified key\n        :param new_value:   value to be associated with the specified key\n        :return: resolving to `true` if the value was replaced\n        \"\"\"\n\n    @abc.abstractmethod\n    async def contains_key(self, key: K) -> bool:\n        \"\"\"\n        Returns `true` if the specified key is mapped a value within the cache.\n\n        :param key: the key whose presence in this cache is to be tested\n        :return: resolving to `true` if the key is mapped to a value, or `false` if it does not\n        \"\"\"\n\n    @abc.abstractmethod\n    async def contains_value(self, value: V) -> bool:\n        \"\"\"\n        Returns `true` if the specified value is mapped to some key.\n\n        :param value: the value expected to be associated with some key\n        :return: resolving to `true` if a mapping exists, or `false` if it does not\n        \"\"\"\n\n    @abc.abstractmethod\n    async def is_empty(self) -> bool:\n        \"\"\"\n        Returns `true` if this map contains no key-value mappings.\n\n        :return: `true` if this map contains no key-value mappings.\n        \"\"\"\n\n    @abc.abstractmethod\n    async def size(self) -> int:\n        \"\"\"\n        Signifies the number of key-value mappings in this map.\n\n        :return: the number of key-value mappings in this map\n        \"\"\"\n\n    @abc.abstractmethod\n    async def invoke(self, key: K, processor: EntryProcessor[R]) -> R:\n        \"\"\"\n        Invoke the passed EntryProcessor against the Entry specified by the\n        passed key, returning the result of the invocation.\n\n        :param key: the key to process - it is not required to exist within the Map\n        :param processor: the EntryProcessor to use to process the specified key\n        :return: the result of the invocation as returned from the EntryProcessor\n        \"\"\"\n\n    @abc.abstractmethod\n    async def invoke_all(\n        self, processor: EntryProcessor[R], keys: Optional[set[K]] = None, filter: Optional[Filter] = None\n    ) -> AsyncIterator[MapEntry[K, R]]:\n        \"\"\"\n        Invoke the passed EntryProcessor against the set of entries that are selected by the given Filter,\n        returning the result of the invocation for each.\n\n        Unless specified otherwise, implementations will perform this operation in two steps:\n            1. use the filter to retrieve a matching entry set\n            2. apply the agent to every filtered entry.\n\n        This algorithm assumes that the agent's processing does not affect the result of the specified filter\n        evaluation, since the filtering and processing could be performed in parallel on different threads. If this\n        assumption does not hold, the processor logic has to be idempotent, or at least re-evaluate the filter. This\n        could be easily accomplished by wrapping the processor with the ConditionalProcessor.\n\n        :param processor: the EntryProcessor to use to process the specified keys\n        :param keys: the keys to process these keys are not required to exist within the Map\n        :param filter: a Filter that results in the set of keys to be processed\n        :return: an AsyncIterator of MapEntry instances containing the results of invoking the EntryProcessor against\n         each of the specified keys\n        \"\"\"\n\n    @abc.abstractmethod\n    async def aggregate(\n        self, aggregator: EntryAggregator[R], keys: Optional[set[K]] = None, filter: Optional[Filter] = None\n    ) -> R:\n        \"\"\"\n        Perform an aggregating operation against the entries specified by the passed keys.\n\n        :param aggregator: the EntryAggregator that is used to aggregate across the specified entries of this Map\n        :param keys: the Iterable of keys that specify the entries within this Map to aggregate across\n        :param filter: the Filter that is used to select entries within this Map to aggregate across\n        :return: the result of the invocation as returned from the EntryProcessor\n        \"\"\"\n\n    @abc.abstractmethod\n    def values(\n        self, filter: Optional[Filter] = None, comparator: Optional[Comparator] = None, by_page: bool = False\n    ) -> AsyncIterator[V]:\n        \"\"\"\n        Return a Set of the values contained in this map that satisfy the criteria expressed by the filter.\n        If no filter or comparator is specified, it returns a Set view of the values contained in this map.The\n        collection is backed by the map, so changes to the map are reflected in the collection, and vice-versa. If\n        the map is modified while an iteration over the collection is in progress (except through the iterator's own\n        `remove` operation), the results of the iteration are undefined.\n\n        :param filter: the Filter object representing the criteria that the entries of this map should satisfy\n        :param comparator:  the Comparator object which imposes an ordering on entries in the resulting set; or null\n         if the entries' natural ordering should be used\n        :param by_page: returns the keys in pages (transparently to the caller).  This option is only valid\n         if no filter or comparator is provided.\n        :return: an AsyncIterator of MapEntry instances resolving to the values that satisfy the specified criteria\n        \"\"\"\n\n    @abc.abstractmethod\n    def keys(self, filter: Optional[Filter] = None, by_page: bool = False) -> AsyncIterator[K]:\n        \"\"\"\n        Return a set view of the keys contained in this map for entries that satisfy the criteria expressed by the\n        filter.\n\n        :param filter: the Filter object representing the criteria that the entries of this map should satisfy\n        :param by_page: returns the keys in pages (transparently to the caller).  This option is only valid\n         if no filter is provided.\n        :return: an AsyncIterator of keys for entries that satisfy the specified criteria\n        \"\"\"\n\n    @abc.abstractmethod\n    def entries(\n        self, filter: Optional[Filter] = None, comparator: Optional[Comparator] = None, by_page: bool = False\n    ) -> AsyncIterator[MapEntry[K, V]]:\n        \"\"\"\n        Return a set view of the entries contained in this map that satisfy the criteria expressed by the filter.\n        Each element in the returned set is a :class:`coherence.client.MapEntry`.\n\n        :param filter: the Filter object representing the criteria that the entries of this map should satisfy\n        :param comparator: the Comparator object which imposes an ordering on entries in the resulting set; or `None`\n         if the entries' values natural ordering should be used\n        :param by_page: returns the keys in pages (transparently to the caller).  This option is only valid\n         if no filter or comparator is provided.\n        :return: an AsyncIterator of MapEntry instances that satisfy the specified criteria\n        \"\"\"\n\n    @abc.abstractmethod\n    def add_index(\n        self, extractor: ValueExtractor[T, E], ordered: bool = False, comparator: Optional[Comparator] = None\n    ) -> None:\n        \"\"\"\n        Add an index to this map.\n\n        :param extractor: The :class:`coherence.extractor.ValueExtractor` object that is used to extract\n                   an indexable Object from a value stored in the\n                   indexed Map. Must not be 'None'.\n        :param ordered: true if the contents of the indexed information\n                   should be ordered false otherwise.\n        :param comparator: The :class:`coherence.comparator.Comparator` object which imposes an ordering\n                   on entries in the indexed map or None if the\n                   entries' values natural ordering should be used.\n        \"\"\"\n\n    @abc.abstractmethod\n    def remove_index(self, extractor: ValueExtractor[T, E]) -> None:\n        \"\"\"\n        Removes an index on this `NamedMap`.\n\n        :param extractor: The :class:`coherence.extractor.ValueExtractor` object that is used to extract\n                  an indexable Object from a value stored in the\n                  indexed Map. Must not be 'None'.\n\n        \"\"\"\n\n\nclass NamedCache(NamedMap[K, V]):\n    \"\"\"\n    A Map-based data-structure that manages entries across one or more processes. Entries are typically managed in\n    memory, and are often comprised of data that is also stored in an external system, for example, a database,\n    or data that has been assembled or calculated at some significant cost.  Such entries are referred to as being\n    `cached`.\n\n    :param K:  the type of the map entry keys\n    :param V:  the type of the map entry values\n    \"\"\"\n\n    @abc.abstractmethod\n    async def put(self, key: K, value: V, ttl: int = 0) -> V:\n        \"\"\"\n        Associates the specified value with the specified key in this map. If the map previously contained a mapping\n        for this key, the old value is replaced.\n\n        :param key: the key with which the specified value is to be associated\n        :param value: the value to be associated with the specified key\n        :param ttl: the expiry time in millis (optional)\n        :return: resolving to the previous value associated with specified key, or `None` if there was no mapping for\n         key. A `None` return can also indicate that the map previously associated `None` with the specified key\n         if the implementation supports `None` values\n\n        \"\"\"\n\n    @abc.abstractmethod\n    async def put_if_absent(self, key: K, value: V, ttl: int = 0) -> V:\n        \"\"\"\n        If the specified key is not already associated with a value (or is mapped to null) associates it with the\n        given value and returns `None`, else returns the current value.\n\n        :param key: the key with which the specified value is to be associated\n        :param value: the value to be associated with the specified key\n        :param ttl: the expiry time in millis (optional)\n        :return: resolving to the previous value associated with specified key, or `None` if there was no mapping for\n         key. A `None` return can also indicate that the map previously associated `None` with the specified key\n         if the implementation supports `None` values\n\n        \"\"\"\n\n\nclass NamedCacheClient(NamedCache[K, V]):\n    def __init__(self, cache_name: str, session: Session, serializer: Serializer):\n        self._cache_name: str = cache_name\n        self._serializer: Serializer = serializer\n        self._client_stub: NamedCacheServiceStub = NamedCacheServiceStub(session.channel)\n        self._request_factory: RequestFactory = RequestFactory(cache_name, session.scope, serializer)\n        self._emitter: EventEmitter = EventEmitter()\n        self._internal_emitter: EventEmitter = EventEmitter()\n        self._destroyed: bool = False\n        self._released: bool = False\n        self._session: Session = session\n        from .event import _MapEventsManager\n\n        self._setup_event_handlers()\n\n        self._events_manager: _MapEventsManager[K, V] = _MapEventsManager(\n            self, session, self._client_stub, serializer, self._internal_emitter\n        )\n\n    @property\n    def name(self) -> str:\n        return self._cache_name\n\n    @property\n    def destroyed(self) -> bool:\n        return self._destroyed\n\n    @property\n    def released(self) -> bool:\n        return self._released\n\n    @_pre_call_cache\n    def on(self, event: MapLifecycleEvent, callback: Callable[[str], None]) -> None:\n        self._emitter.on(str(event.value), callback)\n\n    @_pre_call_cache\n    async def get(self, key: K) -> Optional[V]:\n        g = self._request_factory.get_request(key)\n        v = await self._client_stub.get(g)\n        if v.present:\n            return self._request_factory.get_serializer().deserialize(v.value)\n        else:\n            return None\n\n    @_pre_call_cache\n    async def get_or_default(self, key: K, default_value: Optional[V] = None) -> Optional[V]:\n        v: Optional[V] = await self.get(key)\n        if v is not None:\n            return v\n        else:\n            return default_value\n\n    @_pre_call_cache\n    async def get_all(self, keys: set[K]) -> AsyncIterator[MapEntry[K, V]]:\n        r = self._request_factory.get_all_request(keys)\n        stream = self._client_stub.getAll(r)\n\n        return _Stream(self._request_factory.get_serializer(), stream, _entry_producer)\n\n    @_pre_call_cache\n    async def put(self, key: K, value: V, ttl: int = 0) -> V:\n        p = self._request_factory.put_request(key, value, ttl)\n        v = await self._client_stub.put(p)\n        return self._request_factory.get_serializer().deserialize(v.value)\n\n    @_pre_call_cache\n    async def put_if_absent(self, key: K, value: V, ttl: int = 0) -> V:\n        p = self._request_factory.put_if_absent_request(key, value, ttl)\n        v = await self._client_stub.putIfAbsent(p)\n        return self._request_factory.get_serializer().deserialize(v.value)\n\n    @_pre_call_cache\n    async def put_all(self, map: dict[K, V]) -> None:\n        p = self._request_factory.put_all_request(map)\n        await self._client_stub.putAll(p)\n\n    @_pre_call_cache\n    async def clear(self) -> None:\n        r = self._request_factory.clear_request()\n        await self._client_stub.clear(r)\n\n    async def destroy(self) -> None:\n        self.release()\n        self._internal_emitter.once(MapLifecycleEvent.DESTROYED.value)\n        self._internal_emitter.emit(MapLifecycleEvent.DESTROYED.value, self.name)\n        r = self._request_factory.destroy_request()\n        await self._client_stub.destroy(r)\n\n    @_pre_call_cache\n    def release(self) -> None:\n        self._internal_emitter.once(MapLifecycleEvent.RELEASED.value)\n        self._internal_emitter.emit(MapLifecycleEvent.RELEASED.value, self.name)\n\n    @_pre_call_cache\n    async def truncate(self) -> None:\n        self._internal_emitter.once(MapLifecycleEvent.TRUNCATED.value)\n        r = self._request_factory.truncate_request()\n        await self._client_stub.truncate(r)\n\n    @_pre_call_cache\n    async def remove(self, key: K) -> V:\n        r = self._request_factory.remove_request(key)\n        v = await self._client_stub.remove(r)\n        return self._request_factory.get_serializer().deserialize(v.value)\n\n    @_pre_call_cache\n    async def remove_mapping(self, key: K, value: V) -> bool:\n        r = self._request_factory.remove_mapping_request(key, value)\n        v = await self._client_stub.removeMapping(r)\n        return self._request_factory.get_serializer().deserialize(v.value)\n\n    @_pre_call_cache\n    async def replace(self, key: K, value: V) -> V:\n        r = self._request_factory.replace_request(key, value)\n        v = await self._client_stub.replace(r)\n        return self._request_factory.get_serializer().deserialize(v.value)\n\n    @_pre_call_cache\n    async def replace_mapping(self, key: K, old_value: V, new_value: V) -> bool:\n        r = self._request_factory.replace_mapping_request(key, old_value, new_value)\n        v = await self._client_stub.replaceMapping(r)\n        return self._request_factory.get_serializer().deserialize(v.value)\n\n    @_pre_call_cache\n    async def contains_key(self, key: K) -> bool:\n        r = self._request_factory.contains_key_request(key)\n        v = await self._client_stub.containsKey(r)\n        return self._request_factory.get_serializer().deserialize(v.value)\n\n    @_pre_call_cache\n    async def contains_value(self, value: V) -> bool:\n        r = self._request_factory.contains_value_request(value)\n        v = await self._client_stub.containsValue(r)\n        return self._request_factory.get_serializer().deserialize(v.value)\n\n    @_pre_call_cache\n    async def is_empty(self) -> bool:\n        r = self._request_factory.is_empty_request()\n        v = await self._client_stub.isEmpty(r)\n        return self._request_factory.get_serializer().deserialize(v.value)\n\n    @_pre_call_cache\n    async def size(self) -> int:\n        r = self._request_factory.size_request()\n        v = await self._client_stub.size(r)\n        return self._request_factory.get_serializer().deserialize(v.value)\n\n    @_pre_call_cache\n    async def invoke(self, key: K, processor: EntryProcessor[R]) -> R:\n        r = self._request_factory.invoke_request(key, processor)\n        v = await self._client_stub.invoke(r)\n        return self._request_factory.get_serializer().deserialize(v.value)\n\n    @_pre_call_cache\n    async def invoke_all(\n        self, processor: EntryProcessor[R], keys: Optional[set[K]] = None, filter: Optional[Filter] = None\n    ) -> AsyncIterator[MapEntry[K, R]]:\n        r = self._request_factory.invoke_all_request(processor, keys, filter)\n        stream = self._client_stub.invokeAll(r)\n\n        return _Stream(self._request_factory.get_serializer(), stream, _entry_producer)\n\n    @_pre_call_cache\n    async def aggregate(\n        self, aggregator: EntryAggregator[R], keys: Optional[set[K]] = None, filter: Optional[Filter] = None\n    ) -> R:\n        r = self._request_factory.aggregate_request(aggregator, keys, filter)\n        results = await self._client_stub.aggregate(r)\n        value: Any = self._request_factory.get_serializer().deserialize(results.value)\n        # for compatibility with 22.06\n        if isinstance(aggregator, SumAggregator) and isinstance(value, str):\n            return cast(R, float(value))\n        elif isinstance(aggregator, AverageAggregator) and isinstance(value, str):\n            return cast(R, float(value))\n        elif isinstance(aggregator, PriorityAggregator):\n            pri_agg: PriorityAggregator[R] = aggregator\n            if (\n                isinstance(pri_agg.aggregator, AverageAggregator) or isinstance(pri_agg.aggregator, SumAggregator)\n            ) and isinstance(value, str):\n                return cast(R, float(value))\n        # end compatibility with 22.06\n\n        return cast(R, value)\n\n    @_pre_call_cache\n    def values(\n        self, filter: Optional[Filter] = None, comparator: Optional[Comparator] = None, by_page: bool = False\n    ) -> AsyncIterator[V]:\n        if by_page and comparator is None and filter is None:\n            return _PagedStream(self, _scalar_deserializer)\n        else:\n            r = self._request_factory.values_request(filter)\n            stream = self._client_stub.values(r)\n\n            return _Stream(self._request_factory.get_serializer(), stream, _scalar_producer)\n\n    @_pre_call_cache\n    def keys(self, filter: Optional[Filter] = None, by_page: bool = False) -> AsyncIterator[K]:\n        if by_page and filter is None:\n            return _PagedStream(self, _scalar_deserializer, True)\n        else:\n            r = self._request_factory.keys_request(filter)\n            stream = self._client_stub.keySet(r)\n\n            return _Stream(self._request_factory.get_serializer(), stream, _scalar_producer)\n\n    @_pre_call_cache\n    def entries(\n        self, filter: Optional[Filter] = None, comparator: Optional[Comparator] = None, by_page: bool = False\n    ) -> AsyncIterator[MapEntry[K, V]]:\n        if by_page and comparator is None and filter is None:\n            return _PagedStream(self, _entry_deserializer)\n        else:\n            r = self._request_factory.entries_request(filter, comparator)\n            stream = self._client_stub.entrySet(r)\n\n            return _Stream(self._request_factory.get_serializer(), stream, _entry_producer)\n\n    from .event import MapListener\n\n    # noinspection PyProtectedMember\n    @_pre_call_cache\n    async def add_map_listener(\n        self, listener: MapListener[K, V], listener_for: Optional[K | Filter] = None, lite: bool = False\n    ) -> None:\n        if listener is None:\n            raise ValueError(\"A MapListener must be specified\")\n\n        if listener_for is None or isinstance(listener_for, Filter):\n            await self._events_manager._register_filter_listener(listener, listener_for, lite)\n        else:\n            await self._events_manager._register_key_listener(listener, listener_for, lite)\n\n    # noinspection PyProtectedMember\n    @_pre_call_cache\n    async def remove_map_listener(self, listener: MapListener[K, V], listener_for: Optional[K | Filter] = None) -> None:\n        if listener is None:\n            raise ValueError(\"A MapListener must be specified\")\n\n        if listener_for is None or isinstance(listener_for, Filter):\n            await self._events_manager._remove_filter_listener(listener, listener_for)\n        else:\n            await self._events_manager._remove_key_listener(listener, listener_for)\n\n    @_pre_call_cache\n    async def add_index(\n        self, extractor: ValueExtractor[T, E], ordered: bool = False, comparator: Optional[Comparator] = None\n    ) -> None:\n        if extractor is None:\n            raise ValueError(\"A ValueExtractor must be specified\")\n        r = self._request_factory.add_index_request(extractor, ordered, comparator)\n        await self._client_stub.addIndex(r)\n\n    @_pre_call_cache\n    async def remove_index(self, extractor: ValueExtractor[T, E]) -> None:\n        if extractor is None:\n            raise ValueError(\"A ValueExtractor must be specified\")\n        r = self._request_factory.remove_index_request(extractor)\n        await self._client_stub.removeIndex(r)\n\n    def _setup_event_handlers(self) -> None:\n        \"\"\"\n        Setup handlers to notify cache-level handlers of events.\n        \"\"\"\n        emitter: EventEmitter = self._emitter\n        internal_emitter: EventEmitter = self._internal_emitter\n        this: NamedCacheClient[K, V] = self\n        cache_name = self._cache_name\n\n        # noinspection PyProtectedMember\n        def on_destroyed(name: str) -> None:\n            if name == cache_name and not this.destroyed:\n                this._events_manager._close()\n                this._destroyed = True\n                emitter.emit(MapLifecycleEvent.DESTROYED.value, name)\n\n        # noinspection PyProtectedMember\n        def on_released(name: str) -> None:\n            if name == cache_name and not this.released:\n                this._events_manager._close()\n                this._released = True\n                emitter.emit(MapLifecycleEvent.RELEASED.value, name)\n\n        def on_truncated(name: str) -> None:\n            if name == cache_name:\n                emitter.emit(MapLifecycleEvent.TRUNCATED.value, name)\n\n        internal_emitter.on(MapLifecycleEvent.DESTROYED.value, on_destroyed)\n        internal_emitter.on(MapLifecycleEvent.RELEASED.value, on_released)\n        internal_emitter.on(MapLifecycleEvent.TRUNCATED.value, on_truncated)\n\n    def __str__(self) -> str:\n        return (\n            f\"NamedCache(name={self.name}, session={self._session.session_id}, serializer={self._serializer},\"\n            f\" released={self.released}, destroyed={self.destroyed})\"\n        )\n\n\nclass NamedCacheClient_v1(NamedCache[K, V]):\n\n    def __init__(self, cache_name: str, session: Session, serializer: Serializer):\n        self._cache_name: str = cache_name\n        self._cache_id: int = 0\n        self._serializer: Serializer = serializer\n        self._client_stub: proxy_service_v1_pb2_grpc.ProxyServiceStub = session._v1_init_response_details.get(\"stub\")\n        self._client_stream: grpc.aio._call.StreamStreamCall = session._v1_init_response_details.get(\"stream\")\n        self._request_factory: RequestFactory_v1 = RequestFactory_v1(cache_name, self._cache_id, session, serializer)\n        # self._emitter: EventEmitter = EventEmitter()\n        # self._internal_emitter: EventEmitter = EventEmitter()\n        self._destroyed: bool = False\n        self._released: bool = False\n        self._session: Session = session\n        self._stream_handler: StreamHandler = StreamHandler.getStreamHandler(self._session, self._client_stream)\n        # asyncio.create_task(self._stream_handler.handle_response())\n        # from .event import _MapEventsManager\n\n        # self._setup_event_handlers()\n\n        # self._events_manager: _MapEventsManager[K, V] = _MapEventsManager(\n        #     self, session, self._client_stub, serializer, self._internal_emitter\n        # )\n\n    @property\n    def cache_id(self):\n        return self._cache_id\n\n    @cache_id.setter\n    def cache_id(self, cache_id):\n        self._cache_id = cache_id\n\n    async def ensure_cache(self):\n        named_cache_request = self._request_factory.ensure_request(self._cache_name)\n        proxy_request = self._request_factory.create_proxy_request(named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id, named_cache_request)\n        response = await asyncio.wait_for(self._stream_handler.get_response(request_id), 1.0)\n        self.cache_id = response.cacheId\n        self._session.update_cache_id_map(self.cache_id, self)\n        self._request_factory.cache_id = response.cacheId\n\n    async def get(self, key: K) -> Optional[V]:\n        named_cache_request = self._request_factory.get_request(key)\n        proxy_request = self._request_factory.create_proxy_request(\n            named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        response = await asyncio.wait_for(\n            self._stream_handler.get_response(request_id), 10.0)\n        if response.HasField(\"message\"):\n            optional_value = common_messages_v1_pb2.OptionalValue()\n            response.message.Unpack(optional_value)\n            if optional_value.present:\n                return self._serializer.deserialize(optional_value.value)\n            else:\n                return None\n        else:\n            return None\n\n    async def put(self, key: K, value: V, ttl: int = 0) -> V:\n        named_cache_request = self._request_factory.put_request(key, value, ttl)\n        proxy_request = self._request_factory.create_proxy_request(\n            named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        response = await asyncio.wait_for(\n            self._stream_handler.get_response(request_id), 10.0)\n        if response is None:\n            return None\n        else:\n            if response.HasField(\"message\"):\n                value = BytesValue()\n                response.message.Unpack(value)\n                result = self._serializer.deserialize(value.value)\n                return result\n            else:\n                return None\n\n    async def put_if_absent(self, key: K, value: V, ttl: int = 0) -> V:\n            named_cache_request = self._request_factory.put_if_absent_request(key, value, ttl)\n            proxy_request = self._request_factory.create_proxy_request(\n                named_cache_request)\n            request_id = proxy_request.id\n            await self._stream_handler.write_request(proxy_request, request_id,\n                                                     named_cache_request)\n            response = await asyncio.wait_for(\n                self._stream_handler.get_response(request_id), 10.0)\n            if response is None:\n                return None\n            else:\n                if response.HasField(\"message\"):\n                    value = BytesValue()\n                    response.message.Unpack(value)\n                    result = self._serializer.deserialize(value.value)\n                    return result\n                else:\n                    return None\n\n    @property\n    def name(self) -> str:\n        return self._cache_name\n\n    def on(self, event: MapLifecycleEvent,\n           callback: Callable[[str], None]) -> None:\n        pass\n\n    @property\n    def destroyed(self) -> bool:\n        pass\n\n    @property\n    def released(self) -> bool:\n        pass\n\n    async def add_map_listener(self, listener: MapListener[K, V],\n                               listener_for: Optional[K | Filter] = None,\n                               lite: bool = False) -> None:\n        pass\n\n    async def remove_map_listener(self, listener: MapListener[K, V],\n                                  listener_for: Optional[\n                                      K | Filter] = None) -> None:\n        pass\n\n    async def get_or_default(self, key: K, default_value: Optional[V] = None) -> \\\n    Optional[V]:\n        v: Optional[V] = await self.get(key)\n        if v is not None:\n            return v\n        else:\n            return default_value\n\n    async def get_all(self, keys: set[K]) -> AsyncIterator[MapEntry[K, V]]:\n        named_cache_request = self._request_factory.get_all_request(keys)\n        proxy_request = (self._request_factory.\n                         create_proxy_request(named_cache_request))\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        result_set = await asyncio.wait_for(\n            self._stream_handler.get_response_collection(request_id),\n            1.0)\n        l = list()\n        for entry in result_set:\n            if entry.HasField(\"message\"):\n                binary_key_value = common_messages_v1_pb2.BinaryKeyAndValue()\n                entry.message.Unpack(binary_key_value)\n                m = MapEntry(binary_key_value.key,\n                             binary_key_value.value)\n                l.append(m)\n        return _ListAsyncIterator(self._serializer, l, _entry_producer_from_list)\n\n    async def put_all(self, map: dict[K, V], ttl: Optional[int] = 0) -> None:\n        named_cache_request = self._request_factory.put_all_request(map, ttl)\n        proxy_request = (self._request_factory.\n                         create_proxy_request(named_cache_request))\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        await asyncio.wait_for(\n            self._stream_handler.get_response(request_id),\n            1.0)\n\n    async def clear(self) -> None:\n        named_cache_request = self._request_factory.clear_request()\n        proxy_request = (self._request_factory.\n                         create_proxy_request(named_cache_request))\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        await asyncio.wait_for(self._stream_handler.get_response(request_id),\n                               1.0)\n\n    async def destroy(self) -> None:\n        named_cache_request = self._request_factory.destroy_request()\n        proxy_request = self._request_factory.create_proxy_request(\n            named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        await asyncio.wait_for(self._stream_handler.get_response(request_id),\n                               1.0)\n\n    def release(self) -> None:\n        pass\n\n    async def truncate(self) -> None:\n        named_cache_request = self._request_factory.truncate_request()\n        proxy_request = self._request_factory.create_proxy_request(\n            named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        await asyncio.wait_for(self._stream_handler.get_response(request_id),\n                               1.0)\n\n    async def remove(self, key: K) -> V:\n        named_cache_request = self._request_factory.remove_request(key)\n        proxy_request = self._request_factory.create_proxy_request(\n            named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        response = await asyncio.wait_for(\n            self._stream_handler.get_response(request_id), 10.0)\n        if response.HasField(\"message\"):\n            value = BytesValue()\n            response.message.Unpack(value)\n            result = self._serializer.deserialize(value.value)\n            return result\n        else:\n            return None\n\n    async def remove_mapping(self, key: K, value: V) -> bool:\n        named_cache_request = self._request_factory.remove_mapping_request(key, value)\n        proxy_request = self._request_factory.create_proxy_request(\n            named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        response = await asyncio.wait_for(\n            self._stream_handler.get_response(request_id), 10.0)\n        if response.HasField(\"message\"):\n            value = BoolValue()\n            response.message.Unpack(value)\n            result = self._serializer.deserialize(value.value)\n            return result\n        else:\n            return False\n\n    async def replace(self, key: K, value: V) -> V:\n        named_cache_request = self._request_factory.replace_request(key, value)\n        proxy_request = self._request_factory.create_proxy_request(\n            named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        response = await asyncio.wait_for(\n            self._stream_handler.get_response(request_id), 10.0)\n        if response.HasField(\"message\"):\n            value = BytesValue()\n            response.message.Unpack(value)\n            result = self._serializer.deserialize(value.value)\n            return result\n        else:\n            return None\n\n    async def replace_mapping(self, key: K, old_value: V, new_value: V) -> bool:\n        named_cache_request = self._request_factory.replace_mapping_request(key, old_value, new_value)\n        proxy_request = self._request_factory.create_proxy_request(\n            named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        response = await asyncio.wait_for(\n            self._stream_handler.get_response(request_id), 10.0)\n        if response.HasField(\"message\"):\n            value = BoolValue()\n            response.message.Unpack(value)\n            result = self._serializer.deserialize(value.value)\n            return result\n        else:\n            return False\n\n    async def contains_key(self, key: K) -> bool:\n        named_cache_request = self._request_factory.contains_key_request(key)\n        proxy_request = self._request_factory.create_proxy_request(\n            named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        response = await asyncio.wait_for(\n            self._stream_handler.get_response(request_id), 10.0)\n        if response.HasField(\"message\"):\n            value = BoolValue()\n            response.message.Unpack(value)\n            result = self._serializer.deserialize(value.value)\n            return result\n        else:\n            return False\n\n    async def contains_value(self, value: V) -> bool:\n        named_cache_request = self._request_factory.contains_value_request(value)\n        proxy_request = self._request_factory.create_proxy_request(\n            named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        response = await asyncio.wait_for(\n            self._stream_handler.get_response(request_id), 10.0)\n        if response.HasField(\"message\"):\n            value = BoolValue()\n            response.message.Unpack(value)\n            result = self._serializer.deserialize(value.value)\n            return result\n        else:\n            return False\n\n    async def is_empty(self) -> bool:\n        named_cache_request = self._request_factory.is_empty_request()\n        proxy_request = self._request_factory.create_proxy_request(\n            named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        response = await asyncio.wait_for(\n            self._stream_handler.get_response(request_id), 10.0)\n        if response.HasField(\"message\"):\n            value = BoolValue()\n            response.message.Unpack(value)\n            result = self._serializer.deserialize(value.value)\n            return result\n        else:\n            return False\n\n    async def size(self) -> int:\n        named_cache_request = self._request_factory.size_request()\n        proxy_request = self._request_factory.create_proxy_request(\n            named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        response = await asyncio.wait_for(\n            self._stream_handler.get_response(request_id), 10.0)\n        if response.HasField(\"message\"):\n            value = Int32Value()\n            response.message.Unpack(value)\n            result = self._serializer.deserialize(value.value)\n            return result\n        else:\n            return 0\n\n    async def invoke(self, key: K, processor: EntryProcessor[R]) -> R:\n        named_cache_request = self._request_factory.invoke_request(key, processor)\n        proxy_request = self._request_factory.create_proxy_request(\n            named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        result_set = await asyncio.wait_for(\n            self._stream_handler.get_response_collection(request_id),\n            1.0)\n        if len(result_set) == 0:\n            return None\n        else:\n            entry = result_set[0]\n            if entry.HasField(\"message\"):\n                binary_key_value = common_messages_v1_pb2.BinaryKeyAndValue()\n                entry.message.Unpack(binary_key_value)\n                return self._serializer.deserialize(binary_key_value.value)\n\n    async def invoke_all(self, processor: EntryProcessor[R],\n                   keys: Optional[set[K]] = None,\n                   filter: Optional[Filter] = None) -> AsyncIterator[\n        MapEntry[K, R]]:\n        named_cache_request = self._request_factory.invoke_all_request(processor, keys, filter)\n        proxy_request = self._request_factory.create_proxy_request(\n            named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        result_set = await asyncio.wait_for(\n            self._stream_handler.get_response_collection(request_id),\n            1.0)\n        l = list()\n        for entry in result_set:\n            if entry.HasField(\"message\"):\n                binary_key_value = common_messages_v1_pb2.BinaryKeyAndValue()\n                entry.message.Unpack(binary_key_value)\n                m = MapEntry(binary_key_value.key,\n                             binary_key_value.value)\n                l.append(m)\n        return _ListAsyncIterator(self._serializer, l, _entry_producer_from_list)\n\n\n    async def aggregate(self, aggregator: EntryAggregator[R],\n                        keys: Optional[set[K]] = None,\n                        filter: Optional[Filter] = None) -> R:\n        named_cache_request = self._request_factory.aggregate_request(aggregator, keys, filter)\n        proxy_request = self._request_factory.create_proxy_request(\n            named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        response = await asyncio.wait_for(\n            self._stream_handler.get_response(request_id), 1.0)\n        if response.HasField(\"message\"):\n            value = BytesValue()\n            response.message.Unpack(value)\n            result = self._serializer.deserialize(value.value)\n            return result\n        else:\n            return None\n\n    def values(self, filter: Optional[Filter] = None,\n               comparator: Optional[Comparator] = None,\n               by_page: bool = False) -> AsyncIterator[V]:\n        pass\n\n    def keys(self, filter: Optional[Filter] = None, by_page: bool = False) -> \\\n    AsyncIterator[K]:\n        pass\n\n    def entries(self, filter: Optional[Filter] = None,\n                comparator: Optional[Comparator] = None,\n                by_page: bool = False) -> AsyncIterator[MapEntry[K, V]]:\n        pass\n\n    async def add_index(self, extractor: ValueExtractor[T, E],\n                        ordered: bool = False,\n                        comparator: Optional[Comparator] = None) -> None:\n        if extractor is None:\n            raise ValueError(\"A ValueExtractor must be specified\")\n        named_cache_request = self._request_factory.add_index_request(extractor,\n                                                                      ordered,\n                                                                      comparator)\n        proxy_request = self._request_factory.create_proxy_request(\n            named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        await asyncio.wait_for(self._stream_handler.get_response(request_id),\n                               1.0)\n\n    async def remove_index(self, extractor: ValueExtractor[T, E]) -> None:\n        if extractor is None:\n            raise ValueError(\"A ValueExtractor must be specified\")\n        named_cache_request = self._request_factory.remove_index_request(\n            extractor)\n        proxy_request = self._request_factory.create_proxy_request(\n            named_cache_request)\n        request_id = proxy_request.id\n        await self._stream_handler.write_request(proxy_request, request_id,\n                                                 named_cache_request)\n        await asyncio.wait_for(self._stream_handler.get_response(request_id),\n                               1.0)\n\n\nclass TlsOptions:\n    \"\"\"\n    Options specific to the configuration of TLS.\n    \"\"\"\n\n    ENV_CA_CERT = \"COHERENCE_TLS_CERTS_PATH\"\n    \"\"\"\n    Environment variable to configure the path to CA certificates\n    \"\"\"\n    ENV_CLIENT_CERT = \"COHERENCE_TLS_CLIENT_CERT\"\n    \"\"\"\n    Environment variable to configure the path to client certificates\n    \"\"\"\n    ENV_CLIENT_KEY = \"COHERENCE_TLS_CLIENT_KEY\"\n    \"\"\"\n    Environment variable to configure the path to client key\n    \"\"\"\n\n    def __init__(\n        self,\n        locked: bool = False,\n        enabled: bool = False,\n        ca_cert_path: str | None = None,\n        client_cert_path: str | None = None,\n        client_key_path: str | None = None,\n    ) -> None:\n        \"\"\"\n        Construct a new :func:`coherence.client.TlsOptions`\n\n        :param locked: If `true`, prevents further mutations to the options.\n        :param enabled: Enable/disable TLS support.\n        :param ca_cert_path: the path to the CA certificate. If not specified then its configured using the\n            environment variable COHERENCE_TLS_CERTS_PATH\n        :param client_cert_path: the path to the client certificate. If not specified then its configured using the\n            environment variable COHERENCE_TLS_CLIENT_CERT\n        :param client_key_path: the path to the client certificate key. If not specified then its configured using the\n            environment variable COHERENCE_TLS_CLIENT_KEY\n        \"\"\"\n        self._locked = locked\n        self._enabled = enabled\n\n        self._ca_cert_path = ca_cert_path if ca_cert_path is not None else os.getenv(TlsOptions.ENV_CA_CERT)\n        self._client_cert_path = (\n            client_cert_path if client_cert_path is not None else os.getenv(TlsOptions.ENV_CLIENT_CERT)\n        )\n        self._client_key_path = client_key_path if client_key_path is not None else os.getenv(TlsOptions.ENV_CLIENT_KEY)\n\n    @property\n    def enabled(self) -> bool:\n        \"\"\"\n        Property to set/get the boolean state if TLS is enabled(true) or disabled(false)\n        \"\"\"\n        return self._enabled\n\n    @enabled.setter\n    def enabled(self, enabled: bool) -> None:\n        if self.is_locked():\n            return\n        else:\n            self._enabled = enabled\n\n    @property\n    def ca_cert_path(self) -> Optional[str]:\n        \"\"\"\n        Property to set/get the path to the CA certificate\n        \"\"\"\n        return self._ca_cert_path\n\n    @ca_cert_path.setter\n    def ca_cert_path(self, ca_cert_path: str) -> None:\n        if self.is_locked():\n            return\n        else:\n            self._ca_cert_path = ca_cert_path\n\n    @property\n    def client_cert_path(self) -> Optional[str]:\n        \"\"\"\n        Property to set/get the path to the client certificate.\n        \"\"\"\n        return self._client_cert_path\n\n    @client_cert_path.setter\n    def client_cert_path(self, client_cert_path: str) -> None:\n        if self.is_locked():\n            return\n        else:\n            self._client_cert_path = client_cert_path\n\n    @property\n    def client_key_path(self) -> Optional[str]:\n        \"\"\"\n        Property to set/get the path to the client certificate key.\n        \"\"\"\n        return self._client_key_path\n\n    @client_key_path.setter\n    def client_key_path(self, client_key_path: str) -> None:\n        if self.is_locked():\n            return\n        else:\n            self._client_key_path = client_key_path\n\n    def locked(self) -> None:\n        \"\"\"\n        Once called, no further mutations can be made.\n        \"\"\"\n        self._locked = True\n\n    def is_locked(self) -> bool:\n        return self._locked\n\n    def __str__(self) -> str:\n        return (\n            f\"TlsOptions(enabled={self.enabled}, ca-cert-path={self.ca_cert_path}, \"\n            f\"client-cert-path={self.client_cert_path}, client-key-path={self.client_key_path})\"\n        )\n\n\nclass Options:\n    \"\"\"\n    Supported :func:`coherence.client.Session` options.\n    \"\"\"\n\n    ENV_SERVER_ADDRESS = \"COHERENCE_SERVER_ADDRESS\"\n    \"\"\"\n    Environment variable to specify the Coherence gRPC server address for the client to connect to. The\n    environment variable is used if address is not passed as an argument in the constructor. If the environment\n    variable is not set and address is not passed as an argument then `DEFAULT_ADDRESS` is used\n    \"\"\"\n    ENV_REQUEST_TIMEOUT = \"COHERENCE_CLIENT_REQUEST_TIMEOUT\"\n    \"\"\"\n    Environment variable to specify the request timeout for each remote call. The environment variable is used if\n    request timeout is not passed as an argument in the constructor. If the environment variable is not set and\n    request timeout is not passed as an argument then `DEFAULT_REQUEST_TIMEOUT` of 30 seconds is used\n    \"\"\"\n    ENV_READY_TIMEOUT = \"COHERENCE_READY_TIMEOUT\"\n    \"\"\"\n    Environment variable to specify the maximum amount of time an NamedMap or NamedCache operations may wait for the\n    underlying gRPC channel to be ready.  This is independent of the request timeout which sets a deadline on how\n    long the call may take after being dispatched.\n    \"\"\"\n    ENV_SESSION_DISCONNECT_TIMEOUT = \"COHERENCE_SESSION_DISCONNECT_TIMEOUT\"\n    \"\"\"\n    Environment variable to specify the maximum amount of time, in seconds, a Session may remain in a disconnected\n    state without successfully reconnecting.\n    \"\"\"\n\n    DEFAULT_ADDRESS: Final[str] = \"localhost:1408\"\n    \"\"\"The default target address to connect to Coherence gRPC server.\"\"\"\n    DEFAULT_SCOPE: Final[str] = \"\"\n    \"\"\"The default scope.\"\"\"\n    DEFAULT_REQUEST_TIMEOUT: Final[float] = 30.0\n    \"\"\"The default request timeout.\"\"\"\n    DEFAULT_READY_TIMEOUT: Final[float] = 0\n    \"\"\"\n    The default ready timeout is 0 which disables the feature by default.  Explicitly configure the ready timeout\n    session option or use the environment variable to specify a positive value indicating how many seconds an RPC will\n    wait for the underlying channel to be ready before failing.\n    \"\"\"\n    DEFAULT_SESSION_DISCONNECT_TIMEOUT: Final[float] = 30.0\n    \"\"\"\n    The default maximum time a session may be in a disconnected state without having successfully reconnected.\n    \"\"\"\n    DEFAULT_FORMAT: Final[str] = \"json\"\n    \"\"\"The default serialization format\"\"\"\n\n    def __init__(\n        self,\n        address: str = DEFAULT_ADDRESS,\n        scope: str = DEFAULT_SCOPE,\n        request_timeout_seconds: float = DEFAULT_REQUEST_TIMEOUT,\n        ready_timeout_seconds: float = DEFAULT_READY_TIMEOUT,\n        session_disconnect_seconds: float = DEFAULT_SESSION_DISCONNECT_TIMEOUT,\n        ser_format: str = DEFAULT_FORMAT,\n        channel_options: Optional[Sequence[Tuple[str, Any]]] = None,\n        tls_options: Optional[TlsOptions] = None,\n    ) -> None:\n        \"\"\"\n        Construct a new :func:`coherence.client.Options`\n\n        :param address: Address of the target Coherence cluster.  If not explicitly set, this defaults\n          to :func:`coherence.client.Options.DEFAULT_ADDRESS`. See\n          also :func:`coherence.client.Options.ENV_SERVER_ADDRESS`\n        :param scope: scope name used to link this :func:`coherence.client.Options` to the\n          corresponding `ConfigurableCacheFactory` on the server.\n        :param request_timeout_seconds: Defines the request timeout, in `seconds`, that will be applied to each\n          remote call. If not explicitly set, this defaults to :func:`coherence.client.Options.DEFAULT_REQUEST_TIMEOUT`.\n          See also :func:`coherence.client.Options.ENV_REQUEST_TIMEOUT`\n        :param ready_timeout_seconds: Defines the ready timeout, in `seconds`.  If this is a positive\n          float value, remote calls will not fail immediately if no connection is available.  If this is a value of zero\n          or less, then remote calls will fail-fast.  If not explicitly configured, the default of 0 is assumed.\n\n          See also :class:`coherence.client.Options.ENV_READY_TIMEOUT`\n        :param session_disconnect_seconds: Defines the maximum time, in `seconds`, that a session may remain in\n          a disconnected state without successfully reconnecting.\n        :param ser_format: The serialization format.  Currently, this is always `json`\n        :param channel_options: The `gRPC` `ChannelOptions`. See\n            https://grpc.github.io/grpc/python/glossary.html#term-channel_arguments and\n            https://github.com/grpc/grpc/blob/master/include/grpc/impl/grpc_types.h\n        :param tls_options: Optional TLS configuration.\n        \"\"\"\n        addr = os.getenv(Options.ENV_SERVER_ADDRESS)\n        if addr is not None:\n            self._address = addr\n        else:\n            self._address = address\n\n        self._request_timeout_seconds = Options._get_float_from_env(\n            Options.ENV_REQUEST_TIMEOUT, request_timeout_seconds\n        )\n        self._ready_timeout_seconds = Options._get_float_from_env(Options.ENV_READY_TIMEOUT, ready_timeout_seconds)\n        self._session_disconnect_timeout_seconds = Options._get_float_from_env(\n            Options.ENV_READY_TIMEOUT, session_disconnect_seconds\n        )\n\n        self._scope = scope\n        self._ser_format = ser_format\n\n        if channel_options is not None:\n            self._channel_options = channel_options\n\n        if tls_options is not None:\n            self._tls_options = tls_options\n\n    @property\n    def tls_options(self) -> Optional[TlsOptions]:\n        \"\"\"\n        Returns the TLS-specific configuration options.\n\n        :return: the TLS-specific configuration options.\n        \"\"\"\n        return getattr(self, \"_tls_options\", None)\n\n    @tls_options.setter\n    def tls_options(self, tls_options: TlsOptions) -> None:\n        \"\"\"\n        Sets the TLS-specific configuration options.\n\n        :param tls_options: the TLS-specific configuration options.\n        \"\"\"\n        self._tls_options = tls_options\n\n    @property\n    def address(self) -> str:\n        \"\"\"\n        Return the IPv4 host address and port in the format of ``[host]:[port]``.\n\n        :return: the IPv4 host address and port in the format of ``[host]:[port]``.\n        \"\"\"\n        return self._address\n\n    @property\n    def scope(self) -> str:\n        \"\"\"\n        Return the scope name used to link this `Session` with to the corresponding `ConfigurableCacheFactory` on the\n        server.\n\n        :return: the scope name used to link this `Session` with to the corresponding `ConfigurableCacheFactory` on the\n         server.\n        \"\"\"\n        return self._scope\n\n    @property\n    def format(self) -> str:\n        \"\"\"\n        The serialization format used by this session.  This library currently supports JSON serialization only,\n        thus this always returns 'json'.\n\n        :return: the serialization format used by this session.\n        \"\"\"\n        return self._ser_format\n\n    @property\n    def request_timeout_seconds(self) -> float:\n        \"\"\"\n        Returns the request timeout in `seconds`\n\n        :return: the request timeout in `seconds`\n        \"\"\"\n        return self._request_timeout_seconds\n\n    @property\n    def ready_timeout_seconds(self) -> float:\n        \"\"\"\n        Returns the ready timeout in `seconds`.\n\n        :return: the ready timeout in `seconds`\n        \"\"\"\n        return self._ready_timeout_seconds\n\n    @property\n    def session_disconnect_timeout_seconds(self) -> float:\n        \"\"\"\n        Returns the ready timeout in `seconds`.\n\n        :return: the ready timeout in `seconds`\n        \"\"\"\n        return self._session_disconnect_timeout_seconds\n\n    @property\n    def channel_options(self) -> Optional[Sequence[Tuple[str, Any]]]:\n        \"\"\"\n        Return the `gRPC` `ChannelOptions`.\n\n        :return: the `gRPC` `ChannelOptions`.\n        \"\"\"\n        return getattr(self, \"_channel_options\", None)\n\n    @channel_options.setter\n    def channel_options(self, channel_options: Sequence[Tuple[str, Any]]) -> None:\n        \"\"\"\n        Set the `gRPC` `ChannelOptions`.\n\n        :param channel_options: the `gRPC` `ChannelOptions`.\n        \"\"\"\n        self._channel_options = channel_options\n\n    @staticmethod\n    def _get_float_from_env(variable_name: str, default_value: float) -> float:\n        \"\"\"\n        Return a float value parsed from the provided environment variable name.\n\n        :param variable_name: the environment variable name\n        :param default_value: the value to use if the environment variable is not set\n\n        :return: the float value from the environment or the default if the value can't be parsed\n          or the environment variable is not set\n        \"\"\"\n        timeout = os.getenv(variable_name)\n        if timeout is not None:\n            time_out: float = default_value\n            try:\n                time_out = float(timeout)\n            except ValueError:\n                COH_LOG.warning(\n                    \"The timeout value of [%s] specified by environment variable [%s] cannot be converted to a float\",\n                    timeout,\n                    variable_name,\n                )\n\n            return time_out\n        else:\n            return default_value\n\n    def __str__(self) -> str:\n        return (\n            f\"Options(address={self.address}, scope={self.scope}, format={self.format},\"\n            f\" request-timeout-seconds={self.request_timeout_seconds}, \"\n            f\"ready-timeout-seconds={self.ready_timeout_seconds}, \"\n            f\"session-disconnect-timeout-seconds={self.session_disconnect_timeout_seconds}, \"\n            f\"tls-options={self.tls_options}, \"\n            f\"channel-options={self.channel_options})\"\n        )\n\n\ndef _get_channel_creds(tls_options: TlsOptions) -> grpc.ChannelCredentials:\n    client_cert: bytes | None = None\n    client_key: bytes | None = None\n    ca_cert: bytes | None = None\n\n    if tls_options.client_cert_path is not None:\n        with open(tls_options.client_cert_path, \"rb\") as f:\n            client_cert = f.read()\n    if tls_options.client_key_path is not None:\n        with open(tls_options.client_key_path, \"rb\") as f:\n            client_key = f.read()\n    if tls_options.ca_cert_path is not None:\n        with open(tls_options.ca_cert_path, \"rb\") as f:\n            ca_cert = f.read()\n\n    credentials = grpc.ssl_channel_credentials(ca_cert, client_key, client_cert)\n\n    return credentials\n\n\nclass Session:\n    \"\"\"\n    Session represents a logical connection to an endpoint. It also acts as a factory for creating caches.\n\n    This class emits the following events:\n\n        1. :class:`coherence.event.MapLifecycleEvent.DESTROYED`: when the underlying cache is destroyed\n        2. :class:`coherence.event.MapLifecycleEvent.TRUNCATED`: When the underlying cache is truncated\n        3. :class:`coherence.event.MapLifecycleEvent.RELEASED`: When the underlying cache is released\n        4. :class:`coherence.event.SessionLifecycleEvent.CONNECT`: when the Session detects the underlying `gRPC`\n            channel has connected.\n        5. :class:`coherence.event.SessionLifecycleEvent.DISCONNECT`: when the Session detects the underlying `gRPC`\n            channel has disconnected\n        6. :class:`coherence.event.SessionLifecycleEvent.RECONNECTED`: when the Session detects the underlying `gRPC`\n            channel has re-connected\n        7. :class:`coherence.event.SessionLifecycleEvent.CLOSED`: when the Session has been closed\n\n    \"\"\"\n\n    DEFAULT_FORMAT: Final[str] = \"json\"\n    \"\"\"The default serialization format\"\"\"\n\n    _initialized = False\n\n    def __init__(self, session_options: Optional[Options] = None):\n        \"\"\"\n        Construct a new `Session` based on the provided :func:`coherence.client.Options`\n\n        :param session_options: the provided :func:`coherence.client.Options`\n        \"\"\"\n        self._closed: bool = False\n        self._session_id: str = str(uuid.uuid4())\n        self._ready = False\n        self._ready_condition: Condition = Condition()\n        self._caches: dict[str, NamedCache[Any, Any]] = dict()\n        # to map cacheId to Cache instance. Not used in v0\n        self._id_to_caches: dict[int, NamedCache[Any, Any]] = dict()\n        self._lock: Lock = Lock()\n        if session_options is not None:\n            self._session_options = session_options\n        else:\n            self._session_options = Options()\n\n        self._ready_timeout_seconds: float = self._session_options.ready_timeout_seconds\n        self._ready_enabled: bool = self._ready_timeout_seconds > 0\n\n        interceptors = [\n            _InterceptorUnaryUnary(self),\n            _InterceptorUnaryStream(self),\n            _InterceptorStreamUnary(self),\n        ]\n\n        # only add the StreamStream interceptor if ready support is enabled as\n        # when added in the non-ready case, the call will not fail-fast\n        if self._ready_enabled:\n            interceptors.append(_InterceptorStreamStream(self))\n\n        self._tasks: Set[Task[None]] = set()\n\n        options: Sequence[tuple[str, Any]] = [\n            (\"grpc.min_reconnect_backoff_ms\", 1100),\n            (\"grpc.max_reconnect_backoff_ms\", 3000),\n            (\"grpc.lb_policy_name\", \"round_robin\"),\n        ]\n\n        self._is_server_grpc_v1 = False\n        self._v1_init_response_details = None\n\n        if self._session_options.tls_options is None:\n            self._channel: grpc.aio.Channel = grpc.aio.insecure_channel(\n                self._session_options.address,\n                options=(\n                    options if self._session_options.channel_options is None else self._session_options.channel_options\n                ),\n                interceptors=interceptors,\n            )\n        else:\n            creds: grpc.ChannelCredentials = _get_channel_creds(self._session_options.tls_options)\n            self._channel = grpc.aio.secure_channel(\n                self._session_options.address,\n                creds,\n                options=(\n                    options if self._session_options.channel_options is None else self._session_options.channel_options\n                ),\n                interceptors=interceptors,\n            )\n\n        watch_task: Task[None] = asyncio.create_task(watch_channel_state(self))\n        self._tasks.add(watch_task)\n        self._emitter: EventEmitter = EventEmitter()\n        self._channel.get_state(True)  # trigger connect\n\n    @staticmethod\n    async def create(session_options: Optional[Options] = None) -> Session:\n        session: Session = Session(session_options)\n        await session._set_ready(False)\n        return session\n\n    # noinspection PyTypeHints\n    @_pre_call_session\n    def on(\n        self,\n        event: Literal[MapLifecycleEvent.DESTROYED] | Literal[MapLifecycleEvent.RELEASED] | SessionLifecycleEvent,\n        callback: Callable[[str], None] | Callable[[], None],\n    ) -> None:\n        \"\"\"\n        Register a callback to be invoked when the following events are raised:\n\n        * MapLifecycleEvent.DESTROYED\n        * MapLifecycleEvent.RELEASED\n        * Any SessionLifecycleEvent\n\n        The callbacks defined for MapLifecycleEvent DESTROYED and RELEASED should accept a single string\n        argument representing the cache name that the event was raised for.\n\n        The SessionLifecycleEvent callbacks should not accept call arguments.\n        :param event:     the event to listener for\n        :param callback:  the callback to invoke when the event is raised\n        \"\"\"\n        self._emitter.on(str(event.value), callback)\n\n    @property\n    def channel(self) -> grpc.aio.Channel:\n        \"\"\"\n        Return the underlying `gRPC` Channel used by this session.\n\n        :return: the underlying `gRPC` Channel used by this session.\n        \"\"\"\n        return self._channel\n\n    @property\n    def scope(self) -> str:\n        \"\"\"\n        Return the scope name used to link this `Session` with to the corresponding `ConfigurableCacheFactory` on the\n        server.\n\n        :return: the scope name used to link this `Session` with to the corresponding `ConfigurableCacheFactory` on the\n          server.\n        \"\"\"\n        return self._session_options.scope\n\n    @property\n    def format(self) -> str:\n        \"\"\"\n        Returns the default serialization format used by the `Session`\n\n        :return: the default serialization format used by the `Session`\n        \"\"\"\n        return self._session_options.format\n\n    @property\n    def options(self) -> Options:\n        \"\"\"\n        Return the :func:`coherence.client.Options` (read-only) used to configure this session.\n\n        :return: the :func:`coherence.client.Options` (read-only) used to configure this session.\n        \"\"\"\n        return self._session_options\n\n    @property\n    def closed(self) -> bool:\n        \"\"\"\n        Returns `True` if Session is closed else `False`.\n\n        :return: `True` if Session is closed else `False`\n        \"\"\"\n        return self._closed\n\n    @property\n    def session_id(self) -> str:\n        \"\"\"\n        Returns this Session's ID.\n\n        :return: this Session's ID\n        \"\"\"\n        return self._session_id\n\n    def __str__(self) -> str:\n        return (\n            f\"Session(id={self.session_id}, closed={self.closed}, state={self._channel.get_state(False)},\"\n            f\" caches/maps={len(self._caches)}, options={self.options})\"\n        )\n\n    def update_cache_id_map(self, cache_id: int, cache: NamedCache[K, V]):\n        self._id_to_caches.update({cache_id: cache})\n\n    # noinspection PyProtectedMember\n    @_pre_call_session\n    async def get_cache(self, name: str, ser_format: str = DEFAULT_FORMAT) -> NamedCache[K, V]:\n        \"\"\"\n        Returns a :func:`coherence.client.NamedCache` for the specified cache name.\n\n        :param name: the cache name\n        :param ser_format: the serialization format for keys and values stored within the cache\n\n        :return: Returns a :func:`coherence.client.NamedCache` for the specified cache name.\n        \"\"\"\n        serializer = SerializerRegistry.serializer(ser_format)\n        if not Session._initialized:\n            check_result = await self._check_server_grpc_version_is_v1()\n            Session._initialized = True\n            if check_result is None:    # Server is running grpc v0\n                self._is_server_grpc_v1 = False\n            else:   # Server is running grpc v1\n                self._is_server_grpc_v1 = True\n                self._v1_init_response_details = check_result\n\n        if not self._is_server_grpc_v1:\n            with self._lock:\n                c = self._caches.get(name)\n                if c is None:\n                    c = NamedCacheClient(name, self, serializer)\n                    # initialize the event stream now to ensure lifecycle listeners will work as expected\n                    await c._events_manager._ensure_stream()\n                    self._setup_event_handlers(c)\n                    self._caches.update({name: c})\n                return c\n        else:   # Server is running grpc v1\n            with self._lock:\n                c = self._caches.get(name)\n                if c is None:\n                    c = NamedCacheClient_v1(name, self, serializer)\n                    await c.ensure_cache()\n                    # initialize the event stream now to ensure lifecycle listeners will work as expected\n                    # await c._events_manager._ensure_stream()\n                    # self._setup_event_handlers(c)\n                    self._caches.update({name: c})\n                return c\n\n    # noinspection PyProtectedMember\n    @_pre_call_session\n    async def get_map(self, name: str, ser_format: str = DEFAULT_FORMAT) -> NamedMap[K, V]:\n        \"\"\"\n        Returns a :func:`coherence.client.NameMap` for the specified cache name.\n\n        :param name: the map name\n        :param ser_format: the serialization format for keys and values stored within the cache\n\n        :return: Returns a :func:`coherence.client.NamedMap` for the specified cache name.\n        \"\"\"\n        serializer = SerializerRegistry.serializer(ser_format)\n        if not Session._initialized:\n            check_result = await self._check_server_grpc_version_is_v1()\n            Session._initialized = True\n            if check_result == None:    # Server is running grpc v0\n                self._is_server_grpc_v1 = False\n            else:   # Server is running grpc v1\n                self._is_server_grpc_v1 = True\n                self._v1_init_response_details = check_result\n\n        if not self._is_server_grpc_v1:\n            with self._lock:\n                c = self._caches.get(name)\n                if c is None:\n                    c = NamedCacheClient(name, self, serializer)\n                    # initialize the event stream now to ensure lifecycle listeners will work as expected\n                    await c._events_manager._ensure_stream()\n                    self._setup_event_handlers(c)\n                    self._caches.update({name: c})\n                return c\n        else:   # Server is running grpc v1\n            with self._lock:\n                c = self._caches.get(name)\n            if c is None:\n                c = NamedCacheClient_v1(name, self, serializer)\n                await c.ensure_cache()\n                # initialize the event stream now to ensure lifecycle listeners will work as expected\n                # await c._events_manager._ensure_stream()\n                # self._setup_event_handlers(c)\n                self._caches.update({name: c})\n            return c\n\n    def is_ready(self) -> bool:\n        \"\"\"\n        Returns\n        :return:\n        \"\"\"\n        if self._closed:\n            return False\n\n        return True if not self._ready_enabled else self._ready\n\n    async def _set_ready(self, ready: bool) -> None:\n        self._ready = ready\n        if self._ready:\n            if not self._ready_condition.locked():\n                await self._ready_condition.acquire()\n            self._ready_condition.notify_all()\n            self._ready_condition.release()\n        else:\n            await self._ready_condition.acquire()\n\n    async def _wait_for_ready(self) -> None:\n        if self._ready_enabled and not self.is_ready():\n            timeout: float = self._ready_timeout_seconds\n            COH_LOG.debug(f\"Waiting for session {self.session_id} to become active; timeout=[{timeout} seconds]\")\n            try:\n                await asyncio.wait_for(self._ready_condition.wait(), timeout)\n            except TimeoutError:\n                s = \"Deadline [{0} seconds] exceeded \" \"waiting for session {1} to become active\".format(\n                    str(timeout), self.session_id\n                )\n                raise TimeoutError(s)\n\n    # noinspection PyUnresolvedReferences\n    async def close(self) -> None:\n        \"\"\"\n        Close the `Session`\n        \"\"\"\n        if not self._closed:\n            self._closed = True\n            self._emitter.emit(SessionLifecycleEvent.CLOSED.value)\n            for task in self._tasks:\n                task.cancel()\n            self._tasks.clear()\n\n            caches_copy: dict[str, NamedCache[Any, Any]] = self._caches.copy()\n            for cache in caches_copy.values():\n                cache.release()\n\n            self._caches.clear()\n\n            await self._channel.close()  # TODO: consider grace period?\n            self._channel = None\n\n    def _setup_event_handlers(self, client: NamedCacheClient[K, V]) -> None:\n        this: Session = self\n\n        def on_destroyed(name: str) -> None:\n            if name in this._caches:\n                del this._caches[name]\n            self._emitter.emit(MapLifecycleEvent.DESTROYED.value, name)\n\n        def on_released(name: str) -> None:\n            if name in this._caches:\n                del this._caches[name]\n            self._emitter.emit(MapLifecycleEvent.RELEASED.value, name)\n\n        client.on(MapLifecycleEvent.DESTROYED, on_destroyed)\n        client.on(MapLifecycleEvent.RELEASED, on_released)\n\n    async def _check_server_grpc_version_is_v1(self) -> object:\n        stub = proxy_service_v1_pb2_grpc.ProxyServiceStub(self._channel)\n        stream = stub.subChannel()\n        results = dict()\n        results[\"stub\"] = stub\n        results[\"stream\"] = stream\n\n        try:\n            response = await self._send_init_request(stream)\n            results[\"init_response\"] = response\n            return results\n        except grpc.aio._call.AioRpcError:\n            return None\n\n    async def _send_init_request(self, stream) -> object:\n        # InitRequest\n        init_request = proxy_service_messages_v1_pb2.ProxyRequest(\n            id = 2,\n            init = self._create_init_request(),\n        )\n        await stream.write(init_request)\n        try:\n            response = await stream.read()\n            # COH_LOG.info(response)\n            # COH_LOG.info(\"InitRequest request completed.\")\n            return response\n        except grpc.aio._call.AioRpcError as e:\n            if e.details() == 'Method not found: coherence.proxy.v1.ProxyService/subChannel' :\n                COH_LOG.info(\"Server is not running v1 gRPC version\")\n            raise e\n\n\n    def _create_init_request(self):\n\n        init_request = proxy_service_messages_v1_pb2.InitRequest(\n            scope =  \"\",\n            format = \"json\",\n            protocol = \"CacheService\",\n            protocolVersion = 1,\n            supportedProtocolVersion = 1,\n            heartbeat= 0,\n        )\n\n        return init_request\n\n    @property\n    def request_id_map(self):\n        return self._request_id_map\n\n\n# noinspection PyArgumentList\nclass _BaseInterceptor:\n    \"\"\"Base client interceptor to enable waiting for channel connectivity and\n    set call timeouts.\n    Having this base class and four concrete implementations is due to\n    https://github.com/grpc/grpc/issues/31442\"\"\"\n\n    def __init__(self, session: Session):\n        self._session: Session = session\n\n    @no_type_check  # disabling as typing info in gRPC is in protected packages\n    async def _do_intercept(self, continuation, client_call_details, request):\n        \"\"\"\n        Intercepts a gRPC call setting our specific options for the call.\n        :param continuation:         the gRPC call continuation\n        :param client_call_details:  the call details\n        :param request:              the gRPC request (if any)\n        :return:                     the result of the call\n        \"\"\"\n        new_details = grpc.aio.ClientCallDetails(\n            client_call_details.method,\n            self._session.options.request_timeout_seconds,\n            client_call_details.metadata,\n            client_call_details.credentials,\n            True if self._session._ready_enabled else None,\n        )\n        return await continuation(new_details, request)\n\n\nclass _InterceptorUnaryUnary(_BaseInterceptor, grpc.aio.UnaryUnaryClientInterceptor):\n    \"\"\"Interceptor for Unary/Unary calls.\"\"\"\n\n    @no_type_check  # disabling as typing info in gRPC is in protected packages\n    async def intercept_unary_unary(self, continuation, client_call_details, request):\n        return await self._do_intercept(continuation, client_call_details, request)\n\n\nclass _InterceptorUnaryStream(_BaseInterceptor, grpc.aio.UnaryStreamClientInterceptor):\n    \"\"\"Interceptor for Unary/Stream calls.\"\"\"\n\n    @no_type_check  # disabling as typing info in gRPC is in protected packages\n    async def intercept_unary_stream(self, continuation, client_call_details, request):\n        return await self._do_intercept(continuation, client_call_details, request)\n\n\nclass _InterceptorStreamUnary(_BaseInterceptor, grpc.aio.StreamUnaryClientInterceptor):\n    \"\"\"Interceptor for Stream/Unary calls.\"\"\"\n\n    @no_type_check  # disabling as typing info in gRPC is in protected packages\n    async def intercept_stream_unary(self, continuation, client_call_details, request):\n        return await self._do_intercept(continuation, client_call_details, request)\n\n\nclass _InterceptorStreamStream(_BaseInterceptor, grpc.aio.StreamStreamClientInterceptor):\n    \"\"\"Interceptor for Stream/Stream calls.\"\"\"\n\n    # noinspection PyArgumentList,PyUnresolvedReferences\n    @no_type_check  # disabling as typing info in gRPC is in protected packages\n    async def intercept_stream_stream(self, continuation, client_call_details, request):\n        new_details = grpc.aio.ClientCallDetails(\n            client_call_details.method,\n            client_call_details.timeout,\n            client_call_details.metadata,\n            client_call_details.credentials,\n            True,\n        )\n\n        return await continuation(new_details, request)\n\n\n# noinspection PyProtectedMember\nasync def watch_channel_state(session: Session) -> None:\n    emitter: EventEmitter = session._emitter\n    channel: grpc.aio.Channel = session.channel\n    first_connect: bool = True\n    connected: bool = False\n    last_state: grpc.ChannelConnectivity = grpc.ChannelConnectivity.IDLE\n    disconnect_time: float = 0\n\n    def current_milli_time() -> float:\n        return round(time.time() * 1000)\n\n    try:\n        while True:\n            state: grpc.ChannelConnectivity = channel.get_state(True)\n            if COH_LOG.isEnabledFor(logging.DEBUG):\n                COH_LOG.debug(f\"New Channel State: transitioning from [{last_state}] to [{state}].\")\n            if state == grpc.ChannelConnectivity.SHUTDOWN:\n                COH_LOG.info(f\"Session [{session.session_id}] terminated.\")\n                await session._set_ready(False)\n                await session.close()\n                return\n            elif state == grpc.ChannelConnectivity.READY:\n                if not first_connect and not connected:\n                    connected = True\n                    disconnect_time = 0\n                    COH_LOG.info(f\"Session [{session.session_id} re-connected to [{session.options.address}].\")\n                    await emitter.emit_async(SessionLifecycleEvent.RECONNECTED.value)\n                    await session._set_ready(True)\n                elif first_connect and not connected:\n                    connected = True\n                    COH_LOG.info(f\"Session [{session.session_id}] connected to [{session.options.address}].\")\n\n                    first_connect = False\n                    await emitter.emit_async(SessionLifecycleEvent.CONNECTED.value)\n                    await session._set_ready(True)\n            else:\n                if connected:\n                    connected = False\n                    disconnect_time = -1\n                    COH_LOG.warning(\n                        f\"Session [{session.session_id}] disconnected from [{session.options.address}];\"\n                        f\" will attempt reconnect.\"\n                    )\n\n                    await emitter.emit_async(SessionLifecycleEvent.DISCONNECTED.value)\n                    await session._set_ready(False)\n\n                if disconnect_time != 0:\n                    if disconnect_time == -1:\n                        disconnect_time = current_milli_time()\n                    else:\n                        waited: float = current_milli_time() - disconnect_time\n                        timeout = session.options.session_disconnect_timeout_seconds\n                        if COH_LOG.isEnabledFor(logging.DEBUG):\n                            COH_LOG.debug(\n                                f\"Waited [{waited / 1000} seconds] for session [{session.session_id}] to reconnect.\"\n                                f\" [~{(round(timeout - (waited / 1000)))} seconds] remaining to reconnect.\"\n                            )\n                        if waited >= timeout:\n                            COH_LOG.error(\n                                f\"session [{session.session_id}] unable to reconnect within [{timeout} seconds.\"\n                                f\"  Closing session.\"\n                            )\n                            await session.close()\n                            return\n\n            state = channel.get_state(True)\n            if COH_LOG.isEnabledFor(logging.DEBUG):\n                COH_LOG.debug(f\"Waiting for state change from [{state}]\")\n            await channel.wait_for_state_change(state)\n    except asyncio.CancelledError:\n        return\n\n\nclass _Stream(abc.ABC, AsyncIterator[T]):\n    \"\"\"\n    A simple AsyncIterator that wraps a Callable that produces iteration\n    elements.\n    \"\"\"\n\n    def __init__(\n        self,\n        serializer: Serializer,\n        stream: grpc.Channel.unary_stream,\n        next_producer: Callable[[Serializer, grpc.Channel.unary_stream], Awaitable[T]],\n    ) -> None:\n        super().__init__()\n        # A function that may be called to produce a series of results\n        self._next_producer = next_producer\n\n        # the Serializer that should be used to deserialize results\n        self._serializer = serializer\n\n        # the gRPC stream providing results\n        self._stream = stream\n\n    def __aiter__(self) -> AsyncIterator[T]:\n        return self\n\n    def __anext__(self) -> Awaitable[T]:\n        return self._next_producer(self._serializer, self._stream)\n\n\n# noinspection PyProtectedMember\nclass _PagedStream(abc.ABC, AsyncIterator[T]):\n    \"\"\"\n    An AsyncIterator that will stream results in pages.\n    \"\"\"\n\n    def __init__(\n        self, client: NamedCacheClient[K, V], result_handler: Callable[[Serializer, Any], Any], keys: bool = False\n    ) -> None:\n        super().__init__()\n        # flag indicating that all pages have been processed\n        self._exhausted: bool = False\n\n        # the gRPC client\n        self._client: NamedCacheClient[K, V] = client\n\n        # the handler responsible for deserializing the result\n        self._result_handler: Callable[[Serializer, Any], Any] = result_handler\n\n        # the serializer to be used when deserializing streamed results\n        self._serializer: Serializer = client._request_factory.get_serializer()\n\n        # cookie that tracks page streaming; used for each new page request\n        self._cookie: bytes = bytes()\n\n        # the gRPC stream providing the results\n        self._stream: grpc.Channel.unary_stream = None\n\n        # flag indicating a new page has been loaded\n        self._new_page: bool = True\n\n        # flag indicating that pages will be keys only\n        self._keys: bool = keys\n\n    def __aiter__(self) -> AsyncIterator[T]:\n        return self\n\n    async def __anext__(self) -> T:\n        # keep the loop running to ensure we don't exit\n        # prematurely which would result in a None value\n        # being returned incorrectly between pages\n        while True:\n            if self._stream is None and not self._exhausted:\n                await self.__load_next_page()\n\n            if self._stream is None and self._exhausted:\n                raise StopAsyncIteration\n\n            async for item in self._stream:\n                if self._new_page:  # a new page has been loaded; the cookie will be the first result\n                    self._new_page = False\n                    self._cookie = item.value if self._keys else item.cookie\n                    if self._cookie == b\"\":\n                        self._exhausted = True  # processing the last page\n                else:\n                    return self._result_handler(self._serializer, item)\n\n            self._stream = None\n            if self._exhausted:\n                raise StopAsyncIteration\n\n    # noinspection PyProtectedMember\n    async def __load_next_page(self) -> None:\n        \"\"\"\n        Requests the next page of results to be streamed.\n\n        :return: None\n        \"\"\"\n        request: PageRequest = self._client._request_factory.page_request(self._cookie)\n        self._stream = self._get_stream(request)\n        self._new_page = True\n\n    def _get_stream(self, request: PageRequest) -> grpc.Channel.unary_stream:\n        \"\"\"\n        Obtain the gRPC unary_stream for the provided PageRequest.\n\n        :param request: the PageRequest\n        :return: the gRPC unary_stream for the given request\n        \"\"\"\n        client_stub: NamedCacheServiceStub = self._client._client_stub\n        return client_stub.nextKeySetPage(request) if self._keys else client_stub.nextEntrySetPage(request)\n\n\ndef _scalar_deserializer(serializer: Serializer, item: Any) -> Any:\n    \"\"\"\n    Helper method to deserialize a key or value returned in a stream.\n\n    :param serializer: the serializer that should be used\n    :param item: the key or value to deserialize\n    :return: the deserialized key or value\n    \"\"\"\n    return serializer.deserialize(item.value)\n\n\ndef _entry_deserializer(serializer: Serializer, item: Any) -> MapEntry[Any, Any]:\n    \"\"\"\n    Helper method to deserialize entries returned in a stream.\n\n    :param serializer: the serializer that should be used to deserialize the entry\n    :param item: the entry\n    :return: the deserialized entry\n    \"\"\"\n    return MapEntry(serializer.deserialize(item.key), serializer.deserialize(item.value))\n\n\nasync def _scalar_producer(serializer: Serializer, stream: grpc.Channel.unary_stream) -> T:\n    \"\"\"\n    Helper method to iterate over a stream and produce scalar results.\n\n    :param serializer: the serializer that should be used to deserialize the scalar value\n    :param stream: the gRPC stream\n    :return: one or more deserialized scalar values\n    \"\"\"\n    async for item in stream:\n        return _scalar_deserializer(serializer, item)\n    raise StopAsyncIteration\n\n\nasync def _entry_producer(serializer: Serializer, stream: grpc.Channel.unary_stream) -> MapEntry[K, V]:\n    \"\"\"\n    Helper method to iterate over a stream and produce MapEntry instances\n    .\n    :param serializer: the serializer that should be used to deserialize the entry\n    :param stream: the gRPC stream\n    :return: one or more deserialized MapEntry instances\n    \"\"\"\n    async for item in stream:\n        return _entry_deserializer(serializer, item)\n    raise StopAsyncIteration\n\n\nasync def _entry_producer_from_list(serializer: Serializer, the_list: list) -> MapEntry[K, V]:\n    if len(the_list) == 0:\n        raise StopAsyncIteration\n    for item in the_list:\n        await asyncio.sleep(0)\n        the_list.pop(0)\n        return _entry_deserializer(serializer, item)\n\n\nclass _ListAsyncIterator(abc.ABC, AsyncIterator[T]):\n    def __init__(\n            self,\n            serializer: Serializer,\n            the_list: list,\n            next_producer: Callable[[Serializer, list], Awaitable[T]],\n    ) -> None:\n        super().__init__()\n        # A function that may be called to produce a series of results\n        self._next_producer = next_producer\n\n        # the Serializer that should be used to deserialize results\n        self._serializer = serializer\n\n        # the gRPC stream providing results\n        self._the_list = the_list\n\n    def __aiter__(self) -> AsyncIterator[T]:\n        return self\n\n    def __anext__(self) -> Awaitable[T]:\n        return self._next_producer(self._serializer, self._the_list)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/coherence/client.py b/src/coherence/client.py
--- a/src/coherence/client.py	(revision 7ed18d6c9ea66aec63a655a1b556e3c67cdb16b6)
+++ b/src/coherence/client.py	(date 1725567301798)
@@ -868,7 +868,7 @@
         named_cache_request = self._request_factory.ensure_request(self._cache_name)
         proxy_request = self._request_factory.create_proxy_request(named_cache_request)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id, named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         response = await asyncio.wait_for(self._stream_handler.get_response(request_id), 1.0)
         self.cache_id = response.cacheId
         self._session.update_cache_id_map(self.cache_id, self)
@@ -879,8 +879,7 @@
         proxy_request = self._request_factory.create_proxy_request(
             named_cache_request)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         response = await asyncio.wait_for(
             self._stream_handler.get_response(request_id), 10.0)
         if response.HasField("message"):
@@ -894,12 +893,9 @@
             return None
 
     async def put(self, key: K, value: V, ttl: int = 0) -> V:
-        named_cache_request = self._request_factory.put_request(key, value, ttl)
-        proxy_request = self._request_factory.create_proxy_request(
-            named_cache_request)
+        proxy_request = self._request_factory.put_request(key, value, ttl)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         response = await asyncio.wait_for(
             self._stream_handler.get_response(request_id), 10.0)
         if response is None:
@@ -918,8 +914,7 @@
             proxy_request = self._request_factory.create_proxy_request(
                 named_cache_request)
             request_id = proxy_request.id
-            await self._stream_handler.write_request(proxy_request, request_id,
-                                                     named_cache_request)
+            await self._stream_handler.write_request(proxy_request, request_id)
             response = await asyncio.wait_for(
                 self._stream_handler.get_response(request_id), 10.0)
             if response is None:
@@ -972,8 +967,7 @@
         proxy_request = (self._request_factory.
                          create_proxy_request(named_cache_request))
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         result_set = await asyncio.wait_for(
             self._stream_handler.get_response_collection(request_id),
             1.0)
@@ -992,8 +986,7 @@
         proxy_request = (self._request_factory.
                          create_proxy_request(named_cache_request))
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         await asyncio.wait_for(
             self._stream_handler.get_response(request_id),
             1.0)
@@ -1003,8 +996,7 @@
         proxy_request = (self._request_factory.
                          create_proxy_request(named_cache_request))
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         await asyncio.wait_for(self._stream_handler.get_response(request_id),
                                1.0)
 
@@ -1013,8 +1005,7 @@
         proxy_request = self._request_factory.create_proxy_request(
             named_cache_request)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         await asyncio.wait_for(self._stream_handler.get_response(request_id),
                                1.0)
 
@@ -1026,8 +1017,7 @@
         proxy_request = self._request_factory.create_proxy_request(
             named_cache_request)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         await asyncio.wait_for(self._stream_handler.get_response(request_id),
                                1.0)
 
@@ -1036,8 +1026,7 @@
         proxy_request = self._request_factory.create_proxy_request(
             named_cache_request)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         response = await asyncio.wait_for(
             self._stream_handler.get_response(request_id), 10.0)
         if response.HasField("message"):
@@ -1053,8 +1042,7 @@
         proxy_request = self._request_factory.create_proxy_request(
             named_cache_request)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         response = await asyncio.wait_for(
             self._stream_handler.get_response(request_id), 10.0)
         if response.HasField("message"):
@@ -1070,8 +1058,7 @@
         proxy_request = self._request_factory.create_proxy_request(
             named_cache_request)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         response = await asyncio.wait_for(
             self._stream_handler.get_response(request_id), 10.0)
         if response.HasField("message"):
@@ -1087,8 +1074,7 @@
         proxy_request = self._request_factory.create_proxy_request(
             named_cache_request)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         response = await asyncio.wait_for(
             self._stream_handler.get_response(request_id), 10.0)
         if response.HasField("message"):
@@ -1104,8 +1090,7 @@
         proxy_request = self._request_factory.create_proxy_request(
             named_cache_request)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         response = await asyncio.wait_for(
             self._stream_handler.get_response(request_id), 10.0)
         if response.HasField("message"):
@@ -1121,8 +1106,7 @@
         proxy_request = self._request_factory.create_proxy_request(
             named_cache_request)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         response = await asyncio.wait_for(
             self._stream_handler.get_response(request_id), 10.0)
         if response.HasField("message"):
@@ -1138,8 +1122,7 @@
         proxy_request = self._request_factory.create_proxy_request(
             named_cache_request)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         response = await asyncio.wait_for(
             self._stream_handler.get_response(request_id), 10.0)
         if response.HasField("message"):
@@ -1155,8 +1138,7 @@
         proxy_request = self._request_factory.create_proxy_request(
             named_cache_request)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         response = await asyncio.wait_for(
             self._stream_handler.get_response(request_id), 10.0)
         if response.HasField("message"):
@@ -1172,8 +1154,7 @@
         proxy_request = self._request_factory.create_proxy_request(
             named_cache_request)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         result_set = await asyncio.wait_for(
             self._stream_handler.get_response_collection(request_id),
             1.0)
@@ -1194,8 +1175,7 @@
         proxy_request = self._request_factory.create_proxy_request(
             named_cache_request)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         result_set = await asyncio.wait_for(
             self._stream_handler.get_response_collection(request_id),
             1.0)
@@ -1217,8 +1197,7 @@
         proxy_request = self._request_factory.create_proxy_request(
             named_cache_request)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         response = await asyncio.wait_for(
             self._stream_handler.get_response(request_id), 1.0)
         if response.HasField("message"):
@@ -1254,8 +1233,7 @@
         proxy_request = self._request_factory.create_proxy_request(
             named_cache_request)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         await asyncio.wait_for(self._stream_handler.get_response(request_id),
                                1.0)
 
@@ -1267,8 +1245,7 @@
         proxy_request = self._request_factory.create_proxy_request(
             named_cache_request)
         request_id = proxy_request.id
-        await self._stream_handler.write_request(proxy_request, request_id,
-                                                 named_cache_request)
+        await self._stream_handler.write_request(proxy_request, request_id)
         await asyncio.wait_for(self._stream_handler.get_response(request_id),
                                1.0)
 
Index: src/coherence/extractor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Copyright (c) 2022, 2024, Oracle and/or its affiliates.\n# Licensed under the Universal Permissive License v 1.0 as shown at\n# https://oss.oracle.com/licenses/upl.\n\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Generic, Optional, Sequence, TypeVar, Union, cast\n\nfrom typing_extensions import TypeAlias\n\nfrom .serialization import proxy\n\nE = TypeVar(\"E\")\nK = TypeVar(\"K\")\nV = TypeVar(\"V\")\nR = TypeVar(\"R\")\nT = TypeVar(\"T\")\n\n\nclass ValueExtractor(ABC, Generic[T, E]):\n    def __init__(self) -> None:\n        \"\"\"\n        ValueExtractor is used to both extract values (for example, for sorting or filtering) from an object,\n        and to provide an identity for that extraction.\n\n        Construct a ValueExtractor.\n        \"\"\"\n        super().__init__()\n\n    def compose(self, before: ValueExtractor[T, E]) -> ValueExtractor[T, E]:\n        \"\"\"\n        Returns a composed extractor that first applies the *before* extractor to its input, and then applies this\n        extractor to the result. If evaluation of either extractor throws an exception, it is relayed to the caller\n        of the composed extractor.\n\n        :param before: the extractor to apply before this extractor is applied\n        :return: a composed extractor that first applies the *before* extractor and then applies this extractor\n        \"\"\"\n        if before is None:\n            raise ValueError(\"before cannot be null\")\n        if type(before) == ValueExtractor:  # noqa: E721\n            return before.and_then(self)\n        else:\n            return ChainedExtractor([before, self])\n\n    def and_then(self, after: ValueExtractor[T, E]) -> ValueExtractor[T, E]:\n        \"\"\"\n        Returns a composed extractor that first applies this extractor to its input, and then applies the *after*\n        extractor to the result. If evaluation of either extractor throws an exception, it is relayed to the caller\n        of the composed extractor.\n\n        :param after: the extractor to apply after this extractor is applied\n        :return: a composed extractor that first applies this extractor and then applies the *after* extractor\n        \"\"\"\n        if after is None:\n            raise ValueError(\"after cannot be null\")\n        if type(after) == ChainedExtractor:  # noqa: E721\n            return ChainedExtractor([self, after])\n        else:\n            return after.compose(self)\n\n    @classmethod\n    def extract(cls, from_field_or_method: str, params: Optional[list[Any]] = None) -> ValueExtractor[T, E]:\n        \"\"\"\n        Returns an extractor that extracts the value of the specified field.\n\n        :param from_field_or_method: the name of the field or method to extract the value from.\n        :param params: the parameters to pass to the method.\n        :return: an instance of :class:`coherence.extractor.UniversalExtractor`\n        \"\"\"\n        return UniversalExtractor(from_field_or_method, params)\n\n\n@proxy(\"extractor.UniversalExtractor\")\nclass UniversalExtractor(ValueExtractor[T, Any]):\n    def __init__(self, name: str, params: Optional[list[Any]] = None) -> None:\n        \"\"\"\n        Universal ValueExtractor implementation.\n\n        Either a property or method based extractor based on parameters passed to constructor. Generally,\n        the name value passed to the `UniversalExtractor` constructor represents a property unless the *name* value\n        ends in `()`, then this instance is a reflection based method extractor.\n\n        Construct a UniversalExtractor based on a name and optional parameters.\n\n        If *name* does not end in `()`, this extractor is a property extractor. If `name` is prefixed with one of\n        `set` or `get` and ends in `()`, this extractor is a property extractor. If the *name* just ends in `()`,\n        this extractor is considered a method extractor.\n\n        :param name: A method or property name.\n        :param params: the parameter array. Must be `null` or `zero length` for a property based extractor.\n        \"\"\"\n        super().__init__()\n        self.name: str = name\n        self.params = params\n\n    @classmethod\n    def create(cls, name: str, params: Optional[list[Any]] = None) -> UniversalExtractor[T]:\n        \"\"\"\n        Class method to create an instance of :class:`coherence.extractor.UniversalExtractor`\n\n        :param name: A method or property name.\n        :param params: the parameter array. Must be `null` or `zero length` for a property based extractor.\n        :return: an instance of :class:`coherence.extractor.UniversalExtractor`\n        \"\"\"\n        return cls(name, params)\n\n\nclass AbstractCompositeExtractor(ValueExtractor[T, E]):\n    def __init__(self, extractors: Sequence[ValueExtractor[T, E]]) -> None:\n        \"\"\"\n        Abstract super class for :class:`coherence.extractor.ValueExtractor` implementations that are based on an\n        underlying array of :class:`coherence.extractor.ValueExtractor` objects.\n\n        :param extractors: an array of extractors\n        \"\"\"\n        super().__init__()\n        self.extractors = extractors\n\n\n@proxy(\"extractor.ChainedExtractor\")\nclass ChainedExtractor(AbstractCompositeExtractor[T, Any]):\n    def __init__(self, extractors_or_method: str | Sequence[ValueExtractor[T, Any]]) -> None:\n        \"\"\"\n        Composite :class:`coherence.extractor.ValueExtractor` implementation based on an array of extractors. The\n        extractors in the array are applied sequentially left-to-right, so a result of a previous extractor serves as\n        a target object for a next one.\n\n        :param extractors_or_method: an array of :class:`coherence.extractor.ValueExtractor`, or a dot-delimited\n         sequence of method names which results in a ChainedExtractor that is based on an array of corresponding\n         :class:`coherence.extractor.UniversalExtractor` objects\n        \"\"\"\n        if type(extractors_or_method) == str:  # noqa: E721\n            e = list()\n            names = extractors_or_method.split(\".\")\n            for name in names:\n                v: UniversalExtractor[T] = UniversalExtractor(name)\n                e.append(v)\n            super().__init__(e)\n        else:\n            super().__init__(cast(Sequence[ValueExtractor[T, Any]], extractors_or_method))\n\n\n@proxy(\"extractor.MultiExtractor\")\nclass MultiExtractor(AbstractCompositeExtractor[Any, Any]):\n    def __init__(self, extractors_or_method: str | Sequence[ValueExtractor[Any, Any]]) -> None:\n        \"\"\"\n        Composite :class:`coherence.extractor.ValueExtractor` implementation based on an array of extractors. The\n        extractors in the array are applied sequentially left-to-right, so a result of a previous extractor serves as\n        a target object for a next one.\n\n        :param extractors_or_method: an array of :class:`coherence.extractor.ValueExtractor`, or a dot-delimited\n         sequence of method names which results in a ChainedExtractor that is based on an array of corresponding\n         :class:`coherence.extractor.UniversalExtractor` objects\n        \"\"\"\n        if type(extractors_or_method) == str:  # noqa: E721\n            e = list()\n            names = extractors_or_method.split(\",\")\n            for name in names:\n                v: UniversalExtractor[Any] = UniversalExtractor(name)\n                e.append(v)\n            super().__init__(e)\n        else:\n            super().__init__(cast(Sequence[ValueExtractor[Any, Any]], extractors_or_method))\n\n\n@proxy(\"extractor.IdentityExtractor\")\nclass IdentityExtractor(ValueExtractor[T, Any]):\n    __instance = None\n\n    def __init__(self) -> None:\n        \"\"\"\n        A Trivial :class:`coherence.extractor.ValueExtractor` implementation that does not actually extract anything\n        from the passed value, but returns the value itself.\n\n        Constructs a new `IdentityExtractor` instance.\n        \"\"\"\n        super().__init__()\n\n\nclass ValueUpdater(ABC, Generic[T, R]):\n    def __init__(self) -> None:\n        \"\"\"\n        ValueUpdater is used to update an object's state.\n\n        Constructs a new `ValueUpdater`.\n        \"\"\"\n        super().__init__()\n\n\nclass ValueManipulator(Generic[T, R]):\n    \"\"\"ValueManipulator represents a composition of :class:`coherence.extractor.ValueExtractor` and\n    :class:`coherence.extractor.ValueUpdater` implementations.\"\"\"\n\n    @abstractmethod\n    def get_extractor(self) -> ValueExtractor[T, R]:\n        \"\"\"\n        Retrieve the underlying ValueExtractor reference.\n\n        :rtype: the ValueExtractor\n        \"\"\"\n\n    @abstractmethod\n    def get_updator(self) -> ValueUpdater[T, R]:\n        \"\"\"\n        Retrieve the underlying ValueUpdater reference.\n\n        :rtype: the ValueUpdater\n        \"\"\"\n\n\n@proxy(\"extractor.CompositeUpdater\")\nclass CompositeUpdater(ValueManipulator[T, R], ValueUpdater[T, R]):\n    \"\"\"A ValueUpdater implementation based on an extractor-updater pair that could also be used as a\n    ValueManipulator.\"\"\"\n\n    def __init__(\n        self, method_or_extractor: ExtractorExpression[T, R], updater: Optional[ValueUpdater[T, R]] = None\n    ) -> None:\n        \"\"\"\n        Constructs a new `CompositeUpdater`.\n\n        :param method_or_extractor: The ValueExtractor part.\n        :param updater: The ValueUpdater part.\n        \"\"\"\n        super().__init__()\n        if updater is not None:  # Two arg constructor\n            self.extractor = cast(ValueExtractor[T, R], method_or_extractor)\n            self.updater = updater\n        else:  # One arg with method name\n            last = str(method_or_extractor).rfind(\".\")\n            if last == -1:\n                self.extractor = IdentityExtractor()\n            else:\n                self.extractor = ChainedExtractor(str(method_or_extractor)[0:last])\n            self.updater = UniversalUpdater(str(method_or_extractor)[last + 1 :])\n\n    def get_extractor(self) -> ValueExtractor[T, R]:\n        return self.extractor\n\n    def get_updator(self) -> ValueUpdater[T, R]:\n        return self.updater\n\n\n@proxy(\"extractor.UniversalUpdater\")\nclass UniversalUpdater(ValueUpdater[V, R]):\n    \"\"\"Universal ValueUpdater implementation.\n\n    Either a property-based and method-based {@link ValueUpdater} based on whether constructor parameter *name*\n    is evaluated to be a property or method.\"\"\"\n\n    def __init__(self, method: str) -> None:\n        \"\"\"\n        Construct a UniversalUpdater for the provided name.\n\n        If method ends in a '()', then the name is a method name. This implementation assumes that a target's class\n        will have one and only one method with the specified name and this method will have exactly one parameter; if\n        the method is a property name, there should be a corresponding JavaBean property modifier method, or it will\n        be used as a key in a Map.\n\n        :param method: a method or property name\n        \"\"\"\n        super().__init__()\n        self.name = method\n\n\nclass Extractors:\n    \"\"\"\n    A Utility class for creating extractors.\n    \"\"\"\n\n    @classmethod\n    def extract(cls, expression: str, params: Optional[list[Any]] = None) -> ValueExtractor[T, E]:\n        \"\"\"\n        If providing only an expression, the following rules apply:\n          - if the expression contains multiple values separated by a period,\n            the expression will be treated as a chained expression.  E.g.,\n            the expression 'a.b.c' would be treated as extract the 'a'\n            property, from that result, extract the 'b' property, and finally\n            from that result, extract the 'c' property.\n          - if the expression contains multiple values separated by a comma,\n            the expression will be treated as a multi expression.  E.g.,\n            the expression 'a,b,c' would be treated as extract the 'a', 'b',\n            and 'c' properties from the same object.\n          - for either case, the params argument is ignored.\n\n        It is also possible to invoke, and pass arguments to, arbitrary methods.\n        For example, if the target object of the extraction is a String, it's\n        possible to call the length() function by passing an expression of\n        \"length()\".  If the target method accepts arguments, provide a list\n        of one or more arguments to be passed.\n\n        :param expression: the extractor expression\n        :param params:  the params to pass to the method invocation\n        :return: a ValueExtractor based on the provided inputs\n        \"\"\"\n        if expression is None:\n            raise ValueError(\"An expression must be provided\")\n\n        if params is None or len(params) == 0:\n            if \".\" in expression:\n                return ChainedExtractor(expression)\n            elif \",\" in expression:\n                return MultiExtractor(expression)\n            else:\n                return UniversalExtractor(expression)\n\n        expr: str = expression\n        if not expr.endswith(\"()\"):\n            expr = expr + \"()\"\n\n        return UniversalExtractor(expr, params)\n\n    @classmethod\n    def identity(cls) -> IdentityExtractor[Any]:\n        \"\"\"\n        Returns an extractor that does not actually extract anything\n        from the passed value, but returns the value itself.\n\n        :return: an extractor that does not actually extract anything\n          from the passed value, but returns the value itself\n        \"\"\"\n        return IdentityExtractor()\n\n\nExtractorExpression: TypeAlias = Union[ValueExtractor[T, E], str]\nManipulatorExpression: TypeAlias = Union[ValueManipulator[T, E], str]\nUpdaterExpression: TypeAlias = Union[ValueUpdater[T, R], str]\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/coherence/extractor.py b/src/coherence/extractor.py
--- a/src/coherence/extractor.py	(revision 7ed18d6c9ea66aec63a655a1b556e3c67cdb16b6)
+++ b/src/coherence/extractor.py	(date 1725564602814)
@@ -5,9 +5,7 @@
 from __future__ import annotations
 
 from abc import ABC, abstractmethod
-from typing import Any, Generic, Optional, Sequence, TypeVar, Union, cast
-
-from typing_extensions import TypeAlias
+from typing import Any, Generic, Optional, Sequence, TypeVar, Union, cast, TypeAlias
 
 from .serialization import proxy
 
Index: Makefile
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ----------------------------------------------------------------------------------------------------------------------\n# Copyright (c) 2022, 2023, Oracle and/or its affiliates.\n# Licensed under the Universal Permissive License v 1.0 as shown at\n# https://oss.oracle.com/licenses/upl.\n#\n# ----------------------------------------------------------------------------------------------------------------------\n# This is the Makefile to build the Coherence Python Client\n# ----------------------------------------------------------------------------------------------------------------------\n\nSHELL := /bin/bash\nVERSION ?=0.9.0\nCURRDIR := $(shell pwd)\nUSER_ID := $(shell echo \"`id -u`:`id -g`\")\n\noverride BUILD_BIN           := $(CURRDIR)/bin\noverride PROTO_DIR\t\t\t := $(CURRDIR)/etc/proto\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Set the location of various build tools\n# ----------------------------------------------------------------------------------------------------------------------\noverride BUILD_OUTPUT        := $(CURRDIR)/build/_output\noverride BUILD_BIN           := $(CURRDIR)/bin\noverride PROTO_OUT           := $(CURRDIR)/proto\noverride BUILD_TARGETS       := $(BUILD_OUTPUT)/targets\noverride TEST_LOGS_DIR       := $(BUILD_OUTPUT)/test-logs\noverride COVERAGE_DIR        := $(BUILD_OUTPUT)/coverage\noverride COPYRIGHT_JAR       := glassfish-copyright-maven-plugin-2.4.jar\noverride BUILD_CERTS         := $(CURRDIR)/tests/utils/certs\noverride ENV_FILE            := tests/utils/.env\n\n# Maven version is always 1.0.0 as it is only for testing\nMVN_VERSION ?= 1.0.0\n\n# Coherence CE version to run base tests against\nCOHERENCE_VERSION ?= 22.06.7\nCOHERENCE_GROUP_ID ?= com.oracle.coherence.ce\nCOHERENCE_WKA1 ?= server1\nCOHERENCE_WKA2 ?= server1\nCLUSTER_PORT ?= 7574\n# Profiles to include for building\nPROFILES ?= \",-jakarta,javax\"\nCOHERENCE_BASE_IMAGE ?= gcr.io/distroless/java17-debian11\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Set the location of various build tools\n# ----------------------------------------------------------------------------------------------------------------------\nTOOLS_DIRECTORY   = $(CURRDIR)/build/tools\nTOOLS_BIN         = $(TOOLS_DIRECTORY)/bin\n\n# ----------------------------------------------------------------------------------------------------------------------\n# The test application images used in integration tests\n# ----------------------------------------------------------------------------------------------------------------------\nRELEASE_IMAGE_PREFIX     ?= ghcr.io/oracle/\nTEST_APPLICATION_IMAGE_1 := $(RELEASE_IMAGE_PREFIX)coherence-python-test-1:1.0.0\nTEST_APPLICATION_IMAGE_2 := $(RELEASE_IMAGE_PREFIX)coherence-python-test-2:1.0.0\nGO_TEST_FLAGS ?= -timeout 20m\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Options to append to the Maven command\n# ----------------------------------------------------------------------------------------------------------------------\nMAVEN_OPTIONS ?= -Dmaven.wagon.httpconnectionManager.ttlSeconds=25 -Dmaven.wagon.http.retryHandler.count=3\nMAVEN_BUILD_OPTS :=$(USE_MAVEN_SETTINGS) -Drevision=$(MVN_VERSION) -Dcoherence.version=$(COHERENCE_VERSION) -Dcoherence.group.id=$(COHERENCE_GROUP_ID) $(MAVEN_OPTIONS)\n\nCURRDIR := $(shell pwd)\n\nCOMPOSE:=$(shell type -p docker-compose || echo docker compose)\n$(info COMPOSE = $(COMPOSE))\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Clean-up all of the build artifacts\n# ----------------------------------------------------------------------------------------------------------------------\n.PHONY: clean\nclean: ## Cleans the build\n\t@echo \"Cleaning Project\"\n\t-rm -rf $(CURRDIR)/build\n#\t-rm -rf $(PROTO_DIR)\n\t-rm -rf $(CURRDIR)/htmlcov\n\t-rm -rf $(CURRDIR)/.pytest_cache\n\t-rm -rf $(BUILD_CERTS)\n\t@mkdir -p $(BUILD_CERTS)\n\tmvn -B -f tests/java/coherence-python-test $(MAVEN_BUILD_OPTS) clean\n\n.PHONY: certs\ncerts: ## Generates certificates for TLS tests\n\t@echo \"Generating certs\"\n\t./tests/scripts/keys.sh $(BUILD_CERTS)\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Configure the build properties\n# ----------------------------------------------------------------------------------------------------------------------\n$(BUILD_PROPS):\n\t@echo \"Creating build directories\"\n\t@mkdir -p $(BUILD_OUTPUT)\n\t@mkdir -p $(BUILD_BIN)\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Build the Coherence Go Client Test Image\n# ----------------------------------------------------------------------------------------------------------------------\n.PHONY: build-test-images\nbuild-test-images: ## Build the Test images\n\t@echo \"${MAVEN_BUILD_OPTS}\"\n\tmvn -B -f tests/java clean package jib:dockerBuild -DskipTests -P member1$(PROFILES) -Djib.to.image=$(TEST_APPLICATION_IMAGE_1) -Dcoherence.test.base.image=$(COHERENCE_BASE_IMAGE) $(MAVEN_BUILD_OPTS)\n\tmvn -B -f tests/java clean package jib:dockerBuild -DskipTests -P member2$(PROFILES) -Djib.to.image=$(TEST_APPLICATION_IMAGE_2) -Dcoherence.test.base.image=$(COHERENCE_BASE_IMAGE) $(MAVEN_BUILD_OPTS)\n\techo \"CURRENT_UID=$(USER_ID)\" >> $(ENV_FILE)\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Download and build proto files\n# ----------------------------------------------------------------------------------------------------------------------\n.PHONY: generate-proto\ngenerate-proto:  ## Generate Proto Files\n\tmkdir -p $(PROTO_DIR) || true\n\tcurl -o $(PROTO_DIR)/proxy_service_v1.proto \\\n\t\thttps://raw.githubusercontent.com/oracle/coherence/$(COHERENCE_VERSION)/prj/coherence-grpc/src/main/proto/proxy_service_v1.proto\n\tcurl -o $(PROTO_DIR)/proxy_service_messages_v1.proto \\\n\t\thttps://raw.githubusercontent.com/oracle/coherence/$(COHERENCE_VERSION)/prj/coherence-grpc/src/main/proto/proxy_service_messages_v1.proto\n\tcurl -o $(PROTO_DIR)/common_messages_v1.proto \\\n\t\thttps://raw.githubusercontent.com/oracle/coherence/$(COHERENCE_VERSION)/prj/coherence-grpc/src/main/proto/common_messages_v1.proto\n\tcurl -o $(PROTO_DIR)/cache_service_messages_v1.proto \\\n\t\thttps://raw.githubusercontent.com/oracle/coherence/$(COHERENCE_VERSION)/prj/coherence-grpc/src/main/proto/cache_service_messages_v1.proto\n\tpython -m grpc_tools.protoc --proto_path=$(CURRDIR)/etc/proto --python_out=$(CURRDIR)/src/coherence \\\n\t\t\t--grpc_python_out=$(CURRDIR)/src/coherence \\\n\t\t\t$(CURRDIR)/etc/proto/proxy_service_v1.proto \\\n\t\t\t$(CURRDIR)/etc/proto/proxy_service_messages_v1.proto \\\n\t\t\t$(CURRDIR)/etc/proto/common_messages_v1.proto \\\n\t\t\t$(CURRDIR)/etc/proto/cache_service_messages_v1.proto\n\tsed -e 's/import proxy_service_messages_v1_pb2 as proxy__service__messages__v1__pb2/import coherence.proxy_service_messages_v1_pb2 as proxy__service__messages__v1__pb2/' \\\n\t\t< $(CURRDIR)/src/coherence/proxy_service_v1_pb2.py > $(CURRDIR)/src/coherence/proxy_service_v1_pb2.py.out\n\tmv $(CURRDIR)/src/coherence/proxy_service_v1_pb2.py.out $(CURRDIR)/src/coherence/proxy_service_v1_pb2.py\n\tsed -e 's/import common_messages_v1_pb2 as common__messages__v1__pb2/import coherence.common_messages_v1_pb2 as common__messages__v1__pb2/' \\\n\t\t< $(CURRDIR)/src/coherence/proxy_service_messages_v1_pb2.py > $(CURRDIR)/src/coherence/proxy_service_messages_v1_pb2.py.out\n\tmv $(CURRDIR)/src/coherence/proxy_service_messages_v1_pb2.py.out $(CURRDIR)/src/coherence/proxy_service_messages_v1_pb2.py\n\tsed -e 's/import proxy_service_messages_v1_pb2 as proxy__service__messages__v1__pb2/import coherence.proxy_service_messages_v1_pb2 as proxy__service__messages__v1__pb2/' \\\n\t\t< $(CURRDIR)/src/coherence/proxy_service_v1_pb2_grpc.py > $(CURRDIR)/src/coherence/proxy_service_v1_pb2_grpc.py.out\n\tmv $(CURRDIR)/src/coherence/proxy_service_v1_pb2_grpc.py.out $(CURRDIR)/src/coherence/proxy_service_v1_pb2_grpc.py\n\tsed -e 's/import common_messages_v1_pb2 as common__messages__v1__pb2/import coherence.common_messages_v1_pb2 as common__messages__v1__pb2/' \\\n\t\t< $(CURRDIR)/src/coherence/cache_service_messages_v1_pb2.py > $(CURRDIR)/src/coherence/cache_service_messages_v1_pb2.py.out\n\tmv $(CURRDIR)/src/coherence/cache_service_messages_v1_pb2.py.out $(CURRDIR)/src/coherence/cache_service_messages_v1_pb2.py\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Run tests with code coverage\n# ----------------------------------------------------------------------------------------------------------------------\n.PHONY: test\ntest:  ##\n\tpytest -W error --cov src/coherence --cov-report=term --cov-report=html \\\n\t\ttests/test_serialization.py \\\n\t\ttests/test_extractors.py \\\n\t\ttests/test_session.py \\\n\t\ttests/test_client.py \\\n\t\ttests/test_events.py \\\n\t\ttests/test_filters.py \\\n\t\ttests/test_processors.py \\\n\t\ttests/test_aggregators.py \\\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Run standards validation across project\n# ----------------------------------------------------------------------------------------------------------------------\n.PHONY: validate-setup\nvalidate-setup:  ##\n\tpoetry update\n\tpre-commit autoupdate\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Run standards validation across project\n# ----------------------------------------------------------------------------------------------------------------------\n.PHONY: validate\nvalidate:  ##\n\tpre-commit run --all-files\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Obtain the protoc binary\n# ----------------------------------------------------------------------------------------------------------------------\n$(TOOLS_BIN)/protoc:\n\t@mkdir -p $(TOOLS_BIN)\n\tcurl -Lo $(TOOLS_DIRECTORY)/protoc-3.19.4-osx-x86_64.zip https://github.com/protocolbuffers/protobuf/releases/download/v3.19.4/protoc-3.19.4-osx-x86_64.zip\n\tcd $(TOOLS_DIRECTORY)\n\tunzip -d $(TOOLS_DIRECTORY) $(TOOLS_DIRECTORY)/protoc-3.19.4-osx-x86_64.zip\n\n#-----------------------------------------------------------------------------------------------------------------------\n# Generate HTML documentation\n# Run this target only in poetry shell\n# The generated html pages are in $(CURRDIR)/docs/_build\n# ----------------------------------------------------------------------------------------------------------------------\n.PHONY: docs\ndocs:  ## Generate doc\n\tcd $(CURRDIR)/docs;\t\\\n\tpoetry run sphinx-build -b html . _build\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Startup cluster members via docker compose\n# ----------------------------------------------------------------------------------------------------------------------\n.PHONY: test-cluster-startup\ntest-cluster-startup: $(BUILD_PROPS) ## Startup any test cluster members using docker-compose\n\tcd tests/utils && ${COMPOSE} -f docker-compose-2-members.yaml up -d\n\t$(eval LOGFILE_NAME=log-clear-tests-$(COHERENCE_VERSION).txt)\nifeq ($(RUN_SECURE), true)\n\t$(eval LOGFILE_NAME=log-ssl-tests-$(COHERENCE_VERSION).txt)\nendif\n\tcd tests/utils && ${COMPOSE} -f docker-compose-2-members.yaml logs -f --no-color > $(LOGFILE_NAME) &\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Shutdown any cluster members via docker compose\n# ----------------------------------------------------------------------------------------------------------------------\n.PHONY: test-cluster-shutdown\ntest-cluster-shutdown: ## Shutdown any test cluster members using docker-compose\n\tcd tests/utils && ${COMPOSE} -f docker-compose-2-members.yaml down || true\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Startup standalone coherence via java -jar\n# ----------------------------------------------------------------------------------------------------------------------\n.PHONY: test-coherence-startup\ntest-coherence-startup: ## Startup standalone cluster\n\tscripts/startup-clusters.sh $(TEST_LOGS_DIR) $(CLUSTER_PORT) $(COHERENCE_GROUP_ID) ${COHERENCE_VERSION}\n\t@echo \"Clusters started up\"\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Shutdown coherence via java -jar\n# ----------------------------------------------------------------------------------------------------------------------\n.PHONY: test-coherence-shutdown\ntest-coherence-shutdown: ## shutdown standalone cluster\n\t@ps -ef | grep shutMeDownPlease | grep -v grep | awk '{print $$2}' | xargs kill -9 || true\n\t@echo \"Clusters shutdown\"\n\n# ----------------------------------------------------------------------------------------------------------------------\n# wait for 30 seconds\n# ----------------------------------------------------------------------------------------------------------------------\n.PHONY: just-wait\njust-wait: ## sleep for 30 seconds\n\t@echo \"Sleep for 30 seconds\"\n\tsleep 30\n\n# ----------------------------------------------------------------------------------------------------------------------\n# Remove docker images\n# ----------------------------------------------------------------------------------------------------------------------\n.PHONY: remove-app-images\nremove-app-images: ## Remove docker images\n\t@echo \"Remove docker images\"\n\tdocker image rmi $(TEST_APPLICATION_IMAGE_1) $(TEST_APPLICATION_IMAGE_2) || true\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Makefile b/Makefile
--- a/Makefile	(revision 7ed18d6c9ea66aec63a655a1b556e3c67cdb16b6)
+++ b/Makefile	(date 1725488163861)
@@ -118,24 +118,6 @@
 		https://raw.githubusercontent.com/oracle/coherence/$(COHERENCE_VERSION)/prj/coherence-grpc/src/main/proto/common_messages_v1.proto
 	curl -o $(PROTO_DIR)/cache_service_messages_v1.proto \
 		https://raw.githubusercontent.com/oracle/coherence/$(COHERENCE_VERSION)/prj/coherence-grpc/src/main/proto/cache_service_messages_v1.proto
-	python -m grpc_tools.protoc --proto_path=$(CURRDIR)/etc/proto --python_out=$(CURRDIR)/src/coherence \
-			--grpc_python_out=$(CURRDIR)/src/coherence \
-			$(CURRDIR)/etc/proto/proxy_service_v1.proto \
-			$(CURRDIR)/etc/proto/proxy_service_messages_v1.proto \
-			$(CURRDIR)/etc/proto/common_messages_v1.proto \
-			$(CURRDIR)/etc/proto/cache_service_messages_v1.proto
-	sed -e 's/import proxy_service_messages_v1_pb2 as proxy__service__messages__v1__pb2/import coherence.proxy_service_messages_v1_pb2 as proxy__service__messages__v1__pb2/' \
-		< $(CURRDIR)/src/coherence/proxy_service_v1_pb2.py > $(CURRDIR)/src/coherence/proxy_service_v1_pb2.py.out
-	mv $(CURRDIR)/src/coherence/proxy_service_v1_pb2.py.out $(CURRDIR)/src/coherence/proxy_service_v1_pb2.py
-	sed -e 's/import common_messages_v1_pb2 as common__messages__v1__pb2/import coherence.common_messages_v1_pb2 as common__messages__v1__pb2/' \
-		< $(CURRDIR)/src/coherence/proxy_service_messages_v1_pb2.py > $(CURRDIR)/src/coherence/proxy_service_messages_v1_pb2.py.out
-	mv $(CURRDIR)/src/coherence/proxy_service_messages_v1_pb2.py.out $(CURRDIR)/src/coherence/proxy_service_messages_v1_pb2.py
-	sed -e 's/import proxy_service_messages_v1_pb2 as proxy__service__messages__v1__pb2/import coherence.proxy_service_messages_v1_pb2 as proxy__service__messages__v1__pb2/' \
-		< $(CURRDIR)/src/coherence/proxy_service_v1_pb2_grpc.py > $(CURRDIR)/src/coherence/proxy_service_v1_pb2_grpc.py.out
-	mv $(CURRDIR)/src/coherence/proxy_service_v1_pb2_grpc.py.out $(CURRDIR)/src/coherence/proxy_service_v1_pb2_grpc.py
-	sed -e 's/import common_messages_v1_pb2 as common__messages__v1__pb2/import coherence.common_messages_v1_pb2 as common__messages__v1__pb2/' \
-		< $(CURRDIR)/src/coherence/cache_service_messages_v1_pb2.py > $(CURRDIR)/src/coherence/cache_service_messages_v1_pb2.py.out
-	mv $(CURRDIR)/src/coherence/cache_service_messages_v1_pb2.py.out $(CURRDIR)/src/coherence/cache_service_messages_v1_pb2.py
 
 # ----------------------------------------------------------------------------------------------------------------------
 # Run tests with code coverage
Index: src/coherence/aggregator.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Copyright (c) 2022, 2024, Oracle and/or its affiliates.\n# Licensed under the Universal Permissive License v 1.0 as shown at\n# https://oss.oracle.com/licenses/upl.\n\nfrom __future__ import annotations\n\nfrom abc import ABC\nfrom decimal import Decimal\nfrom enum import Enum, IntEnum\nfrom typing import Any, Dict, Generic, List, Optional, TypeVar, Union\n\nfrom typing_extensions import TypeAlias\n\nfrom .comparator import Comparator, InverseComparator, SafeComparator\nfrom .extractor import ExtractorExpression, Extractors, ValueExtractor\nfrom .filter import Filter\nfrom .serialization import proxy\n\nE = TypeVar(\"E\")\nG = TypeVar(\"G\")\nK = TypeVar(\"K\")\nR = TypeVar(\"R\")\nT = TypeVar(\"T\")\nV = TypeVar(\"V\")\n\nReducerResult: TypeAlias = Dict[K, Union[Any, List[Any]]]\n\n\nclass EntryAggregator(ABC, Generic[R]):\n    \"\"\"An EntryAggregator represents processing that can be directed to occur against some subset of the entries in\n    n cache, resulting in an aggregated result. Common examples of aggregation include functions such as min(),\n    max() and avg(). However, the concept of aggregation applies to any process that needs to evaluate a group of\n    entries to come up with a single answer.\"\"\"\n\n    def __init__(self, extractor_or_property: Optional[ExtractorExpression[T, E]] = None):\n        \"\"\"\n        Construct an AbstractAggregator that will aggregate values extracted from the cache entries.\n\n        :param extractor_or_property: the extractor that provides values to aggregate or the name of the method that\n            could be invoked via Java reflection and that returns values to aggregate; this parameter can also be a\n            dot-delimited sequence of method names which would result in an aggregator based on the\n            :class:`coherence.extractor.ChainedExtractor` that is based on an array of corresponding\n            :class:`coherence.extractor.UniversalExtractor` objects; must not be `None`\n        \"\"\"\n        super().__init__()\n        if extractor_or_property is not None:\n            if isinstance(extractor_or_property, ValueExtractor):\n                self.extractor = extractor_or_property\n            else:\n                self.extractor = Extractors.extract(extractor_or_property)\n\n    def and_then(self, aggregator: EntryAggregator[R]) -> EntryAggregator[List[R]]:\n        \"\"\"\n        Returns a :class:`coherence.aggregator.CompositeAggregator` comprised of this and the provided aggregator.\n\n        :param aggregator: the next aggregator\n        :return: a :class:`coherence.aggregator.CompositeAggregator` comprised of this and the provided aggregator\n        \"\"\"\n        return CompositeAggregator[R]([self, aggregator])\n\n\nclass AbstractComparableAggregator(EntryAggregator[R]):\n    \"\"\"Abstract aggregator that processes values extracted from a set of entries in a Map, with knowledge of how to\n    compare those values. There are two-way to use the AbstractComparableAggregator:\n\n    * All the extracted objects must implement the Java Comparable interface, or\n\n    * The AbstractComparableAggregator has to be provided with a :class:`coherence.comparator.Comparator` object.\n      This :class:`coherence.comparator.Comparator` must exist on the server in order to be usable.\n\n    If there are no entries to aggregate, the returned result will be `None`.\"\"\"\n\n    def __init__(self, extractor_or_property: ExtractorExpression[T, E]):\n        \"\"\"\n        Construct an AbstractComparableAggregator that will aggregate Java-Comparable values extracted from the cache\n        entries.\n\n        :param extractor_or_property: the extractor that provides values to aggregate or the name of the method that\n            could be invoked via Java reflection and that returns values to aggregate; this parameter can also be a\n            dot-delimited sequence of method names which would result in an aggregator based on the\n            :class:`coherence.extractor.ChainedExtractor` that is based on an array of corresponding\n            :class:`coherence.extractor.UniversalExtractor` objects; must not be `None`\n        \"\"\"\n        super().__init__(extractor_or_property)\n\n\nclass AbstractDoubleAggregator(EntryAggregator[Decimal]):\n    \"\"\"Abstract aggregator that processes numeric values extracted from a set of entries in a Map. All the extracted\n    Number objects will be treated as Java `double` values and the result of the aggregator is a Double. If\n    the set of entries is empty, a `None` result is returned.\"\"\"\n\n    def __init__(self, extractor_or_property: ExtractorExpression[T, E]):\n        \"\"\"\n        Construct an AbstractDoubleAggregator that will aggregate numeric values extracted from the cache entries.\n\n        :param extractor_or_property: the extractor that provides values to aggregate or the name of the method that\n            could be invoked via Java reflection and that returns values to aggregate; this parameter can also be a\n            dot-delimited sequence of method names which would result in an aggregator based on the\n            :class:`coherence.extractor.ChainedExtractor` that is based on an array of corresponding\n            :class:`coherence.extractor.UniversalExtractor` objects; must not be `None`\n        \"\"\"\n        super().__init__(extractor_or_property)\n\n\n@proxy(\"aggregator.CompositeAggregator\")\nclass CompositeAggregator(EntryAggregator[List[R]]):\n    \"\"\"`CompositeAggregator` provides an ability to execute a collection of aggregators against the same subset of\n    the entries in a Map, resulting in a list of corresponding aggregation results. The size of the returned list\n    will always be equal to the length of the aggregators list.\"\"\"\n\n    def __init__(self, aggregators: list[EntryAggregator[R]]):\n        \"\"\"\n        Construct a CompositeAggregator based on a specified :class:`coherence.aggregator.EntryAggregator` list.\n\n        :param aggregators: an array of :class:`coherence.aggregator.EntryAggregator` objects; may not be `None`\n        \"\"\"\n        super().__init__()\n        if aggregators is not None:\n            self.aggregators = aggregators\n        else:\n            raise ValueError(\"no aggregators provided\")\n\n\n@proxy(\"aggregator.ComparableMax\")\nclass MaxAggregator(AbstractComparableAggregator[R]):\n    \"\"\"Calculates a maximum of numeric values extracted from a set of entries in a Map in a form of a numerical\n    value. All the extracted objects will be treated as numerical values. If the set of entries is empty,\n    a `None` result is returned.\"\"\"\n\n    def __init__(self, extractor_or_property: ExtractorExpression[T, E]):\n        \"\"\"\n        Constructs a new `MaxAggregator`.\n\n        :param extractor_or_property: the extractor that provides values to aggregate or the name of the method that\n            could be invoked via Java reflection and that returns values to aggregate; this parameter can also be a\n            dot-delimited sequence of method names which would result in an aggregator based on the\n            :class:`coherence.extractor.ChainedExtractor` that is based on an array of corresponding\n            :class:`coherence.extractor.UniversalExtractor` objects; must not be `None`\n        \"\"\"\n        super().__init__(extractor_or_property)\n\n\n@proxy(\"aggregator.ComparableMin\")\nclass MinAggregator(AbstractComparableAggregator[R]):\n    \"\"\"Calculates a minimum of numeric values extracted from a set of entries in a Map in a form of a numerical\n    value. All the extracted objects will be treated as numerical values. If the set of entries is empty,\n    a `None` result is returned.\"\"\"\n\n    def __init__(self, extractor_or_property: ExtractorExpression[T, E]):\n        \"\"\"\n        Constructs a new `MinAggregator`.\n\n        :param extractor_or_property: the extractor that provides values to aggregate or the name of the method that\n            could be invoked via Java reflection and that returns values to aggregate; this parameter can also be a\n            dot-delimited sequence of method names which would result in an aggregator based on the\n            :class:`coherence.extractor.ChainedExtractor` that is based on an array of corresponding\n            :class:`coherence.extractor.UniversalExtractor` objects; must not be `None`\n        \"\"\"\n        super().__init__(extractor_or_property)\n\n\n@proxy(\"aggregator.BigDecimalSum\")\nclass SumAggregator(AbstractDoubleAggregator):\n    \"\"\"Calculates a sum for values of any numeric type extracted from a set of entries in a Map in a form of a\n    numeric value.\n\n    If the set of entries is empty, a 'None' result is returned.\"\"\"\n\n    def __init__(self, extractor_or_property: ExtractorExpression[T, E]):\n        \"\"\"\n        Constructs a new `SumAggregator`.\n\n        :param extractor_or_property: the extractor that provides values to aggregate or the name of the method that\n            could be invoked via Java reflection and that returns values to aggregate; this parameter can also be a\n            dot-delimited sequence of method names which would result in an aggregator based on the\n            :class:`coherence.extractor.ChainedExtractor` that is based on an array of corresponding\n            :class:`coherence.extractor.UniversalExtractor` objects; must not be `None`\n        \"\"\"\n        super().__init__(extractor_or_property)\n\n\n@proxy(\"aggregator.BigDecimalAverage\")\nclass AverageAggregator(AbstractDoubleAggregator):\n    \"\"\"Calculates an average for values of any numeric type extracted from a set of entries in a Map in a form of a\n    numerical value. All the extracted objects will be treated as numerical values. If the set of entries is empty,\n    a `None` result is returned.\"\"\"\n\n    def __init__(self, extractor_or_property: ExtractorExpression[T, E]):\n        \"\"\"\n        Construct an `AverageAggregator` that will sum numeric values extracted from the cache entries.\n\n        :param extractor_or_property: the extractor that provides values to aggregate or the name of the method that\n            could be invoked via Java reflection and that returns values to aggregate; this parameter can also be a\n            dot-delimited sequence of method names which would result in an aggregator based on the\n            :class:`coherence.extractor.ChainedExtractor` that is based on an array of corresponding\n            :class:`coherence.extractor.UniversalExtractor` objects; must not be `None`\n        \"\"\"\n        super().__init__(extractor_or_property)\n\n\n@proxy(\"aggregator.Count\")\nclass CountAggregator(EntryAggregator[int]):\n    \"\"\"Calculates a number of values in an entry set.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Constructs a new `CountAggregator`.\n        \"\"\"\n        super().__init__()\n\n\n@proxy(\"aggregator.DistinctValues\")\nclass DistinctValuesAggregator(EntryAggregator[R]):\n    \"\"\"Return the set of unique values extracted from a set of entries in a Map. If the set of entries is empty,\n    an empty array is returned.\n\n    This aggregator could be used in combination with :class:`coherence.extractor.UniversalExtractor` allowing to\n    collect all unique combinations (tuples) of a given set of attributes.\n\n    The DistinctValues aggregator covers a simple case of a more generic aggregation pattern implemented by the\n    `GroupAggregator`, which in addition to collecting all distinct values or tuples, runs an aggregation against\n    each distinct entry set (group).\"\"\"\n\n    def __init__(self, extractor_or_property: ExtractorExpression[T, E]):\n        \"\"\"\n        Construct a DistinctValuesAggregator that will aggregate numeric values extracted from the cache entries.\n\n        :param extractor_or_property: the extractor that provides values to aggregate or the name of the method that\n            could be invoked via Java reflection and that returns values to aggregate; this parameter can also be a\n            dot-delimited sequence of method names which would result in an aggregator based on the\n            :class:`coherence.extractor.ChainedExtractor` that is based on an array of corresponding\n            :class:`coherence.extractor.UniversalExtractor` objects; must not be `None`\n        \"\"\"\n        super().__init__(extractor_or_property)\n\n\n@proxy(\"aggregator.TopNAggregator\")\nclass TopAggregator(Generic[E, R], EntryAggregator[List[R]]):\n    \"\"\"`TopAggregator` aggregates the top *N* extracted values into an array.  The extracted values must not be\n    `None`, but do not need to be unique.\"\"\"\n\n    def __init__(\n        self,\n        number: int = 0,\n        inverse: bool = False,\n        extractor: ValueExtractor[Any, Any] = Extractors.identity(),\n        comparator: Optional[Comparator] = None,\n        property_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Constructs a new `TopAggregator`.\n\n        :param number: the maximum number of results to include in the aggregation result.\n        :param inverse: Result order.  By default, results will be ordered in descending order.\n        :param extractor: The extractor to obtain the values to aggregate.  If not explicitly set, this will default\n            to an :class:`coherence.extractor.IdentityExtractor`.\n        :param comparator: The :class:`coherence.comparator.Comparator` to apply against the extracted values.\n        :param property_name:  The property that results will be ordered by.\n        \"\"\"\n        super().__init__()\n        self.results = number\n        self.inverse = inverse\n        self.extractor = extractor\n        self.comparator = comparator\n        self.property = property_name\n\n    def order_by(self, property_name: str) -> TopAggregator[E, R]:\n        \"\"\"\n        Order the results based on the values of the specified property.\n\n        :param property_name: the property name\n        :return: an instance of :class:`coherence.aggregator.TopAggregator`\n        \"\"\"\n        self.property = property_name\n        self.comparator = InverseComparator(property_name) if self.inverse else SafeComparator(property_name)\n        return self\n\n    @property\n    def ascending(self) -> TopAggregator[E, R]:\n        \"\"\"\n        Sort the returned values in ascending order.\n\n        :return: an instance of :class:`coherence.aggregator.TopAggregator`\n        \"\"\"\n        if self.property is not None:\n            self.inverse = True\n            self.comparator = InverseComparator(self.property)\n        return self\n\n    @property\n    def descending(self) -> TopAggregator[E, R]:\n        \"\"\"\n        Sort the returned values in descending order.\n\n        :return: an instance of :class:`coherence.aggregator.TopAggregator`\n        \"\"\"\n        if self.property is not None:\n            self.inverse = False\n            self.comparator = SafeComparator(self.property)\n        return self\n\n    def extract(self, property_name: str) -> TopAggregator[E, R]:\n        \"\"\"\n        The property name of the value to extract.\n\n        :param property_name: the property name\n        :return:\n        \"\"\"\n        self.extractor = Extractors.extract(property_name)\n        return self\n\n\n@proxy(\"aggregator.GroupAggregator\")\nclass GroupAggregator(EntryAggregator[R]):\n    \"\"\"The `GroupAggregator` provides an ability to split a subset of entries in a Map into a collection of\n    non-intersecting subsets and then aggregate them separately and independently. The splitting (grouping) is\n    performed using the results of the underlying :class:`coherence.extractor.UniversalExtractor` in such a way that\n    two entries will belong to the same group if and only if the result of the corresponding extract call produces\n    the same value or tuple (list of values). After the entries are split into the groups, the underlying aggregator\n    is applied separately to each group. The result of the aggregation by the` GroupAggregator` is a Map that has\n    distinct values (or tuples) as keys and results of the individual aggregation as values. Additionally,\n    those results could be further reduced using an optional :class:`coherence.filter.Filter` object.\n\n    Informally speaking, this aggregator is analogous to the SQL `group by` and `having` clauses. Note that the\n    `having` Filter is applied independently on each server against the partial aggregation results; this generally\n    implies that data affinity is required to ensure that all required data used to generate a given result exists\n    within a single cache partition. In other words, the `group by` predicate should not span multiple partitions if\n    the `having` clause is used.\n\n    The `GroupAggregator` is somewhat similar to the DistinctValues aggregator, which returns back a list of distinct\n    values (tuples) without performing any additional aggregation work.\"\"\"\n\n    def __init__(\n        self,\n        extractor_or_property: ExtractorExpression[T, E],\n        aggregator: EntryAggregator[R],\n        filter: Optional[Filter] = None,\n    ):\n        \"\"\"\n        Construct a `GroupAggregator` based on a specified :class:`coherence.extractor.ValueExtractor` and underlying\n        :class:`coherence.aggregator.EntryAggregator`.\n\n        :param extractor_or_property:  a :class:`coherence.extractor.ValueExtractor` object that is used to split\n         entries into non-intersecting subsets; may not be `None`. This parameter can also be a dot-delimited sequence\n         of method names which would result in an aggregator based on the :class:`coherence.extractor.ChainedExtractor`\n         that is based on an array of corresponding :class:`coherence.extractor.UniversalExtractor` objects; may not be\n         `NONE`\n\n        :param aggregator: an EntryAggregator object; may not be null\n        :param filter: an optional Filter object used to filter out results of individual group aggregation results\n        \"\"\"\n        super().__init__(extractor_or_property)\n        if aggregator is not None:\n            self.aggregator = aggregator\n        else:\n            raise ValueError(\"no aggregator provided\")\n        self.filter = filter\n\n\nclass Timeout(IntEnum):\n    NONE: int = -1\n    \"\"\"A special timeout value to indicate that this task or request can run indefinitely.\"\"\"\n\n    DEFAULT: int = 0\n    \"\"\"A special timeout value to indicate that the corresponding service's default timeout value should be used.\"\"\"\n\n\nclass Schedule(Enum):\n    STANDARD = 0\n    \"\"\"Scheduling value indicating that this task is to be queued and execute in a natural (based on the request\n    arrival time) order.\"\"\"\n\n    FIRST = 1\n    \"\"\"Scheduling value indicating that this task is to be queued in front of any equal or lower scheduling priority\n    tasks and executed as soon as any of the worker threads become available.\"\"\"\n\n    IMMEDIATE = 2\n    \"\"\"Scheduling value indicating that this task is to be immediately executed by any idle worker thread; if all\n    of them are active, a new thread will be created to execute this task.\"\"\"\n\n\n@proxy(\"aggregator.PriorityAggregator\")\nclass PriorityAggregator(Generic[R], EntryAggregator[R]):\n    \"\"\"A `PriorityAggregator` is used to explicitly control the scheduling priority and timeouts for execution of\n    EntryAggregator-based methods.\n\n    For example, lets assume that there is an `Orders` cache that belongs to a partitioned cache service configured\n    with a *request-timeout* and *task-timeout* of 5 seconds. Also assume that we are willing to wait longer for a\n    particular aggregation request that scans the entire cache. Then we could override the default timeout values by\n    using the PriorityAggregator as follows::\n\n        sumAggr = SumAggregator(\"cost\")\n        priorityAgg = PriorityAggregator(sumAggr)\n        priorityAgg.executionTimeout = Timeout.NONE\n        priorityAgg.requestTimeout = Timeout.NONE\n        cacheOrders.aggregate(aFilter, priorityAgg)\n\n    This is an advanced feature which should be used judiciously.\"\"\"\n\n    def __init__(\n        self,\n        aggregator: EntryAggregator[R],\n        execution_timeout: int = Timeout.DEFAULT,\n        request_timeout: int = Timeout.DEFAULT,\n        scheduling_priority: Schedule = Schedule.STANDARD,\n    ):\n        \"\"\"\n        Construct a new `PriorityAggregator`.\n\n        :param aggregator: The wrapped :class:`coherence.aggregator.EntryAggregator`.\n        :param execution_timeout: The task execution timeout value.\n        :param request_timeout: The request timeout value.\n        :param scheduling_priority: The scheduling priority.\n        \"\"\"\n        super().__init__()\n        self.aggregator = aggregator\n        self._execution_timeout = execution_timeout\n        self._request_timeout = request_timeout\n        self._scheduling_priority = scheduling_priority\n\n    @property\n    def scheduling_priority(self) -> Schedule:\n        \"\"\"\n        Return the scheduling priority or, if not explicitly set, the default is\n        :class:`coherence.aggregator.Schedule.STANDARD`\n\n        :return: the scheduling priority\n        \"\"\"\n        return self._scheduling_priority\n\n    @scheduling_priority.setter\n    def scheduling_priority(self, scheduling_priority: Schedule) -> None:\n        \"\"\"\n        Set the scheduling priority.\n\n        :param scheduling_priority: the scheduling priority.\n        \"\"\"\n        self._scheduling_priority = scheduling_priority\n\n    @property\n    def execution_timeout_in_millis(self) -> int:\n        \"\"\"\n        Return the execution timeout in milliseconds.\n\n        :return: the execution timeout\n        \"\"\"\n        return self._execution_timeout\n\n    @execution_timeout_in_millis.setter\n    def execution_timeout_in_millis(self, execution_timeout: int) -> None:\n        \"\"\"\n        Set the execution timeout in milliseconds.\n\n        :param execution_timeout: the new execution timeout in milliseconds\n        \"\"\"\n        self._execution_timeout = execution_timeout\n\n    @property\n    def request_timeout_in_millis(self) -> int:\n        \"\"\"\n        Return the request timeout in milliseconds.\n\n        :return: the request timeout\n        \"\"\"\n        return self._request_timeout\n\n    @request_timeout_in_millis.setter\n    def request_timeout_in_millis(self, request_timeout: int) -> None:\n        \"\"\"\n        Set the request timeout in milliseconds.\n\n        :param request_timeout: the new request timeout in milliseconds\n        \"\"\"\n        self._request_timeout = request_timeout\n\n\n@proxy(\"aggregator.ScriptAggregator\")\nclass ScriptAggregator(Generic[R], EntryAggregator[R]):\n    \"\"\"ScriptAggregator is a :class:`coherence.aggregator.EntryAggregator` that wraps a script written in one of the\n    languages supported by Graal VM.\"\"\"\n\n    def __init__(self, language: str, script_name: str, characteristics: int = 0, *args: Any):\n        \"\"\"\n        Create a :class:`coherence.aggregator.EntryAggregator` that wraps the specified script.\n\n        :param language: The language with which the script is written in.\n        :param script_name: The name of the :class:`coherence.aggregator.EntryAggregator` that needs to be evaluated.\n        :param characteristics: Present only for serialization purposes.\n        :param args: The arguments to be passed to the script for evaluation\n        \"\"\"\n        super().__init__()\n        self.language = language\n        self.name = script_name\n        self.args = list()\n        for arg in args:\n            self.args.append(arg)\n        self.characteristics = characteristics\n\n\nclass RecordType(Enum):\n    EXPLAIN = 0\n    \"\"\"Produce an object that contains an estimated cost of the query execution.\"\"\"\n\n    TRACE = 1\n    \"\"\"Produce an object that contains the actual cost of the query execution.\"\"\"\n\n\n# TODO IMPROVE\n@proxy(\"aggregator.QueryRecorder\")\nclass QueryRecorder(EntryAggregator[Any]):\n    \"\"\"This aggregator is used to produce an object that contains an estimated or actual cost of the query execution\n    for a given :class:`coherence.filter.Filter`.\n\n    For example, the following code will print a *QueryRecord*,\n    containing the estimated query cost and corresponding execution steps::\n\n        agent  = QueryRecorder(RecordType.EXPLAIN);\n        record = cache.aggregate(someFilter, agent);\n        print(json.dumps(record));\n    \"\"\"\n\n    EXPLAIN: str = \"EXPLAIN\"\n    \"\"\"String constant for serialization purposes.\"\"\"\n\n    TRACE: str = \"TRACE\"\n    \"\"\"String constant for serialization purposes.\"\"\"\n\n    def __init__(self, query_type: RecordType):\n        \"\"\"\n        Construct a new `QueryRecorder`.\n\n        :param query_type: the type for this aggregator\n        \"\"\"\n        super().__init__()\n        self.type = QueryRecorder.get_type(query_type)\n\n    @classmethod\n    def get_type(cls, query_type: RecordType) -> dict[str, str]:\n        if query_type == RecordType.EXPLAIN:\n            return {\"enum\": cls.EXPLAIN}\n        elif query_type == RecordType.TRACE:\n            return {\"enum\": cls.TRACE}\n\n\n@proxy(\"aggregator.ReducerAggregator\")\nclass ReducerAggregator(EntryAggregator[R]):\n    \"\"\"The `ReducerAggregator` is used to implement functionality similar to :class:`coherence.client.NamedMap.getAll(\n    )` API.  Instead of returning the complete set of values, it will return a portion of value attributes based on\n    the provided :class:`coherence.extractor.ValueExtractor`.\n\n    This aggregator could be used in combination with {@link MultiExtractor} allowing one to collect tuples that are\n    a subset of the attributes of each object stored in the cache.\"\"\"\n\n    def __init__(self, extractor_or_property: ExtractorExpression[T, E]):\n        \"\"\"\n        Creates a new `ReducerAggregator`.\n\n        :param extractor_or_property: the extractor that provides values to aggregate or the name of the method that\n            could be invoked via Java reflection and that returns values to aggregate; this parameter can also be a\n            dot-delimited sequence of method names which would result in an aggregator based on the\n            :class:`coherence.extractor.ChainedExtractor` that is based on an array of corresponding\n            :class:`coherence.extractor.UniversalExtractor` objects; must not be `None`\n        \"\"\"\n        super().__init__(extractor_or_property)\n\n\nclass Aggregators:\n    \"\"\"Simple Aggregator DSL.\n\n    The methods in this class are for the most part simple factory methods for various\n    :class:`coherence.aggregator.EntryAggregator`  classes, but in some cases provide additional type safety. They\n    also tend to make the code more readable, especially if imported statically, so their use is strongly encouraged\n    in lieu of direct construction of :class:`coherence.aggregator.EntryAggregator`  classes.\"\"\"\n\n    @staticmethod\n    def max(extractor_or_property: ExtractorExpression[T, E]) -> EntryAggregator[R]:\n        \"\"\"\n        Return an aggregator that calculates a maximum of the numeric values extracted from a set of entries in a Map.\n\n        :param extractor_or_property: the extractor or method/property name to provide values for aggregation\n        :return: an aggregator that calculates a maximum of the numeric values extracted from a set of entries in a Map\n        \"\"\"\n        return MaxAggregator(extractor_or_property)\n\n    @staticmethod\n    def min(extractor_or_property: ExtractorExpression[T, E]) -> EntryAggregator[R]:\n        \"\"\"\n        Return an aggregator that calculates a minimum of the numeric values extracted from a set of entries in a Map.\n\n        :param extractor_or_property: the extractor or method/property name to provide values for aggregation\n        :return: an aggregator that calculates a minimum of the numeric values extracted from a set of entries in a Map.\n        \"\"\"\n        return MinAggregator(extractor_or_property)\n\n    @staticmethod\n    def sum(extractor_or_property: ExtractorExpression[T, E]) -> EntryAggregator[Decimal]:\n        \"\"\"\n        Return an aggregator that calculates a sum of the numeric values extracted from a set of entries in a Map.\n\n        :param extractor_or_property: the extractor or method/property name to provide values for aggregation\n        :return: an aggregator that calculates a sum of the numeric values extracted from a set of entries in a Map.\n        \"\"\"\n        return SumAggregator(extractor_or_property)\n\n    @staticmethod\n    def average(extractor_or_property: ExtractorExpression[T, E]) -> EntryAggregator[Decimal]:\n        \"\"\"\n        Return an aggregator that calculates an average of the numeric values extracted from a set of entries in a Map.\n\n        :param extractor_or_property: the extractor or method/property name to provide values for aggregation\n        :return: an aggregator that calculates an average of the numeric values extracted from a\n                 set of entries in a Map.\n        \"\"\"\n        return AverageAggregator(extractor_or_property)\n\n    @staticmethod\n    def distinct(extractor_or_property: ExtractorExpression[T, E]) -> EntryAggregator[List[R]]:\n        \"\"\"\n        Return an aggregator that calculates the set of distinct values from the entries in a Map.\n\n        :param extractor_or_property: the extractor or method/property name to provide values for aggregation\n        :return: an aggregator that calculates the set of distinct values from the entries in a Map.\n        \"\"\"\n        return DistinctValuesAggregator(extractor_or_property)\n\n    @staticmethod\n    def count() -> EntryAggregator[int]:\n        \"\"\"\n        Return an aggregator that calculates a number of values in an entry set.\n\n        :return: an aggregator that calculates a number of values in an entry set.\n        \"\"\"\n        return CountAggregator()\n\n    @staticmethod\n    def top(count: int) -> TopAggregator[Any, Any]:\n        \"\"\"\n        Return an aggregator that aggregates the top *N* extracted values into an array.\n\n        :param count: the maximum number of results to include in the aggregation result\n        :return: an aggregator that aggregates the top *N* extracted values into an array.\n        \"\"\"\n        return TopAggregator(count)\n\n    @staticmethod\n    def group_by(\n        extractor_or_property: ExtractorExpression[T, E],\n        aggregator: EntryAggregator[Any],\n        filter: Optional[Filter] = None,\n    ) -> EntryAggregator[Dict[G, T]]:\n        \"\"\"\n        Return a :class:`coherence.aggregator.GroupAggregator` based on a specified property or method name(s) and an\n        :class:`coherence.aggregator.EntryAggregator`.\n\n        :param extractor_or_property: the extractor or method/property name to provide values for aggregation\n        :param aggregator: the underlying :class:`coherence.aggregator.EntryAggregator`\n        :param filter: an optional :class:`coherence.filter.Filter` object used to filter out results of individual\n          group aggregation results\n        :return: a :class:`coherence.aggregator.GroupAggregator` based on a specified property or method name(s) and an\n          :class:`coherence.aggregator.EntryAggregator`.\n        \"\"\"\n        return GroupAggregator(extractor_or_property, aggregator, filter)\n\n    @staticmethod\n    def priority(\n        aggregator: EntryAggregator[R],\n        execution_timeout: Timeout = Timeout.DEFAULT,\n        request_timeout: Timeout = Timeout.DEFAULT,\n        scheduling_priority: Schedule = Schedule.STANDARD,\n    ) -> EntryAggregator[R]:\n        \"\"\"\n        Return a new :class:`coherence.aggregator.PriorityAggregator` to control scheduling priority of an aggregation\n        operation.\n\n        :param aggregator: the underlying :class:`coherence.aggregator.EntryAggregator`\n        :param execution_timeout: the execution :class:`coherence.aggregator.Timeout`\n        :param request_timeout: the request :class:`coherence.aggregator.Timeout`\n        :param scheduling_priority: the :class:`coherence.aggregator.Schedule` priority\n        :return: a new :class:`coherence.aggregator.PriorityAggregator` to control scheduling priority of an aggregation\n         operation.\n        \"\"\"\n        return PriorityAggregator(aggregator, execution_timeout, request_timeout, scheduling_priority)\n\n    @staticmethod\n    def script(language: str, script_name: str, characteristics: int = 0, *args: Any) -> EntryAggregator[R]:\n        \"\"\"\n        Return an aggregator that is implemented in a script using the specified language.\n\n        :param language: The language with which the script is written in.\n        :param script_name: The name of the :class:`coherence.aggregator.EntryAggregator` that needs to be evaluated.\n        :param characteristics: Present only for serialization purposes.\n        :param args: The arguments to be passed to the script for evaluation\n        :return: an aggregator that is implemented in a script using the specified language.\n        \"\"\"\n        return ScriptAggregator(language, script_name, characteristics, *args)\n\n    @staticmethod\n    def record(query_type: RecordType = RecordType.EXPLAIN) -> EntryAggregator[Any]:\n        \"\"\"\n        Returns a new :class:`coherence.aggregator.QueryRecorder` aggregator which may be used is used to produce an\n        object that contains an estimated or actual cost of the query execution for a given\n        :class:`coherence.filter.Filter`.\n\n        :param query_type: the :class:`coherence.aggregator.RecordType`\n        :return: a new :class:`coherence.aggregator.QueryRecorder` aggregator which may be used is used to produce an\n         object that contains an estimated or actual cost of the query execution for a given\n         :class:`coherence.filter.Filter`.\n        \"\"\"\n        return QueryRecorder(query_type)\n\n    @staticmethod\n    def reduce(extractor_or_property: ExtractorExpression[T, E]) -> EntryAggregator[ReducerResult[K]]:\n        \"\"\"\n        Return an aggregator that will return the extracted value for each entry in the map.\n\n        :param extractor_or_property: the extractor or method/property name to provide values for aggregation\n        :return: an aggregator that will return the extracted value for each entry in the map.\n        \"\"\"\n        return ReducerAggregator(extractor_or_property)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/coherence/aggregator.py b/src/coherence/aggregator.py
--- a/src/coherence/aggregator.py	(revision 7ed18d6c9ea66aec63a655a1b556e3c67cdb16b6)
+++ b/src/coherence/aggregator.py	(date 1725564602805)
@@ -7,9 +7,7 @@
 from abc import ABC
 from decimal import Decimal
 from enum import Enum, IntEnum
-from typing import Any, Dict, Generic, List, Optional, TypeVar, Union
-
-from typing_extensions import TypeAlias
+from typing import Any, Dict, Generic, List, Optional, TypeVar, Union, TypeAlias
 
 from .comparator import Comparator, InverseComparator, SafeComparator
 from .extractor import ExtractorExpression, Extractors, ValueExtractor
